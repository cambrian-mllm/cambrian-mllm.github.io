<!doctype html>
<html lang="en">
    <head>
        <title>Towards Spatial Supersensing in Video</title>
        <link rel="icon" type="image/x-icon" href="figs/header.png">

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Open Graph -->
        <meta property="og:url" content="https://cambrian-s.github.io/" />
        <meta property="og:image" content="https://cambrian-s.github.io/new_header.webp" />
        <meta property="og:title" content="Cambrian-S" />
        <meta property="og:description" content="Towards Spatial Supersensing in Video" />

        <!-- Twitter -->
        <meta name="twitter:url" content="https://cambrian-s.github.io/" />
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:image" content="https://cambrian-s.github.io/new_header.webp" />
        <meta name="twitter:title" content="Cambrian-S" />
        <meta name="twitter:description" content="Towards Spatial Supersensing in Video" />

        <script src="./static/js/distill_template.v2.js"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

        <script defer="" src="./static/js/hider.js"></script>
        <script src="./static/js/image_interact.js"></script>
        <script src="./static/js/switch_videos.js"></script>

        <link rel="preload" as="image" href="figs/new_header.png" fetchpriority="high">
        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>

        <!-- medium zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>
        <script>
            function toggleAnswer(answerId, element) {
                var answerDiv = document.getElementById(answerId);
                if (answerDiv.style.display === "none" || answerDiv.style.display === "") {
                    answerDiv.style.display = "block";
                    element.textContent = "Click to hide answer";
                } else {
                    answerDiv.style.display = "none";
                    element.textContent = "Click to view Ground Truth";
                }
            }
            
            // Set default playback speed to 1.25x for video_46
            document.addEventListener('DOMContentLoaded', function() {
                var video46 = document.getElementById('video_46');
                if (video46) {
                    video46.addEventListener('loadedmetadata', function() {
                        this.playbackRate = 1.25;
                    });
                    // Also set it immediately if video is already loaded
                    if (video46.readyState >= 1) {
                        video46.playbackRate = 1.25;
                    }
                }
            });
        </script>
        <style>
            .authors a,
            .authors a * {
                text-decoration: none !important;
                border-bottom: none !important;
                text-decoration-line: none !important;
                text-decoration-style: none !important;
                text-decoration-color: transparent !important;
            }
            .authors a:visited,
            .authors a:focus,
            .authors a:active,
            .authors a:visited *,
            .authors a:focus *,
            .authors a:active * {
                text-decoration: none !important;
                border-bottom: none !important;
                text-decoration-line: none !important;
                text-decoration-style: none !important;
                text-decoration-color: transparent !important;
            }
            .authors a:hover,
            .authors a:hover * {
                color: #007bff !important;
                text-decoration: underline !important;
                text-decoration-line: underline !important;
                text-decoration-style: solid !important;
                text-decoration-color: #007bff !important;
            }
        </style>
    </head>
    <body>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <div class="header-content">
                    <h1 style="margin-top: 0px"><i>Cambrian-S</i></h1>
                    <h2>Towards Spatial Supersensing in Video</h2>

                    <div class="icon-container">
                        <div class="icon-item">
                            <img src="./static/img/icons/positioning.png" alt="Visual Representation Icon">
                            <div><strong>Position</strong>: We argue that advancing toward true multimodal intelligence requires a shift from language-centric perception toward spatial supersensing: the capacity not only to see, but also to construct, update and predict with an implicit 3D world model from continual sensory experience.</div>
                        </div>
                        <div class="icon-item">
                            <img src="./static/img/icons/eval.svg" alt="Connector Design Icon">
                            <div><strong>Benchmark</strong>: We re-examine existing benchmarks through the lens of our supersensing hierarchy and design a two-part benchmark to better probe spatial supersensing.</div>
                        </div>
                        <div class="icon-item">
                            <img src="./static/img/icons/data_v2.svg" alt="Speech Logo" class="icon">
                            <div><strong>Dataset</strong>: We investigate whether spatial supersensing is simply a data problem, and curate a large-scale spatially focused dataset VSI-590K to push the limit under existing paradigm.</div>
                        </div>
                        <div class="icon-item">
                            <img src="./static/img/icons/nn_model.svg" alt="Vision Logo" class="icon">
                            <div><strong>Model</strong>: We develop Cambrian-S, a family of spatially-grounded models with leading spatial sensing performance and competitive general capabilities.</div>
                        </div>
                        <div class="icon-item">
                            <img src="./static/img/icons/visual.svg" alt="Vision Logo" class="icon">
                            <div><strong>Predictive Sensing</strong>: We prototype predictive sensing, using latent frame prediction to build MLLM's internal world model, and measuring surprise to handle unbounded visual streams.</div>
                        </div>
                    </div>
                    <div class="button-container">
                        <a href="https://arxiv.org/abs/2511.04670" class="button">
                            <span class="icon is-small">
                                <i class="ai ai-arxiv"></i>
                            </span>
                            <span>arXiv</span>
                        </a>
                        <a href="https://github.com/cambrian-mllm/cambrian-s" class="button" target="_blank">
                            <span class="icon is-small">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                        </a>
                        <a href="https://huggingface.co/collections/nyu-visionx/cambrian-s-models" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>Models</span>
                        </a>
                        <a href="https://hf.co/datasets/nyu-visionx/vsi-590k" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>Data</span>
                        </a>
                        <a href="https://hf.co/collections/nyu-visionx/vsi-super" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>Benchmark</span>
                        </a>
                    </div>
                </div>
                <div class="header-image">
                    <img src="figs/header_fg.png" class="header-hero-image" alt="Cambrian-era spatial intelligence" draggable="false" loading="eager" fetchpriority="high" decoding="async">
                </div>
            </div>
        </div>
        <d-article>
            <div class="byline">
                <div class="byline-container">
                    <div class="authors" style="text-align: center; margin-bottom: 1rem;">
                        <div style="margin-bottom: 0.5rem;">
                            <a href="https://github.com/vealocia" target="_blank" style="color: inherit; text-decoration: none !important;"><span class="author-name">Shusheng Yang</span></a><sup>1*</sup>,
                            <a href="https://jihanyang.github.io/" target="_blank" style="color: inherit; text-decoration: none !important;"><span class="author-name">Jihan Yang</span></a><sup>1*</sup>,
                            <a href="https://pinzhihuang.github.io/" target="_blank" style="color: inherit; text-decoration: none !important;"><span class="author-name">Pinzhi Huang</span></a><sup>1&dagger;</sup>,
                            <a href="https://ellisbrown.github.io/" target="_blank" style="color: inherit; text-decoration: none !important;"><span class="author-name">Ellis Brown</span></a><sup>1&dagger;</sup>,
                            <a href="https://redagavin.github.io/" target="_blank" style="color: inherit; text-decoration: none !important;"><span class="author-name">Zihao Yang</span></a><sup>1</sup>,
                        </div>
                        <div style="margin-bottom: 0.5rem;">
                            <a href="https://github.com/czyuyue" target="_blank" style="color: inherit; text-decoration: none !important;"><span class="author-name">Yue Yu</span></a><sup>1</sup>,
                            <a href="https://tsb0601.github.io/" target="_blank" style="color: inherit; text-decoration: none !important;"><span class="author-name">Shengbang Tong</span></a><sup>1</sup>,
                            <a href="#" target="_blank" style="color: inherit; text-decoration: none !important;"><span class="author-name">Zihan Zheng</span></a><sup>1</sup>,
                            <a href="https://www.yfxu.com/" target="_blank" style="color: inherit; text-decoration: none !important;"><span class="author-name">Yifan Xu</span></a><sup>1</sup>,
                            <a href="https://github.com/wang-muhan" target="_blank" style="color: inherit; text-decoration: none !important;"><span class="author-name">Muhan Wang</span></a><sup>1</sup>,
                            <a href="https://daohanlu.github.io/" target="_blank" style="color: inherit; text-decoration: none !important;"><span class="author-name">Daohan Lu</span></a><sup>1</sup>,
                        </div>
                        <div>
                            <a href="https://cs.nyu.edu/~fergus/pmwiki/pmwiki.php" target="_blank" style="color: inherit; text-decoration: none !important;"><span class="author-name">Rob Fergus</span></a><sup>1</sup>,
                            <a href="http://yann.lecun.com/" target="_blank" style="color: inherit; text-decoration: none !important;"><span class="author-name">Yann LeCun</span></a><sup>1</sup>,
                            <a href="https://profiles.stanford.edu/fei-fei-li" target="_blank" style="color: inherit; text-decoration: none !important;"><span class="author-name">Li Fei-Fei</span></a><sup>2</sup>,
                            <a href="https://www.sainingxie.com/" target="_blank" style="color: inherit; text-decoration: none !important;"><span class="author-name">Saining Xie</span></a><sup>1</sup>
                        </div>
                    </div>
                    <div style="text-align: center; font-size: 1.1rem; color: #666;"><span><sup>*</sup>Equal Contribution</span> &nbsp; <span><sup>&dagger;</sup>Core Contributor</span></div>
                    <div class="affiliations" style="text-align: center; font-size: 1.1rem; color: #666;"><span><sup>1</sup>New York University</span> &nbsp; <span><sup>2</sup>Stanford University</span></div>
                </div>
            </div>

            <div style="max-width: 100%; margin: 30px auto; width: 100%;">
                <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);">
                    <iframe 
                        style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: 0;"
                        src="https://www.youtube.com/embed/denldZGVyzM" 
                        title="YouTube video player" 
                        frameborder="0" 
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
                        allowfullscreen>
                    </iframe>
                </div>
            </div>
            
            <h1 class="text">Overview</h1>
            <p class="text">
                A video is not just a sequence of frames in isolation. It is a continual, high-bandwidth projection of a hidden, evolving 3D world onto pixels. Although multimodal large language models (MLLMs) have advanced rapidly by pairing strong image encoders with language models, most video extensions remain fundamentally constrained. They still treat video as sparse frames, underrepresent spatial structure and dynamics, and lean heavily on textual recall, thus overlooking what makes the video modality uniquely powerful. 
            </p>
            <p class="text" >
                We argue that advancing toward true multimodal intelligence requires a shift from language-centric perception toward spatial <em>supersensing</em>: the capacity not only to see, but also to construct, update, and predict an implicit model of the 3-D world from continual sensory experience. We define spatial supersensing as a hierarchy of capabilities:
            </p>

            <section id="teaser" style="margin-top: 2rem; margin-bottom: 4rem;">
                <d-figure>
                    <figure>
                        <img src="figs/teaser_v10.png" alt="Spatial supersensing teaser" class="pdf-figure" style="width: auto; height: auto; border: none;" data-zoomable="" draggable="false">
                        <figcaption>
                            <strong>Figure:</strong> From pixels to predictive mind. We look beyond linguistic-only understanding to envision multimodal intelligence that sees, remembers, and reasons as part of a continuous, lived world.
                        </figcaption>
                    </figure>
                </d-figure>
            </section>
        
            <ul class="text">
                <li>
                    <strong><span style="color: gray;">(Linguistic-only understanding):</span></strong> no sensory capabilities; reasoning confined to text and symbols. Current MLLMs have progressed beyond this stage, yet still retain traces of its bias.
                </li>
                <li>
                    <strong>Semantic perception:</strong> parsing pixels into objects, attributes, and relations. This corresponds to the strong multimodal <em>show and tell</em> capabilities present in MLLMs.
                </li>
                <li>
                    <strong>Streaming event cognition:</strong> processing live, unbounded streams while proactively interpreting and responding to ongoing events. This aligns with efforts to make MLLMs real-time assistants.
                </li>
                <li>
                    <strong>Implicit 3D spatial cognition:</strong> understanding video as projections of a 3D world. Agents must know what is present, where, how things relate, and how configurations change over time. Today's video models remain limited here.
                </li>
                <li>
                    <strong>Predictive world modeling:</strong> the brain makes <em>unconscious inference</em> by predicting latent world states based on prior expectations. When these predictions are violated, surprise guides attention, memory, and learning. However, current multimodal systems lack an internal model that anticipates future states and uses surprise to organize perception for memory and decision making.
                </li>
            </ul>


            <section id="hierarchy-and-benchmarks">
                <h1 class="text">Deconstructing Existing Video Benchmarks</h1>
                <p class="text">Recent advances in MLLM have led to a surge in video QA benchmarks. However, a critical question remains: to what extent do existing video benchmarks truly examine visual sensing capabilities rather than simply testing language priors? We establish several experimental conditions for feeding video input to a Cambrian-1 model:</p> 
                
                <p class="text">
                    <strong>Diagnostic Setup.</strong> We establish five experimental conditions to isolate the contributions of different information sources. We provide the model with either a <strong>Single Frame</strong> (the middle frame), <strong>Multiple Frames</strong> (32 uniformly sampled frames), or textual <strong>Frame Captions</strong> generated from those 32 frames. We compare these against two baselines: a <strong>Blind Test</strong>, where the model only receives the question, and <strong>Chance Acc</strong>, which represents random guessing. By analyzing performance differences between these conditions—such as <code>diff(Multiple, Single)</code> to assess temporal cues or <code>diff(Multiple, Captions)</code> to control for textual solvability—we can create a fine-grained profile of each benchmark's characteristics.
                </p>
                
                <d-figure>
                    <figure style="position: relative;">
                        <img src="figs/benchmark_analysis_sy_v4.png" alt="Benchmark diagnostic study" class="pdf-figure" style="width: 100%; height: auto; border: none;" data-zoomable="" draggable="false">
                        <div style="position: absolute; bottom: 50px; right: 10px; background: rgba(255, 255, 255, 0.95); padding: 15px; border-radius: 5px; font-size: 1.0em; line-height: 1.4; box-shadow: 0 2px 5px rgba(0,0,0,0.1); border: 1px solid black;">
                            <strong>Legend:</strong><br>
                            <strong>MME:</strong> VideoMME<br>
                            <strong>ES:</strong> EgoSchema<br>
                            <strong>VM:</strong> VideoMMMU<br>
                            <strong>LV:</strong> LongVideoBench<br>
                            <strong>TM:</strong> Tomato<br>
                            <strong>MV:</strong> MVBench<br>
                            <strong>PT:</strong> Perception Test<br>
                            <strong>HV:</strong> HourVideo<br>
                            <strong>VSI:</strong> VSIBench<br>
                            <strong>VSR:</strong> VSI-SUPER Recall<br>
                            <strong>VSC:</strong> VSI-SUPER Count
                        </div>
                        <figcaption >
                            <strong>Figure:</strong> Comprehensive analysis comparing visual, caption-only, and blind settings across existing video benchmarks.
                        </figcaption>
                    </figure>
                </d-figure>
                
                <div style="margin: 2rem auto; max-width: auto; padding: 1.5rem; background-color: #f5f5f5; border: 2px solid #008080; border-radius: 12px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);">
                    <p style="margin: 0; color: #000; font-size: 1.1em; line-height: 1.6;">
                        <i class="fas fa-bookmark" style="color: #000; margin-right: 10px;"></i>
                        Existing benchmarks overwhelmingly focus on linguistic understanding and semantic perception while neglecting the more advanced spatial and temporal reasoning required for supersensing.
                    </p>
                </div>
                
                <p class="text">
                    <strong>Analysis of Results.</strong> Results presented in the above figure (a-c) demonstrate that Cambrian-1, an image-based MLLM without any video post-training, can attain reasonable performance across many benchmarks, in some instances surpassing chance-level accuracy by 10-30% (see g,h). This suggests that much of the knowledge these benchmarks target is accessible via standard single-image instruction-tuning pipelines. 
                    Also, employing textual captions in place of visual inputs also yields notable performance improvements, surpassing chance accuracy by more than 20% on benchmarks such as Egoschema, VideoMME, LongVideoBench, VideoMMMU, Perception Test, and MVBench (i, j). Such performance implies that these benchmarks primarily probe abilities inferable from textual summaries of video content.
                </p>
                
                <p class="text">
                    Nevertheless, on existing datasets focusing on spatial-temporal reasoning (like VSI-Bench, HourVideo and Tomato), Cambrian-1 still struggles with only textual or single-frame inputs.
                </p>
                
                <p class="text">
                    <strong>Remark.</strong> We hope to emphasize the inherent challenges in benchmarking and the impracticality of creating a single, all-encompassing benchmark to evaluate every capability. For example, reliance on language priors should not be viewed merely as a drawback, as access to rich world knowledge and its effective retrieval is undoubtedly beneficial in many scenarios. We argue that <strong>video benchmarks should not be treated as measuring a single, uniform notion of "video understanding." Instead, their design and evaluation should be grounded in the specific capabilities they aim to assess.</strong> The preceding analyses are therefore intended to guide the development of tasks that more effectively drive progress towards <em>spatial supersensing</em>, which will be the central focus of the rest of the paper.
                </p>
                
                <h1 class="text">VSI-SUPER: Towards Benchmarking Spatial Supersensing in Multimodal LLMs</h1>
                
                <div class="sub-section">
                    <h3 class="text">VSR: Long-Horizon Spatial Observation and Recall</h3>
                    <p class="text">
                        The VSR benchmark requires MLLMs to observe long-horizon spatiotemporal videos, and sequentially recall the locations of an unusual object.
                        As shown in the figure below, to construct this benchmark, human annotators use an image editing model (like Gemini) to insert surprising or out-of-place objects (like a Teddy Bear, Hello Kitty) into four distinct frames of a space-scanning video. This edited video is then concatenated with other similar space-scan videos to create an arbitrarily long and continuous visual stream. This task parallels the needle-in-a-haystack (NIAH) test commonly used in the language domain to stress test the long-context capabilities of LLMs. Similar NIAH setups have also been proposed for long-video evaluation. However, unlike benchmarks that insert unrelated text segments or frames, VSR preserves the realism of the "needle" through in-frame editing. It further extends the challenge by requiring sequential recall, effectively a multi-hop reasoning task, and remains arbitrarily scalable in video length.
                    </p>
                    <d-figure>
                        <figure>
                            <img src="figs/memory_benchmark_design_v3.png" alt="VSR task illustration" class="pdf-figure" style="width: 90%; height: auto; border: none;" data-zoomable="" draggable="false">
                            <figcaption >
                                <strong>Figure:</strong> VSR benchmark design for long-horizon spatial recall. To thoroughly evaluate model performance across different time scales, the benchmark is provided in five durations: 10, 30, 60, 120, and 240 minutes.
                            </figcaption>
                        </figure>
                    </d-figure>
                </div>
                
                <div class="sub-section">
                    <h3 class="text">VSC: Continual Counting under Changing Viewpoints and Scenes</h3>
                    <p class="text">
                        VSC test the capacity of MLLMs to continuously accumulate information in long-form spatial videos. To build VSC, we concatenate multiple room-tour video clips from VSI-Bench and task models with counting the total number of target objects across multiple rooms (see the figure below). This setting is challenging because the model must handle viewpoint shifts, repeat sightings, and scene transitions, all while maintaining a consistent cumulative count. For humans, counting is an intuitive and generalizable process. Once the concept of "one" is understood, extending it to larger quantities is natural. In contrast, as we later demonstrate, current MLLMs lack true spatial cognition and depend excessively on learned statistical patterns.
                    </p>
                    <d-figure>
                        <figure>
                            <img src="figs/mem_count_illustration_jy_v2.png" alt="VSC task illustration" class="pdf-figure" style="width: 95%; height: auto; border: none;" data-zoomable="" draggable="false">
                            <figcaption >
                                <strong>Figure:</strong> VSC benchmark for continual counting across changing viewpoints and scenes.
                            </figcaption>
                        </figure>
                    </d-figure>

                    <p style="text-align: center; font-weight: bold; margin-top: 20px;">
                        VSI-SUPER Demo
                    </p>

                    <div class="l-body">
                        <!-- Preview Images in a Flex Container -->
                        <div class="preview-container">
                            <img id="VSIvideo1Preview" class="preview" src="static/img/cropped_thumbnails/vsc_thumbnail.png" alt="VSC video thumbnail" onclick="switchVideo('VSI', 'video1Container', 'video1Preview')">
                            <img id="VSIvideo2Preview" class="preview" src="static/img/cropped_thumbnails/vso_thumbnail.png" alt="VSR video thumbnail" onclick="switchVideo('VSI','video2Container', 'video2Preview')">
                        </div>
                        
                        <!-- VSC Video -->
                        <div id="VSIvideo1Container" class="video-container">
                            <div class="video-flex">
                                <div class="video-area">
                                    <div class="video-label">VSC: Continual Counting</div>
                                    <video id="video_46" class="video-music" controls preload="metadata" playsinline>
                                        <source src="videos/video_46_trimmed_357s_2x.mp4" type="video/mp4">
                                        Your browser does not support the video tag.
                                    </video>
                                    <div class="speed-reminder">
                                        <i class="fas fa-info-circle"></i>This video is playing at 1.25x speed. You can adjust the speed using the controls above.
                                    </div>
                                </div>
                                <div class="video-qa">
                                    <div class="qa-item">
                                        <p><strong>Question:</strong> How many different chair(s) are there in the video?</p>
                                        <p><strong>Answer:</strong></p>
                                        <input type="text" id="chair-count-input" placeholder="Enter your answer" style="width: 100%; padding: 10px; margin-bottom: 10px; border: 2px solid #ccc; border-radius: 4px; font-size: 16px;">
                                        <div id="answer-1" style="display:none; text-align: center; margin-top: 20px;">
                                            <div style="display: flex; flex-direction: column; gap: 10px; align-items: center;">
                                                <p>
                                                    <strong style="color: green;">Ground Truth:</strong> 4+4+9 = 17
                                                </p>
                                                <p>
                                                    <strong style="color: red;">Gemini 2.5 Pro:</strong> 11
                                                </p>
                                                <p>
                                                    <strong style="color: red;">Gemini 2.5 Flash:</strong> 9
                                                </p>
                                            </div>
                                        </div>
                                        <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-1', this)">
                                            <img src="static/img/icons/teaser.gif" style="width:1.5rem">
                                            <strong>Click to view Ground Truth and MLLM's answer!</strong>
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <!-- VSR Video -->
                        <div id="VSIvideo2Container" class="video-container" style="display:none;">
                            <div class="video-flex">
                                <div class="video-area">
                                    <div class="video-label">VSR: Spatial Recall</div>
                                    <video class="video-music" controls preload="metadata" playsinline>
                                        <source src="videos/sor_random_5min_2x.mp4" type="video/mp4">
                                        Your browser does not support the video tag.
                                    </video>
                                    <div class="speed-reminder">
                                        <i class="fas fa-info-circle"></i>This video is playing at 2x speed. You can adjust the speed using the controls above.
                                    </div>
                                </div>
                                <div class="video-qa">
                                    <div class="qa-item">
                                        <p><strong>Question:</strong> Which of the following correctly represents the order in which the white Ragdoll cat appeared in the video?</p>
                                        <p><strong>Options:</strong></p>
                                        <ul style="list-style-type: none; padding-left: 0; text-align: left; width: 100%; margin: 0;">
                                            <li style="margin-bottom: 10px;"><input type="radio" name="ragdoll_order" style="margin-right: 10px;"> A. Table, Windowsill, Chair, Shelf</li>
                                            <li style="margin-bottom: 10px;"><input type="radio" name="ragdoll_order" style="margin-right: 10px;"> B. Chair, Table, Windowsill, Shelf</li>
                                            <li style="margin-bottom: 10px;"><input type="radio" name="ragdoll_order" style="margin-right: 10px;"> C. Windowsill, Shelf, Table, Chair</li>
                                            <li style="margin-bottom: 10px;"><input type="radio" name="ragdoll_order" style="margin-right: 10px;"> D. Chair, Shelf, Table, Windowsill</li>
                                        </ul>  
                                        <div id="answer-2" style="display:none; text-align: center; margin-top: 20px;">
                                            <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
                                                <p>
                                                    <strong style="color: green;">Ground Truth:</strong> C
                                                </p>
                                            </div>
                                        </div>
                                        <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-2', this)">
                                            <img src="static/img/icons/teaser.gif" style="width:1.5rem">
                                            <strong>Click to view Ground Truth!</strong>
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>

                </div>
            
                <h3 class="text">How VSI-SUPER Challenges Current MLLM Paradigm?</h3>

                <div id="tab:gemini_results" style="display: flex; flex-direction: column; align-items: center; margin-top: 1.5rem; margin-bottom: 1.5rem;">
                    <div class="table-container">
                        <table class="data-table">
                            <thead>
                            <tr>
                                <th class="tb-hdr">Model</th>
                                <th class="tb-hdr">VideoMME</th>
                                <th class="tb-hdr">VideoMMMU</th>
                                <th class="tb-hdr">VSI-Bench</th>
                                <th colspan="2" class="tb-hdr">VSR</th>
                                <th colspan="2" class="tb-hdr">VSC</th>
                            </tr>
                            <tr>
                                <th></th>
                                <th></th>
                                <th></th>
                                <th></th>
                                <th>60 mins</th>
                                <th>120 mins</th>
                                <th>60 mins</th>
                                <th>120 mins</th>
                            </tr>
                            </thead>
                            <tbody>
                            <tr>
                                <td>Gemini-2.5-Flash</td>
                                <td style="background-color: #c8e6c9;">81.5</td>
                                <td style="background-color: #c8e6c9;">79.2</td>
                                <td style="background-color: #ffebeb;">45.7</td>
                                <td style="background-color: #ffcccb;">41.5</td>
                                <td style="background-color: #ff9999;">Out of Ctx.</td>
                                <td style="background-color: #ffcccb;">10.9</td>
                                <td style="background-color: #ff9999;">Out of Ctx.</td>
                            </tr>
                            </tbody>
                        </table>
                    </div>
                    <figcaption style="text-align: center; width: 140%;">
                        <strong>Table:</strong> s a state-of-the-art video understanding model with long-context capabilities, Gemini demonstrates strong performance on general video benchmarks but shows clear limitations towards spatial supersensing.
                    </figcaption>
                </div>

                <p class="text">
                    <strong>State-of-the-art models struggle on VSI-SUPER.</strong> As shown in the table above, the model reaches its context limit when handling two-hour videos, despite a context length of 1,048,576 tokens. This highlights the <em>open-ended</em> nature of video understanding, where continuous streams effectively require an "infinite-in, infinite-out" context and can grow arbitrarily long, suggesting that simply scaling up tokens, context length, or model size may not suffice. Though synthetic, our benchmark reflects a real challenge in spatial supersensing: humans effortlessly integrate and retain information from ongoing sensory experiences that unfold over hours or years, yet current models lack comparable mechanisms for sustained perception and memory. Gemini-2.5-Flash demonstrates strong performance on semantic-perception and linguistic-understanding-focused video benchmarks such as VideoMME, achieving around 80% accuracy. However, even for 60-minute videos in \vsisuper that fall well within its context window, performance on VSR and VSC remains limited: only 41.5 and 10.9, respectively. In addition, as shown in the figure below, the model's predicted object counts fail to scale with video length or the true number of objects, instead saturating at a small constant value, suggesting a lack of generalization in counting ability and a reliance on training distribution priors.
                </p>

                <d-figure>
                    <figure>
                        <img src="figs/count_pred_distribution_1x3_gemini_only.png" alt="Count prediction distribution analysis" class="pdf-figure" style="width: 100%; height: auto; border: none;" data-zoomable="" draggable="false">
                        <figcaption>
                            <strong>Figure:</strong> Visualization shows Gemini-2.5-Flash's failure to scale counting ability with the number of objects.
                        </figcaption>
                    </figure>
                </d-figure>

                <div style="margin: 2rem auto; max-width: auto; padding: 1.5rem; background-color: #f5f5f5; border: 2px solid #008080; border-radius: 12px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);">
                    <p style="margin: 0; color: #000; font-size: 1.1em; line-height: 1.6;">
                        <i class="fas fa-bookmark" style="color: #000; margin-right: 10px;"></i>
                        VSI-SUPER tasks challenge the belief that scaling alone guarantees progress.
                    </p>
                </div>

                <p class="text">
                    By allowing arbitrarily long video inputs that emulate the dynamics of streaming cognition, VSI-SUPER is intentionally constructed to exceed any fixed context window. This design suggests that frame-by-frame tokenization and processing are unlikely to be computationally viable as a long-term solution. Humans address such problems efficiently and adaptively by selectively attending to and retaining only a small fraction of sensory input, often unconsciously. This predictive and selective mechanism, core to human cognition, remains absent in current MLLMs but is fundamental to a predictive world model.
                </p>

                <div style="margin: 2rem auto; max-width: auto; padding: 1.5rem; background-color: #f5f5f5; border: 2px solid #008080; border-radius: 12px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);">
                    <p style="margin: 0; color: #000; font-size: 1.1em; line-height: 1.6;">
                        <i class="fas fa-bookmark" style="color: #000; margin-right: 10px;"></i>
                        VSI-SUPER tasks demand generalization to new temporal and spatial scales at test time.
                    </p>
                </div>

                <p class="text">
                    For example, VSC requires counting in arbitrarily long videos, similar to how humans, who understand the concept of counting, can extend it to any number. The key is not maintaining an extremely long context window, humans do not retain every visual detail from extended visual experiences, but rather learning the process of counting itself. Predictive sensing facilitates this by <em>segmenting</em> continuous visual streams into coherent events, using moments of "surprise" to impose temporal structure. This segmentation acts as a divide-and-conquer mechanism that allows the model to decide when to start, continue, or reset behaviors in dynamically changing scenes. 
                </p>

                <p class="text">
                    Together, these challenges, which span computational efficiency, generalization, and cognitive mechanisms such as unconscious inference and predictive sensing, call for a paradigm shift. Rather than relying solely on scaling data, parameters, or context length, future models should learn internal world models capable of perceiving and predicting within an endlessly unfolding visual world across space and time.
                </p>
            </section>

            <section id="cambrian-s">
                <h1 class="text">Pushing the Limits of Spatial Sensing in the Current Paradigm</h1>

                <p class="text">Frontier MLLM, Gemini-2.5-Flash, performs suboptimally on both spatial supersensing and spatial reasoning. This raises a question:
                    <em>is suboptimal spatial supersensing intrinsically bounded by a model's foundational spatial sensing abilities, and can it be addressed by data scaling?</em></p>
                <h3 class="text">VSI-590K: Is Limited Spatial Sensing Simply a Data Problem?</h3>

                <p class="text">
                    It is well recognized that data quality and diversity play a critical role in the training of MLLMs. We hypothesize that the performance gap on VSI-Bench comes mainly from the lack of high-quality, spatially grounded data in current instruction-tuning datasets. To fill this gap, we build VSI-590K, a large-scale instruction-tuning dataset designed to improve visual-spatial understanding.
                </p>

                <div id="tab:vsi_590k_stats" style="display: flex; flex-direction: column; align-items: center; margin-top: 1.5rem; margin-bottom: 1.5rem;">
                    <div class="table-container">
                        <table class="data-table">
                            <thead>
                            <tr>
                                <th class="tb-hdr">Dataset</th>
                                <th class="tb-hdr"># Videos</th>
                                <th class="tb-hdr"># Images</th>
                                <th class="tb-hdr"># QA Pairs</th>
                            </tr>
                            </thead>
                            <tbody>
                            <!-- Annotated Real Videos -->
                            <tr style="background-color: #f2f8fc;">
                                <td colspan="4" style="font-style: italic; font-weight: bold; padding: 8px; text-align: left;"><i>Annotated Real Videos</i></td>
                            </tr>
                            <tr>
                                <td>S3DIS</td>
                                <td>199</td>
                                <td>-</td>
                                <td>5,187</td>
                            </tr>
                            <tr>
                                <td>Aria Digital Twin </td>
                                <td>183</td>
                                <td>-</td>
                                <td>60,207</td>
                            </tr>
                            <tr>
                                <td>ScanNet</td>
                                <td>1,201</td>
                                <td>-</td>
                                <td>92,145</td>
                            </tr>
                            <tr>
                                <td>ScanNet++ V2</td>
                                <td>856</td>
                                <td>-</td>
                                <td>138,701</td>
                            </tr>
                            <tr>
                                <td style="border-bottom: 1px solid #666;">ARKitScenes</td>
                                <td style="border-bottom: 1px solid #666;">2,899</td>
                                <td style="border-bottom: 1px solid #666;">-</td>
                                <td style="border-bottom: 1px solid #666;">57,816</td>
                            </tr>
                            <!-- Simulated Data -->
                            <tr style="background-color: #f2f8fc;">
                                <td colspan="4" style="font-style: italic; font-weight: bold; padding: 8px; text-align: left;"><i>Simulated Data</i></td>
                            </tr>
                            <tr>
                                <td>ProcTHOR</td>
                                <td>625</td>
                                <td>-</td>
                                <td>20,092</td>
                            </tr>
                            <tr>
                                <td style="border-bottom: 1px solid #666;">Hypersim </td>
                                <td style="border-bottom: 1px solid #666;">-</td>
                                <td style="border-bottom: 1px solid #666;">5,113</td>
                                <td style="border-bottom: 1px solid #666;">176,774</td>
                            </tr>
                            <!-- Unannotated Real Videos -->
                            <tr style="background-color: #f2f8fc;">
                                <td colspan="4" style="font-style: italic; font-weight: bold; padding: 8px; text-align: left;"><i>Unannotated Real Videos</i></td>
                            </tr>
                            <tr>
                                <td>YouTube Room Tour</td>
                                <td>-</td>
                                <td>20,100</td>
                                <td>20,100</td>
                            </tr>
                            <tr>
                                <td>Open X-Embodiment</td>
                                <td>-</td>
                                <td>14,801</td>
                                <td>14,801</td>
                            </tr>
                            <tr>
                                <td style="border-bottom: 1px solid #666;">AgiBot-World</td>
                                <td style="border-bottom: 1px solid #666;">-</td>
                                <td style="border-bottom: 1px solid #666;">4,844</td>
                                <td style="border-bottom: 1px solid #666;">4,844</td>
                            </tr>
                            <!-- Total -->
                            <tr style="font-weight: bold;">
                                <td style="border-top: 2px solid #333;">Total</td>
                                <td style="border-top: 2px solid #333;">5,963</td>
                                <td style="border-top: 2px solid #333;">44,858</td>
                                <td style="border-top: 2px solid #333;">590,667</td>
                            </tr>
                            </tbody>
                        </table>
                    </div>
                    <figcaption style="text-align: center; width: 140%;">
                        <strong>Table:</strong> Statistics for VSI-590K. The curated data draws from 10 sources (with different modalities and types of annotations) to improve diversity.
                    </figcaption>
                </div>

                <h3 class="text">Cambrian-S: A Spatially-Grounded MLLM</h3>
                <p class="text">
                    <strong>Improved spatial cognition.</strong>
                    As shown in the table below, our method yields a new state-of-the-art in spatial reasoning. Cambrian-S-7B achieves a score of 67.5% on VSI-Bench, significantly outperforming all open-source models and even surpassing the proprietary Gemini-2.5-Pro by over 16 absolute points. Furthermore, our training recipe is highly effective even at smaller scales, with our 0.5B model achieving comparable results with Gemini-1.5 Pro on VSI-Bench. This focus on spatial skills does not compromise general capabilities, as Cambrian-S maintains competitive performance on standard video benchmarks such as Perception Test and EgoSchema.
                </p>

                <div id="tab:comparison_mllms" style="display: flex; flex-direction: column; align-items: center;">
                    <div class="table-container">
                        <table class="data-table">
                            <thead>
                            <tr>
                                <th rowspan="2" class="tb-hdr">Model</th>
                                <th rowspan="2" class="tb-hdr">Base LM</th>
                                <th colspan="9" class="tb-hdr">Video</th>
                                <th colspan="4" class="tb-hdr">Image</th>
                            </tr>
                            <tr>
                                <th>VSI-Bench</th>
                                <th>Tomato</th>
                                <th>HourVideo</th>
                                <th>Video<sup>MME</sup></th>
                                <th>EgoSchema</th>
                                <th>Video<sup>MMMU</sup></th>
                                <th>LongVBench</th>
                                <th>MVBench</th>
                                <th>Percept. Test</th>
                                <th>RWQA</th>
                                <th>3DSR</th>
                                <th>CV-Bench</th>
                            </tr>
                            </thead>
                            <tbody>
                            <!-- Proprietary Models -->
                            <tr style="background-color: #f2f8fc;">
                                <td colspan="15" style="font-style: italic; font-weight: bold; padding: 8px; text-align: left;"><i>Proprietary Models</i></td>
                            </tr>
                            <tr>
                                <td>Claude-3.5-sonnet</td>
                                <td>UNK.</td>
                                <td>-</td>
                                <td>27.8</td>
                                <td>-</td>
                                <td>62.9</td>
                                <td>-</td>
                                <td>65.8</td>
                                <td>-</td>
                                <td>-</td>
                                <td>-</td>
                                <td>51.9</td>
                                <td>48.2</td>
                                <td>-</td>
                            </tr>
                            <tr>
                                <td>GPT-4o</td>
                                <td>UNK.</td>
                                <td>34.0</td>
                                <td>37.7</td>
                                <td>37.2</td>
                                <td>71.9</td>
                                <td>-</td>
                                <td>61.2</td>
                                <td>66.7</td>
                                <td>-</td>
                                <td>-</td>
                                <td>-</td>
                                <td>44.2</td>
                                <td>-</td>
                            </tr>
                            <tr>
                                <td>Gemini-1.5-Pro</td>
                                <td>UNK.</td>
                                <td>48.8</td>
                                <td>36.1</td>
                                <td>37.3</td>
                                <td>75.0</td>
                                <td>72.2</td>
                                <td>53.9</td>
                                <td>64.0</td>
                                <td>-</td>
                                <td>-</td>
                                <td>67.5</td>
                                <td>-</td>
                                <td>-</td>
                            </tr>
                            <tr>
                                <td>Gemini-2.5 Pro</td>
                                <td>UNK.</td>
                                <td>51.5</td>
                                <td>-</td>
                                <td>-</td>
                                <td>-</td>
                                <td>-</td>
                                <td>83.6</td>
                                <td>67.4</td>
                                <td>-</td>
                                <td>-</td>
                                <td>-</td>
                                <td>-</td>
                                <td>-</td>
                            </tr>
                            <!-- Open-Source Models -->
                            <tr style="background-color: #f2f8fc">
                                <td colspan="15" style="font-style: italic; font-weight: bold; padding: 8px; text-align: left;"><i>Open-Source Models</i></td>
                            </tr>
                            <tr>
                                <td>LLaVA-Video-7B</td>
                                <td>Qwen2-7B</td>
                                <td>35.6</td>
                                <td>22.5</td>
                                <td>28.6</td>
                                <td>63.3</td>
                                <td>57.3</td>
                                <td>36.1</td>
                                <td>58.2</td>
                                <td>58.6</td>
                                <td>67.9</td>
                                <td>66.4</td>
                                <td>-</td>
                                <td>75.7</td>
                            </tr>
                            <tr>
                                <td>LLaVA-One-Vision-7B</td>
                                <td>Qwen2-7B</td>
                                <td>32.4</td>
                                <td>25.5</td>
                                <td>28.3</td>
                                <td>58.2</td>
                                <td>60.1</td>
                                <td>33.9</td>
                                <td>56.4</td>
                                <td>56.7</td>
                                <td>57.1</td>
                                <td>66.3</td>
                                <td>-</td>
                                <td>74.3</td>
                            </tr>
                            <tr>
                                <td>Qwen-VL-2.5-7B</td>
                                <td>Qwen2.5-7B</td>
                                <td>33.5</td>
                                <td>-</td>
                                <td>-</td>
                                <td>65.1</td>
                                <td>65.0</td>
                                <td>47.4</td>
                                <td>56.0</td>
                                <td>69.6</td>
                                <td>-</td>
                                <td>-</td>
                                <td>48.4</td>
                                <td>-</td>
                            </tr>
                            <tr>
                                <td>InternVL2.5-8B</td>
                                <td>InternLM2.5-7B</td>
                                <td>34.6</td>
                                <td>-</td>
                                <td>-</td>
                                <td>64.2</td>
                                <td>50.6</td>
                                <td>-</td>
                                <td>60.0</td>
                                <td>72.0</td>
                                <td>-</td>
                                <td>68.4</td>
                                <td>50.9</td>
                                <td>-</td>
                            </tr>
                            <tr>
                                <td>InternVL3.5-8B</td>
                                <td>Qwen3-8B</td>
                                <td>56.3</td>
                                <td>-</td>
                                <td>-</td>
                                <td>66.0</td>
                                <td>61.2</td>
                                <td>49.0</td>
                                <td>62.1</td>
                                <td>72.1</td>
                                <td>-</td>
                                <td>67.5</td>
                                <td>-</td>
                                <td>-</td>
                            </tr>
                            <tr class="highlight">
                                <td style="border-bottom: 1px solid #666;"><strong>Cambrian-S-7B</strong></td>
                                <td style="border-bottom: 1px solid #666;">Qwen2.5-7B</td>
                                <td style="border-bottom: 1px solid #666;"><strong>67.5</strong></td>
                                <td style="border-bottom: 1px solid #666;"><strong>27.0</strong></td>
                                <td style="border-bottom: 1px solid #666;"><strong>36.5</strong></td>
                                <td style="border-bottom: 1px solid #666;">63.4</td>
                                <td style="border-bottom: 1px solid #666;"><strong>76.8</strong></td>
                                <td style="border-bottom: 1px solid #666;">38.6</td>
                                <td style="border-bottom: 1px solid #666;">59.4</td>
                                <td style="border-bottom: 1px solid #666;">64.5</td>
                                <td style="border-bottom: 1px solid #666;"><strong>69.9</strong></td>
                                <td style="border-bottom: 1px solid #666;">64.8</td>
                                <td style="border-bottom: 1px solid #666;"><strong>54.8</strong></td>
                                <td style="border-bottom: 1px solid #666;"><strong>76.9</strong></td>
                            </tr>
                            <tr>
                                <td>VILA1.5-3B</td>
                                <td>Sheared-LLaMA-2.7B</td>
                                <td>-</td>
                                <td>-</td>
                                <td>-</td>
                                <td>42.2</td>
                                <td>-</td>
                                <td>-</td>
                                <td>42.9</td>
                                <td>-</td>
                                <td>49.1</td>
                                <td>-</td>
                                <td>-</td>
                                <td>-</td>
                            </tr>
                            <tr>
                                <td>Qwen2.5-VL-3B</td>
                                <td>Qwen2.5-3B</td>
                                <td>26.8</td>
                                <td>-</td>
                                <td>-</td>
                                <td>61.5</td>
                                <td>-</td>
                                <td>-</td>
                                <td>54.2</td>
                                <td>-</td>
                                <td>66.9</td>
                                <td>-</td>
                                <td>-</td>
                                <td>-</td>
                            </tr>
                            <tr class="highlight">
                                <td style="border-bottom: 1px solid #666;"><strong>Cambrian-S-3B</strong></td>
                                <td style="border-bottom: 1px solid #666;">Qwen2.5-3B</td>
                                <td style="border-bottom: 1px solid #666;"><strong>57.3</strong></td>
                                <td style="border-bottom: 1px solid #666;"><strong>25.4</strong></td>
                                <td style="border-bottom: 1px solid #666;"><strong>36.8</strong></td>
                                <td style="border-bottom: 1px solid #666;">60.2</td>
                                <td style="border-bottom: 1px solid #666;"><strong>73.5</strong></td>
                                <td style="border-bottom: 1px solid #666;"><strong>25.2</strong></td>
                                <td style="border-bottom: 1px solid #666;">52.3</td>
                                <td style="border-bottom: 1px solid #666;"><strong>60.2</strong></td>
                                <td style="border-bottom: 1px solid #666;">65.9</td>
                                <td style="border-bottom: 1px solid #666;"><strong>60.1</strong></td>
                                <td style="border-bottom: 1px solid #666;"><strong>50.9</strong></td>
                                <td style="border-bottom: 1px solid #666;"><strong>75.2</strong></td>
                            </tr>
                            <tr>
                                <td>SmolVLM2-2.2B</td>
                                <td>SmolLM2-1.7B</td>
                                <td>27.0</td>
                                <td>-</td>
                                <td>-</td>
                                <td>-</td>
                                <td>34.1</td>
                                <td>-</td>
                                <td>-</td>
                                <td>48.7</td>
                                <td>51.1</td>
                                <td>-</td>
                                <td>-</td>
                                <td>-</td>
                            </tr>
                            <tr>
                                <td>InternVL2.5-2B</td>
                                <td>InternLM2.5-1.8B</td>
                                <td>25.8</td>
                                <td>-</td>
                                <td>-</td>
                                <td>51.9</td>
                                <td>47.4</td>
                                <td>-</td>
                                <td>52.0</td>
                                <td>68.8</td>
                                <td>-</td>
                                <td>60.1</td>
                                <td>-</td>
                                <td>-</td>
                            </tr>
                            <tr>
                                <td>InternVL3.5-2B</td>
                                <td>Qwen3-1.7B</td>
                                <td>51.5</td>
                                <td>-</td>
                                <td>-</td>
                                <td>58.4</td>
                                <td>50.8</td>
                                <td>-</td>
                                <td>57.4</td>
                                <td>65.9</td>
                                <td>-</td>
                                <td>62.0</td>
                                <td>-</td>
                                <td>-</td>
                            </tr>
                            <tr class="highlight">
                                <td style="border-bottom: 1px solid #666;"><strong>Cambrian-S-1.5B</strong></td>
                                <td style="border-bottom: 1px solid #666;">Qwen2.5-1.5B</td>
                                <td style="border-bottom: 1px solid #666;"><strong>54.8</strong></td>
                                <td style="border-bottom: 1px solid #666;"><strong>22.5</strong></td>
                                <td style="border-bottom: 1px solid #666;"><strong>31.4</strong></td>
                                <td style="border-bottom: 1px solid #666;">55.6</td>
                                <td style="border-bottom: 1px solid #666;"><strong>68.8</strong></td>
                                <td style="border-bottom: 1px solid #666;"><strong>24.9</strong></td>
                                <td style="border-bottom: 1px solid #666;">50.0</td>
                                <td style="border-bottom: 1px solid #666;">58.1</td>
                                <td style="border-bottom: 1px solid #666;"><strong>63.2</strong></td>
                                <td style="border-bottom: 1px solid #666;">54.5</td>
                                <td style="border-bottom: 1px solid #666;"><strong>51.9</strong></td>
                                <td style="border-bottom: 1px solid #666;"><strong>69.6</strong></td>
                            </tr>
                            <tr>
                                <td>SmolVLM2-0.5B</td>
                                <td>SmolLM2-360M</td>
                                <td>26.1</td>
                                <td>-</td>
                                <td>-</td>
                                <td>-</td>
                                <td>20.3</td>
                                <td>-</td>
                                <td>-</td>
                                <td>43.7</td>
                                <td>44.8</td>
                                <td>-</td>
                                <td>-</td>
                                <td>-</td>
                            </tr>
                            <tr>
                                <td>LLaVA-One-Vision-0.5B</td>
                                <td>Qwen2-0.5B</td>
                                <td>28.5</td>
                                <td>-</td>
                                <td>-</td>
                                <td>44.0</td>
                                <td>26.8</td>
                                <td>-</td>
                                <td>45.8</td>
                                <td>45.5</td>
                                <td>49.2</td>
                                <td>55.6</td>
                                <td>-</td>
                                <td>55.5</td>
                            </tr>
                            <tr>
                                <td>InternVL2.5-1B</td>
                                <td>Qwen2.5-0.5B</td>
                                <td>22.5</td>
                                <td>-</td>
                                <td>-</td>
                                <td>50.3</td>
                                <td>39.8</td>
                                <td>-</td>
                                <td>47.9</td>
                                <td>64.3</td>
                                <td>-</td>
                                <td>58.1</td>
                                <td>-</td>
                                <td>-</td>
                            </tr>
                            <tr>
                                <td>InternVL3.5-1B</td>
                                <td>Qwen3-0.6B</td>
                                <td>49.9</td>
                                <td>-</td>
                                <td>-</td>
                                <td>51.0</td>
                                <td>41.5</td>
                                <td>33.0</td>
                                <td>53.0</td>
                                <td>61.0</td>
                                <td>-</td>
                                <td>57.6</td>
                                <td>-</td>
                                <td>-</td>
                            </tr>
                            <tr class="highlight">
                                <td style="border-bottom: 1px solid #666;"><strong>Cambrian-S-0.5B</strong></td>
                                <td style="border-bottom: 1px solid #666;">Qwen2.5-0.5B</td>
                                <td style="border-bottom: 1px solid #666;"><strong>50.6</strong></td>
                                <td style="border-bottom: 1px solid #666;"><strong>23.4</strong></td>
                                <td style="border-bottom: 1px solid #666;"><strong>27.9</strong></td>
                                <td style="border-bottom: 1px solid #666;">44.0</td>
                                <td style="border-bottom: 1px solid #666;"><strong>62.4</strong></td>
                                <td style="border-bottom: 1px solid #666;">15.7</td>
                                <td style="border-bottom: 1px solid #666;">44.0</td>
                                <td style="border-bottom: 1px solid #666;">51.8</td>
                                <td style="border-bottom: 1px solid #666;"><strong>56.0</strong></td>
                                <td style="border-bottom: 1px solid #666;">51.1</td>
                                <td style="border-bottom: 1px solid #666;"><strong>48.5</strong></td>
                                <td style="border-bottom: 1px solid #666;"><strong>59.8</strong></td>
                            </tr>
                            </tbody>
                        </table>
                    </div>
                    <figcaption style="text-align: center; width: 130%;">
                        <strong>Table:</strong> Comparison of Cambrian-S with other leading MLLMs. Cambrian-S leads against proprietary and open-sourced models on various image and video visual-spatial benchmarks.
                    </figcaption>
                </div>

                <div style="margin: 2rem auto; max-width: auto; padding: 1.5rem; background-color: #f5f5f5; border: 2px solid #008080; border-radius: 12px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);">
                    <p style="margin: 0; color: #000; font-size: 1.1em; line-height: 1.6;">
                        <i class="fas fa-bookmark" style="color: #000; margin-right: 10px;"></i>
                        Cambrian-S achieves state-of-the-art spatial sensing performance with robust generalization to unseen spatial question types, while staying competitive in general video understanding.
                    </p>
                </div>

                
                <p class="text">
                <strong>Results on VSI-SUPER: Limitations in Continual Spatial Sensing.</strong>
                Despite the success on tasks involving spatial reasoning in short and pre-segmented clips on VSI-Bench, Cambrian-S is ill-suited for the demands of continual sensing. This limitation is evident in two ways. First, performance collapses on long videos.
                As shown in the table below, on the VSI-SUPER benchmark, scores drop to zero for videos exceeding 30 minutes, as they fall outside the model's context window. Even within this window, performance on VSR steadily degrades from 38.3% to 6.0% as video length increases from 10 to 60 minutes, and the model fails completely on videos longer than 60 minutes.
                Second, the model has difficulty generalizing to new test scenarios. Although trained on multi-room house tour videos, it fails to handle unseen examples with just a few additional rooms. This issue isn't simply about context length: performance drops even on short 10-minute videos that fit comfortably within model's context window. These results highlight that a purely data-driven approach within the current MLLM framework, no matter how much data or engineering effort is invested, faces fundamental limits. Addressing these limitations calls for a paradigm shift toward AI systems that can actively model and anticipate the world while organizing their experiences more efficiently, which we explore next.
                </p>

                <div id="tab:cambrian_vsr_vsc" style="display: flex; flex-direction: column; align-items: center; margin-top: 1.5rem; margin-bottom: 1.5rem;">
                    <div class="table-container" style="max-width: 95vw;">
                        <table class="data-table" style="border-spacing: 8px; width: auto; min-width: 100%; font-size: 14px;">
                            <thead>
                            <tr>
                                <th class="tb-hdr" style="padding: 0.8em 1.2em;">Eval Setup</th>
                                <th colspan="5" class="tb-hdr" style="padding: 0.8em 1.2em;">VSR</th>
                                <th colspan="4" class="tb-hdr" style="padding: 0.8em 1.2em;">VSC</th>
                            </tr>
                            <tr>
                                <th style="padding: 0.6em 1.2em;"></th>
                                <th style="padding: 0.6em 1.2em;">10 min</th>
                                <th style="padding: 0.6em 1.2em;">30 min</th>
                                <th style="padding: 0.6em 1.2em;">60 min</th>
                                <th style="padding: 0.6em 1.2em;">120 min</th>
                                <th style="padding: 0.6em 1.2em;">240 min</th>
                                <th style="padding: 0.6em 1.2em;">10 mins</th>
                                <th style="padding: 0.6em 1.2em;">30 min</th>
                                <th style="padding: 0.6em 1.2em;">60 min</th>
                                <th style="padding: 0.6em 1.2em;">120 min</th>
                            </tr>
                            </thead>
                            <tbody>
                            <tr style="color: #999;">
                                <td style="padding: 0.8em 1.2em;">Uni. Sampling, 128F</td>
                                <td style="padding: 0.8em 1.2em;">26.7</td>
                                <td style="padding: 0.8em 1.2em;">21.7</td>
                                <td style="padding: 0.8em 1.2em;">23.3</td>
                                <td style="padding: 0.8em 1.2em;">30.0</td>
                                <td style="padding: 0.8em 1.2em;">28.2</td>
                                <td style="padding: 0.8em 1.2em;">16.0</td>
                                <td style="padding: 0.8em 1.2em;">0.0</td>
                                <td style="padding: 0.8em 1.2em;">0.0</td>
                                <td style="padding: 0.8em 1.2em;">0.0</td>
                            </tr>
                            <tr>
                                <td style="padding: 0.8em 1.2em;">FPS Sampling, 1FPS</td>
                                <td style="padding: 0.8em 1.2em;">38.3</td>
                                <td style="padding: 0.8em 1.2em;">35.0</td>
                                <td style="padding: 0.8em 1.2em;">6.0</td>
                                <td style="padding: 0.8em 1.2em;">0.0</td>
                                <td style="padding: 0.8em 1.2em;">0.0</td>
                                <td style="padding: 0.8em 1.2em;">0.6</td>
                                <td style="padding: 0.8em 1.2em;">0.0</td>
                                <td style="padding: 0.8em 1.2em;">0.0</td>
                                <td style="padding: 0.8em 1.2em;">0.0</td>
                            </tr>
                            </tbody>
                        </table>
                    </div>
                    <figcaption style="text-align: center; width: 100%;">
                        <strong>Table:</strong> Despite strong performance on VSI-Bench, accuracy
                        on VSR drops sharply from 38.3% (10 min) to 0.0% (>60 min), and VSC completely fails. 
                    </figcaption>
                </div>

                <div style="margin: 2rem auto; max-width: auto; padding: 1.5rem; background-color: #f5f5f5; border: 2px solid #008080; border-radius: 12px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);">
                    <p style="margin: 0; color: #000; font-size: 1.1em; line-height: 1.6;">
                        <i class="fas fa-bookmark" style="color: #000; margin-right: 10px;"></i>
                        Scaling data and models is essential, but alone it cannot unlock true spatial supersensing.
                    </p>
                </div>

            </section>

            <section id="predictive-sensing">
                <h2 class="text">Predictive Sensing as a New Paradigm</h2>
                <p class="text">
                    Performance of both Gemini-2.5-Flash and Cambrian-S drops sharply on VSI-SUPER, revealing a fundamental paradigm gap: scaling data and context alone is insufficient for supersensing.
                    We propose <em>predictive sensing</em> as a path forward, where models learn to anticipate their sensory input and construct internal world models to handle unbounded visual streams.
                    This design is inspired by theories of human cognition. Unlike current video multimodal models that tokenize and process entire data streams, human perception (and memory) is highly selective, retaining only a fraction of sensory input.
                    The brain continuously updates internal models to predict incoming stimuli, compressing or discarding predictable inputs that contribute no novel information.
                    In contrast, unexpected sensory information that violates predictions generates "surprise" and drives increased attention and memory encoding.
                    We prototype this concept via a self-supervised next-latent-frame prediction approach. The resulting prediction error serves as a control signal for two key capabilities: memory management to selectively retain important information, and event segmentation to partition unbounded streams into meaningful chunks. We demonstrate through two case studies on VSI-SUPER that this approach substantially outperforms strong long-context and streaming video model baselines.
                </p>
                
                <h3 class="text">Predictive Sensing via Latent Frame Prediction</h3>

                <p class="text"> We implement our predictive sensing paradigm through a lightweight, self-supervised module called the Latent Frame Prediction (LFP) head, which is trained jointly with the primary instruction-tuning objective. During inference, we leverage the trained LFP head to evaluate the "surprise" for every incoming visual sensory input (<em>a.k.a</em>, Violation-of-Expectation (VoE) paradigm). Specifically, during inference, video frames are fed into Cambrian-S at a constant sampling rate. Unless otherwise noted, the videos in the following experiments are sampled at 1 FPS before being input into the model.
                As the model receives incoming video frames, it continuously predicts the latent features of the next frame. We then measure the <em>cosine distance</em> between the model's prediction and the actual ground truth feature of that incoming frame. This distance serves as a quantitative measure of surprise: a larger value indicating a greater deviation from the model's learned expectations. This surprise score acts as a powerful, self-supervised guidance signal for the downstream tasks explored next.
                </p>

                <d-figure>
                    <figure>
                        <img src="figs/nfp_illustration_v8.png" alt="Latent frame prediction" class="pdf-figure" style="width: 100%; height: auto; border: none;" data-zoomable="" draggable="false">
                        <figcaption >
                            <strong>Figure:</strong> Latent frame prediction (LFP) architecture and training.
                        </figcaption>
                    </figure>
                </d-figure>
                
                <h3 class="text">Case Study I: Surprise-driven Memory Management System for VSI-SUPER Recall.</h3>

                <p class="text">
                    Most current MLLMs treat all video frames equally, storing every frame without selective compression or forgetting, which limits efficiency and scalability. In this case study, we explore augmenting MLLMs with a surprise-driven memory management framework to support continual spatial-sensing question answering over long-duration videos.
                    We show that through the surprise-guided compression, Cambrian-S maintains consistent accuracy and stable GPU memory footprints, independent of video length.
                </p>

                <p class="text">
                    Our memory management system dynamically compresses and consolidates visual streams based on the estimate of "surprise". As shown in the figure below (a), we encode incoming frames using sliding window attention with fixed window size. The latent frame prediction module then measures a "surprise level" and assigns it to each frame's KV caches. Frames with a surprise level below a predefined threshold undergo <em>2x</em> compression before being pushed into long-term memory. To maintain a stable GPU memory footprint, this long-term memory is constrained to a fixed size by a consolidation function that, once again, operates based on surprise: dropping or merging frames according to their surprise scores (see the figure below (b)). Finally, upon receiving a user query, the system retrieves the top-K most relevant frames from the long-term memory by calculating the cosine similarity between the query and the stored frame features (see the figure below (c)). 
                </p>

                <d-figure>
                    <figure>
                        <img src="figs/sor_memory_design_jy_v5.png" alt="Surprise-driven memory design" class="pdf-figure" style="width: 60%; height: auto; border: none;" data-zoomable="" draggable="false">
                        <figcaption >
                            <strong>Figure:</strong> Surprise-driven memory management for VSR.
                        </figcaption>
                    </figure>
                </d-figure>


                <p class="text">
                    We compare Cambrian-S with and without the surprise-based memory system, against two advanced proprietary models Gemini-1.5-Flash and Gemini-2.5-Flash, on the VSR benchmark. As shown in the figure below, Cambrian-S (w/ Mem.) outperforms Gemini-1.5-Flash and Cambrian-S (w/o Mem.) at all video lengths, demonstrating consistent and remarkable spatial sensing across video lengths. Although Gemini-2.5-Flash yields promising results for videos within an hour, it fails to process longer inputs. On par with Cambrian-S (w/ Mem.) remarkable performance, it maintains a stable GPU memory usage across different video lengths. This demonstrates that the unconscious surprise level inference effectively compresses the redundant data without losing critical information.
                </p>


                <d-figure>
                    <figure>
                        <img src="figs/sor_results.png" alt="VSR performance comparison" class="pdf-figure" style="width: 45%; height: auto; border: none;" data-zoomable="" draggable="false">
                        <figcaption >
                            <strong>Figure:</strong> VSR performance comparison across different video durations showing the effectiveness of surprise-driven memory management.
                        </figcaption>
                    </figure>
                </d-figure>

                <div style="margin: 2rem auto; max-width: auto; padding: 1.5rem; background-color: #f5f5f5; border: 2px solid #008080; border-radius: 12px; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);">
                    <p style="margin: 0; color: #000; font-size: 1.1em; line-height: 1.6;">
                        <i class="fas fa-bookmark" style="color: #000; margin-right: 10px;"></i>
                        Predictive sensing provides a more principled approach to modeling the spatiotemporal dynamics of video data than static similarity measures based on per-frame features.
                    </p>
                </div>

                <h3 class="text">Case Study II: Surprise-driven continual video segment for VSI-SUPER Count.</h3>
                <p class="text">
                    While VSR focuses on evaluating the long-term observation and recall abilities of MLLMs, a more challenging test of supersensing would involve testing a model's capacity to interpret its sensory input, navigate across varied environments, and perform cumulative, multihop reasoning. For example, the model might need to complete a task in one environment, move to another, and ultimately integrate information from all experiences to reach a final decision.
                </p>

                <p class="text">
                    In the VSI-SUPER Count benchmark, we segment videos using the prediction error to detect ''surprise'', using them as natural breakpoints to partition the video into manageable segments. This approach mirrors human problem-solving. For instance, a person counting objects in a large area would naturally handle one section at a time before aggregating the results. As shown in the figure below, the model continuously buffers low-surprise frames in short-term memory. Upon detecting a high-surprise frame, the buffer is summarized to create a segment answer and then cleared. This process repeats until the end of the video. Finally, the final answer is aggregated by all segment answers.
                </p>

                <d-figure>
                    <figure>
                        <img src="figs/mem_couting_design_jy_v3.png" alt="Memory counting design" class="pdf-figure" style="width: 50%; height: auto; border: none;" data-zoomable="" draggable="false">
                        <figcaption >
                            <strong>Figure:</strong> Memory counting design illustration.
                        </figcaption>
                    </figure>
                </d-figure>

                <p class="text">
                    As shown in the figure below, Gemini-1.5-Flash achieves nearly zero performance in VSC, demonstrating the difficulty of this task. Although Gemini-2.5-Flash yields much better results on 10-minute videos, its performance declines rapidly on longer videos. In contrast, he surprise-driven event segmentation approach used by Cambrian-S (w/ Surprise Seg) achieves higher and more stable performance across all video lengths. When the video is segmented using ground-truth scene transitions (ie, Cambrian-S w/ GT Seg), performance improves further, representing an approximate upper bound.
                </p>

                <d-figure>
                    <figure>
                        <img src="figs/count_results.png" alt="Count results analysis" class="pdf-figure" style="width: 45%; height: auto; border: none;" data-zoomable="" draggable="false">
                        <figcaption >
                            <strong>Figure:</strong> Count results analysis showing scaling behavior across different models.
                        </figcaption>
                    </figure>
                </d-figure>

                <h2 class="text">Conclusion</h2>
                <p class="text">
                    We highlight the importance of and propose a hierarchy for spatial supersensing capabilities in videos, arguing that achieving superintelligence requires AI systems to move beyond text-based knowledge and semantic perception, the current focus of most MLLMs, to also develop spatial cognition and predictive world models. To measure progress, we introduce VSI-SUPER and find that current MLLMs struggle with it. To test whether current progress is limited by data, we curate VSI-590K and train our spatially grounded MLLM, Cambrian-S, on it. Although Cambrian-S performs well on standard benchmarks, its results on VSI-SUPER reveal the limitations of the current MLLM paradigm. We prototype predictive sensing, using latent frame prediction and surprise estimation to handle unbounded visual streams. It improves Cambrian-S performance on VSI-SUPER and marks an early step toward spatial supersensing.
                </p>

                <p class="text">
                    <strong>Limitations.</strong>
                    Our goal is to present a conceptual framework that encourages the community to reconsider the importance of developing spatial supersensing. As a long-term research direction, our current benchmark, dataset, and model design remain limited in quality, scale, and generalizability, and the prototype serves only as a proof of concept. Future work should explore more diverse and embodied scenarios and build stronger connections with recent advances in vision, language, and world modeling.
                </p>
                
            </section>

        <div id="bibtex" style="position: relative; margin-top: 40px; margin-bottom: 0px; color: gray;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">BibTeX</h2>
            <!-- <p class="bibtex">
                @article{yang2024think,<br>
                    title={{towards spatial supersensing in videos}},<br>
                    author={tbd},<br>
                    year={2024},<br>
                    journal={arXiv preprint},<br>
                }
            </p> -->
            <pre><code>@article{yang2025cambrian,
    title={Cambrian-S: Towards Spatial Supersensing in Video},
    author={Yang, Shusheng and Yang, Jihan and Huang, Pinzhi and Brown, Ellis and Yang, Zihao and Yu, Yue and Tong, Shengbang and Zheng, Zihan and Xu, Yifan and Wang, Muhan and Lu, Daohan and Fergus, Rob and LeCun, Yann and Fei-Fei, Li and Xie, Saining},
    journal={arXiv preprint arXiv:2511.04670},
    year={2025}
    }</code></pre>
        </div>
        </d-article>
    </body>
</html>
</html>