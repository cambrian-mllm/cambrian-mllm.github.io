<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TPU Training Experiments | Cambrian</title>
    <link rel="icon" type="image/x-icon" href="../imgs/favicon.ico">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&family=Lora:ital,wght@0,400;0,600;1,400&display=swap');

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --text-primary: #111;
            --text-secondary: #555;
            --accent-color: #000;
            --link-color: #0066cc;
            --bg-main: #ffffff;
            --bg-secondary: #f9f9f9;
            --border-color: #eee;
            --code-bg: #f5f5f5;
        }

        body {
            font-family: 'Lora', serif; /* Use serif for body text like many academic blogs */
            line-height: 1.8;
            color: var(--text-primary);
            background-color: var(--bg-main);
            font-size: 19px;
            -webkit-font-smoothing: antialiased;
        }

        h1, h2, h3, h4, h5, h6, .sans-serif {
            font-family: 'Inter', sans-serif;
            color: #000;
        }

        .container {
            max-width: 740px;
            margin: 0 auto;
            padding: 0 24px;
        }

        /* Top Navigation */
        .site-nav {
            padding: 20px 0;
            border-bottom: 1px solid transparent;
            font-family: 'Inter', sans-serif;
            font-size: 15px;
            font-weight: 500;
            margin-bottom: 40px;
        }

        .site-nav a {
            color: #000;
            text-decoration: none;
            margin-right: 20px;
        }
        
        .site-nav .brand {
            font-weight: 700;
            letter-spacing: -0.02em;
        }

        /* Header */
        header {
            padding: 20px 0 40px;
        }

        header h1 {
            font-size: 3rem;
            font-weight: 800;
            margin-bottom: 16px;
            letter-spacing: -0.03em;
            line-height: 1.1;
        }

        .meta-info {
            font-family: 'Inter', sans-serif;
            color: var(--text-secondary);
            font-size: 15px;
            margin-bottom: 30px;
            display: flex;
            gap: 8px;
        }

        /* Content Layout */
        .content-wrapper {
            display: flex;
            gap: 60px;
            margin-bottom: 80px;
            position: relative;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 200px;
            flex-shrink: 0;
            display: none; /* Hidden on mobile by default */
        }

        @media (min-width: 1024px) {
            .toc-sidebar {
                display: block;
            }
            
            .container {
                max-width: 1000px; /* Increased to accommodate sidebar */
            }
        }

        .toc-sticky {
            position: sticky;
            top: 40px;
        }

        .toc-title {
            font-family: 'Inter', sans-serif;
            font-size: 12px;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-secondary);
            margin-bottom: 16px;
            font-weight: 600;
        }

        .toc-list {
            list-style: none;
            font-family: 'Inter', sans-serif;
            font-size: 14px;
        }

        .toc-list li {
            margin-bottom: 10px;
        }

        .toc-list a {
            color: var(--text-secondary);
            text-decoration: none;
            transition: color 0.2s;
            display: block;
            line-height: 1.4;
        }

        .toc-list a:hover {
            color: var(--accent-color);
        }

        /* Article content adjustments */
        article {
            flex-grow: 1;
            max-width: 700px; /* Keep reading width optimal */
            min-width: 0; /* Prevent flex overflow */
            margin-bottom: 0;
        }

        article h2 {
            font-size: 1.8rem;
            font-weight: 700;
            margin-top: 50px;
            margin-bottom: 20px;
            letter-spacing: -0.02em;
        }

        article h3 {
            font-size: 1.4rem;
            font-weight: 600;
            margin-top: 40px;
            margin-bottom: 16px;
            letter-spacing: -0.01em;
        }

        article p {
            margin-bottom: 24px;
            font-weight: 400;
        }

        article a {
            color: var(--link-color);
            text-decoration: none;
        }

        article a:hover {
            text-decoration: underline;
        }

        /* Code Blocks - Light theme like Thinking Machines */
        code {
            background: rgba(0, 0, 0, 0.05);
            padding: 2px 4px;
            border-radius: 3px;
            font-family: 'SF Mono', 'Monaco', 'Inconsolata', 'Fira Code', 'Courier New', monospace;
            font-size: 0.85em;
        }

        pre {
            background: var(--code-bg);
            padding: 20px;
            border-radius: 4px;
            overflow-x: auto;
            margin: 30px 0;
            border: 1px solid var(--border-color);
        }

        pre code {
            background: none;
            padding: 0;
            font-size: 14px;
            color: #333;
        }

        /* Math */
        .katex-display {
            margin: 30px 0;
            overflow-x: auto;
            overflow-y: hidden;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 30px 0;
            font-family: 'Inter', sans-serif;
            font-size: 15px;
        }

        th {
            text-align: left;
            border-bottom: 2px solid #ddd;
            padding: 10px;
            font-weight: 600;
        }

        td {
            border-bottom: 1px solid #eee;
            padding: 10px;
        }

        /* Citation Box */
        .citation-section {
            margin-top: 60px;
            padding-top: 30px;
            border-top: 1px solid var(--border-color);
        }

        .citation-box {
            background: #f4f4f4;
            padding: 20px;
            font-family: 'SF Mono', 'Monaco', monospace;
            font-size: 13px;
            overflow-x: auto;
            white-space: pre;
            border-radius: 4px;
            margin-top: 10px;
            color: #444;
        }

        /* Footer */
        .site-footer {
            border-top: 1px solid var(--border-color);
            padding: 30px 0;
            font-family: 'Inter', sans-serif;
            font-size: 14px;
            color: var(--text-secondary);
            display: flex;
            justify-content: space-between;
        }
        
        .footer-links a {
            color: var(--text-secondary);
            margin-left: 15px;
            text-decoration: none;
        }

        @media (max-width: 768px) {
            header h1 { font-size: 2.2rem; }
            body { font-size: 18px; }
        }
    </style>
</head>
<body>
    <div class="container">
        <nav class="site-nav">
            <a href="../index.html" class="brand">Cambrian World</a>
        </nav>

        <header>
            <h1>Lessons from Two Years of TPU Training in Academia</h1>
            <div class="meta-info">
                Dec 2025
            </div>
        </header>

        <div class="content-wrapper">
            <aside class="toc-sidebar">
                <div class="toc-sticky">
                    <div class="toc-title">Contents</div>
                    <ul class="toc-list">
                        <li><a href="#why-tpus">Why TPUs?</a></li>
                        <li><a href="#why-torchxla">Why TorchXLA?</a></li>
                        <li><a href="#transition">From Torch To TorchXLA</a></li>
                        <li><a href="#distributed-strategies">Distributed Strategies</a></li>
                        <li><a href="#portability">Does LLM Code Work?</a></li>
                        <li><a href="#data-bottleneck">The Data Bottleneck</a></li>
                        <li><a href="#citation">Citation</a></li>
                    </ul>
                </div>
            </aside>

            <article>
                <figure style="margin-top: 0; margin-bottom: 40px;">
                    <img src="assets/images/teaser.png" alt="A comic showing two researchers awake at 5 AM debugging TPU errors" style="width: 100%; border-radius: 8px; border: 1px solid #eee;">
                    <figcaption>The shared reality of TPU debugging: 5 AM messages and shared pain.</figcaption>
                </figure>

                <p>
                    Training large foundation models in academia presents a unique set of challenges. Unlike industry labs with 
                    dedicated infrastructure teams and infinite compute, academic research often requires us to be our own 
                    DevOps engineers, kernel optimizers, and distributed systems architects.
                </p>

                <p>
                    Over the past two years, we've spent thousands of hours debugging, profiling, and optimizing training 
                    runs on TPUs (v3 through v6e). This post distills the hard-earned lessons from that journey.
                </p>

                <h2 id="why-tpus">Why TPUs?</h2>
                <p>
                    With the recent developments in Generative AI (LLMs, diffusion models, etc.), it is getting more and more expensive to do 
                    impactful research. Training state-of-the-art models often requires massive amounts of compute, but in academia, 
                    most students simply cannot access that scale of hardware.
                </p>
                <p>
                    However, we still want to do good research and train large models. This is where the 
                    <a href="https://sites.research.google/trc/about/" target="_blank">TPU Research Cloud (TRC)</a> program comes in. 
                    We want to sincerely thank TRC for sponsoring free Cloud TPUs, which has empowered us and many other students 
                    to tackle ambitious research problems that would otherwise be impossible.
                </p>

                <h2 id="why-torchxla">Why TorchXLA?</h2>
                <p>
                    [Why stick with PyTorch instead of moving to JAX? The ecosystem, existing codebase, or familiarity? Discuss the trade-offs of using the XLA compiler backend with PyTorch semantics.]
                </p>

                <h2 id="transition">From Torch To TorchXLA: The Transition</h2>
                <p>
                    Migrating a standard PyTorch training loop to run efficiently on TPUs isn't just a one-line change. It requires understanding how XLA compiles your graph.
                </p>

                <h3>Static Shape! Static Shape! Static Shape!</h3>
                <p>
                    If there is one mantra to memorize before touching a TPU, it is this: <strong>Dynamic shapes are the enemy.</strong>
                </p>
                <p>
                    PyTorch allows us to pass tensors of varying sizes 
                    through our model, and CUDA kernels handle it gracefully. On TPUs, however, the execution model is different. 
                    The XLA compiler traces your computation graph and compiles it into a static executable. 
                </p>
                <p>
                    Every time the input shape changes, XLA must <strong>recompile</strong> the entire graph. This recompilation 
                    can take minutes. If your batch size or sequence length changes every step, your training will spend 99% of its 
                    time compiling and 1% computing.
                </p>

                <h4>A Simple Example</h4>
                <p>
                    Consider a simple function that processes a batch of sentences. In standard PyTorch, we might just pass 
                    the list of sentences directly.
                </p>
                
                <pre><code class="language-python"># ❌ BAD: Dynamic Shapes
# If 'batch' has different lengths each time, XLA recompiles every step!
def forward(batch_of_tokens):
    # shape: [B, dynamic_seq_len]
    return model(batch_of_tokens)</code></pre>

                <p>
                    To fix this, we must ensure every tensor has a fixed, known size at compile time. This usually means 
                    padding everything to a maximum length.
                </p>

                <pre><code class="language-python"># ✅ GOOD: Static Shapes
# Pad everything to a fixed max_length (e.g., 2048)
def forward(batch_of_tokens_padded):
    # shape: [B, 2048] <- Always the same!
    return model(batch_of_tokens_padded)</code></pre>

                <h4>A Real-World Nightmare: The Cambrian Experience</h4>
                <p>
                    When we started developing <a href="https://cambrian-mllm.github.io/">Cambrian-1</a>, we spent the first month struggling to adapt the LLaVA codebase to run on TPUs. The symptoms were baffling: training would start, but it was excruciatingly slow. We were seeing step times measured in minutes rather than seconds.
                </p>
                <p>
                    The demon hiding in the details was <strong>dynamic shapes</strong>.
                </p>
                <p>
                    In multimodal LLM training, data is naturally "ragged." One sample might be a text-only conversation (0 images). The next might describe a single photo (1 image). Another might compare three different charts (3 images).
                </p>
                <p>
                    In standard PyTorch/CUDA, you just loop through the images you have. But on TPU, this variation is catastrophic:
                </p>
                <ul>
                    <li><strong>Batch 1:</strong> Max 2 images → Shape <code>[B, 2, C, H, W]</code> → <em>Compile!</em></li>
                    <li><strong>Batch 2:</strong> Max 5 images → Shape <code>[B, 5, C, H, W]</code> → <em>Recompile!</em></li>
                    <li><strong>Batch 3:</strong> Text only → Shape <code>[B, 0]</code> → <em>Recompile!</em></li>
                </ul>
                <p>
                    The XLA compiler was recompiling the vision encoder and projector for nearly every single batch.
                </p>
                
                <h4>The Solution: Padding with Dummy Images</h4>
                <p>
                    To fix this, we had to standardize the data shape at the dataloader level. We defined a fixed "max images" budget (e.g., 5 images per sample).
                </p>
                <p>
                    If a sample has fewer images, we pad it with <strong>dummy black images</strong> up to the max count. We then use a boolean mask to ensure the model ignores these dummy images during the forward pass.
                </p>

                <pre><code class="language-python"># Simplified fix for static multimodal batches
MAX_IMAGES = 5

def collate_fn(batch):
    # 1. Pad image tensors to [B, MAX_IMAGES, C, H, W]
    # 2. Create an attention mask for valid images
    
    padded_images = torch.zeros(batch_size, MAX_IMAGES, 3, 336, 336)
    image_masks = torch.zeros(batch_size, MAX_IMAGES, dtype=torch.bool)
    
    for i, sample in enumerate(batch):
        n_imgs = len(sample['images'])
        # Fill valid images
        padded_images[i, :n_imgs] = sample['images']
        image_masks[i, :n_imgs] = True
        # Remaining slots are zeros (dummy images)
        
    return padded_images, image_masks</code></pre>
                
                <p>
                    Once we implemented this, our step times dropped instantly from minutes to milliseconds. The graph compiled once, and the TPU could finally fly.
                </p>

                <h2 id="distributed-strategies">Distributed Training Strategies</h2>
                <p>
                    [Discuss the evolution of parallelism on TPUs:
                    <ul>
                        <li><strong>DDP (Distributed Data Parallel):</strong> The starting point.</li>
                        <li><strong>FSDP (Fully Sharded Data Parallel):</strong> Scaling model size.</li>
                        <li><strong>SPMD (Single Program Multiple Data):</strong> The GSPMD approach and advanced sharding.</li>
                    </ul>
                    Compare their complexity vs. performance gains.]
                </p>

                <h2 id="portability">Can LLMs Write TPU Code?</h2>
                <p>
                    For many researchers, "vibe-coding" or LLM-assisted coding has become a de-facto practice when starting new projects. 
                    But do LLMs actually understand the nuances of TPU and TorchXLA development? Over the past two years, especially with 
                    the introduction of reasoning models, our experience has been a mix of impressive capabilities and frustrating hallucinations.
                </p>

                <h3>What Works Well</h3>
                <p>
                    <ul>
                        <li>
                            <strong>Infrastructure Management:</strong> LLMs have mastered most TPU CLI commands. Since many TRC TPUs are preemptible, 
                            having LLMs write scripts to create, inspect, and auto-restart TPUs is a huge time-saver.
                        </li>
                    </ul>
                </p>

                <h3>What Breaks & How to Fix It</h3>
                <p>
                    <ul>
                        <li>
                            <strong>Outdated TPU APIs.</strong> TPU commands change frequently. LLMs are often stuck on older versions, leading to code that is either deprecated or pure hallucination.
                            <br><em>Fix:</em> When prompting, explicitly paste the latest API documentation (or web link) or error logs.
                        </li>
                        <br>
                        <li>
                            <strong>TorchXLA Codes.</strong> TorchXLA looks deceptively similar to standard PyTorch. LLMs often default to writing standard PyTorch code (like using `.cuda()` or code with dynamic shapes) that is catastrophic for XLA performance. 
                            TorchXLA APIs also change quite frequently, so LLMs generation can be full of hallucinations. Overall this makes TorchXLA codes very fragile and hard to maintain with LLMs. 
                            <br><em>Fix:</em> Unfortunately there is no good way to fix this. On the bright side, we have been (forced) to learn about the fundamentals of many code, like sharding, model resuming, etc. This is a motivation to 
                            learn more about the fundamentals of the code. Some tips that might help: try using the best model, turn the max reasoning and tell the model explicitly this is TPU codee, TorchXLA code, please reason your best instead of relying on pre-existing knowledge. 
                        </li>
                    </ul>
                </p>

                <h2 id="data-bottleneck">The Data Bottleneck</h2>
                <p>
                    [Compute is often not the bottleneck—data is.
                    <ul>
                        <li><strong>Storage:</strong> Using GCS buckets effectively.</li>
                        <li><strong>Transfer:</strong> The latency of pulling data to TPU VMs.</li>
                        <li><strong>Loading:</strong> CPU bottlenecks in data preprocessing/tokenization while TPUs wait.</li>
                    </ul>
                    Tips on pre-tokenizing and efficient data streaming.]
                </p>

                <h2 id="conclusion">Conclusion</h2>
                <p>
                    Despite the initial friction, mastering the TPU stack has allowed us to train models like Cambrian-1 
                    at a scale that competes with industry baselines. The ecosystem is maturing, and the gap between 
                    "research code" and "production training" is narrowing.
                </p>

                <div class="citation-section" id="citation">
                    <h3 class="sans-serif" style="font-size: 1.2rem; margin-top: 0;">Citation</h3>
                    <p style="font-size: 0.95rem; color: #666; margin-bottom: 10px;">Please cite this work as:</p>
                    
                <div class="citation-box">"Lessons from Two Years of TPU Training in Academia",
Cambrian Blog, Dec 2025.</div>

                <div class="citation-box">@article{cambrian2025tpu,
  title = {Lessons from Two Years of TPU Training in Academia},
  journal = {Cambrian Blog},
  year = {2025},
  note = {https://cambrian-mllm.github.io/blog/tpu-training-experiments.html}
}</div>
                </div>
            </article>
        </div>

        <footer class="site-footer">
            <div>Cambrian Lab © 2025</div>
            <div class="footer-links">
                <a href="#">Terms of service</a>
                <a href="#">Privacy notice</a>
            </div>
        </footer>
    </div>
</body>
</html>

