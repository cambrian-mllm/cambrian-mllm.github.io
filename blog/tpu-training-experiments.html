<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TPU Training Experiments | Cambrian</title>
    <link rel="icon" type="image/x-icon" href="../imgs/favicon.ico">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&family=Lora:ital,wght@0,400;0,600;1,400&display=swap');

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --text-primary: #111;
            --text-secondary: #555;
            --accent-color: #000;
            --link-color: #0066cc;
            --bg-main: #ffffff;
            --bg-secondary: #f9f9f9;
            --border-color: #eee;
            --code-bg: #f5f5f5;
        }

        body {
            font-family: 'Lora', serif; /* Use serif for body text like many academic blogs */
            line-height: 1.8;
            color: var(--text-primary);
            background-color: var(--bg-main);
            font-size: 19px;
            -webkit-font-smoothing: antialiased;
        }

        h1, h2, h3, h4, h5, h6, .sans-serif {
            font-family: 'Inter', sans-serif;
            color: #000;
        }

        .container {
            max-width: 740px;
            margin: 0 auto;
            padding: 0 24px;
        }

        /* Top Navigation */
        .site-nav {
            padding: 20px 0;
            border-bottom: 1px solid transparent;
            font-family: 'Inter', sans-serif;
            font-size: 15px;
            font-weight: 500;
            margin-bottom: 40px;
        }

        .site-nav a {
            color: #000;
            text-decoration: none;
            margin-right: 20px;
        }
        
        .site-nav .brand {
            font-weight: 700;
            letter-spacing: -0.02em;
        }

        /* Header */
        header {
            padding: 20px 0 40px;
        }

        header h1 {
            font-size: 3rem;
            font-weight: 800;
            margin-bottom: 16px;
            letter-spacing: -0.03em;
            line-height: 1.1;
        }

        .meta-info {
            font-family: 'Inter', sans-serif;
            color: var(--text-secondary);
            font-size: 15px;
            margin-bottom: 30px;
            display: flex;
            gap: 8px;
        }

        /* Content Layout */
        .content-wrapper {
            display: flex;
            gap: 60px;
            margin-bottom: 80px;
            position: relative;
        }

        /* Sidebar TOC */
        .toc-sidebar {
            width: 200px;
            flex-shrink: 0;
            display: none; /* Hidden on mobile by default */
        }

        @media (min-width: 1024px) {
            .toc-sidebar {
                display: block;
            }
            
            .container {
                max-width: 1000px; /* Increased to accommodate sidebar */
            }
        }

        .toc-sticky {
            position: sticky;
            top: 40px;
        }

        .toc-title {
            font-family: 'Inter', sans-serif;
            font-size: 12px;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-secondary);
            margin-bottom: 16px;
            font-weight: 600;
        }

        .toc-list {
            list-style: none;
            font-family: 'Inter', sans-serif;
            font-size: 14px;
        }

        .toc-list li {
            margin-bottom: 10px;
        }

        .toc-list a {
            color: var(--text-secondary);
            text-decoration: none;
            transition: color 0.2s;
            display: block;
            line-height: 1.4;
        }

        .toc-list a:hover {
            color: var(--accent-color);
        }

        /* Article content adjustments */
        article {
            flex-grow: 1;
            max-width: 700px; /* Keep reading width optimal */
            min-width: 0; /* Prevent flex overflow */
            margin-bottom: 0;
        }

        article h2 {
            font-size: 1.8rem;
            font-weight: 700;
            margin-top: 50px;
            margin-bottom: 20px;
            letter-spacing: -0.02em;
        }

        article h3 {
            font-size: 1.4rem;
            font-weight: 600;
            margin-top: 40px;
            margin-bottom: 16px;
            letter-spacing: -0.01em;
        }

        article p {
            margin-bottom: 24px;
            font-weight: 400;
        }

        article a {
            color: var(--link-color);
            text-decoration: none;
        }

        article a:hover {
            text-decoration: underline;
        }

        /* Code Blocks - Light theme like Thinking Machines */
        code {
            background: rgba(0, 0, 0, 0.05);
            padding: 2px 4px;
            border-radius: 3px;
            font-family: 'SF Mono', 'Monaco', 'Inconsolata', 'Fira Code', 'Courier New', monospace;
            font-size: 0.85em;
        }

        pre {
            background: var(--code-bg);
            padding: 20px;
            border-radius: 4px;
            overflow-x: auto;
            margin: 30px 0;
            border: 1px solid var(--border-color);
        }

        pre code {
            background: none;
            padding: 0;
            font-size: 14px;
            color: #333;
        }

        /* Math */
        .katex-display {
            margin: 30px 0;
            overflow-x: auto;
            overflow-y: hidden;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 30px 0;
            font-family: 'Inter', sans-serif;
            font-size: 15px;
        }

        th {
            text-align: left;
            border-bottom: 2px solid #ddd;
            padding: 10px;
            font-weight: 600;
        }

        td {
            border-bottom: 1px solid #eee;
            padding: 10px;
        }

        /* Citation Box */
        .citation-section {
            margin-top: 60px;
            padding-top: 30px;
            border-top: 1px solid var(--border-color);
        }

        .citation-box {
            background: #f4f4f4;
            padding: 20px;
            font-family: 'SF Mono', 'Monaco', monospace;
            font-size: 13px;
            overflow-x: auto;
            white-space: pre;
            border-radius: 4px;
            margin-top: 10px;
            color: #444;
        }

        /* Footer */
        .site-footer {
            border-top: 1px solid var(--border-color);
            padding: 30px 0;
            font-family: 'Inter', sans-serif;
            font-size: 14px;
            color: var(--text-secondary);
            display: flex;
            justify-content: space-between;
        }
        
        .footer-links a {
            color: var(--text-secondary);
            margin-left: 15px;
            text-decoration: none;
        }

        @media (max-width: 768px) {
            header h1 { font-size: 2.2rem; }
            body { font-size: 18px; }
        }
    </style>
</head>
<body>
    <div class="container">
        <nav class="site-nav">
            <a href="../index.html" class="brand">Cambrian World</a>
        </nav>

        <header>
            <h1>Lessons from Two Years of TPU Training in Academia</h1>
            <div class="meta-info">
                Shengbang Tong, Boyang Zheng ¬∑ Jan 2026
            </div>
        </header>

        <div class="content-wrapper">
            <aside class="toc-sidebar">
                <div class="toc-sticky">
                    <div class="toc-title">Contents</div>
                    <ul class="toc-list">
                        <li><a href="#why-tpus">Why TPUs?</a></li>
                        <li><a href="#why-torchxla">Why TorchXLA?</a></li>
                        <li><a href="#transition">From Torch To TorchXLA</a></li>
                        <li><a href="#distributed-strategies">Distributed Strategies</a></li>
                        <li><a href="#portability">Does LLM Code Work?</a></li>
                        <li><a href="#data-bottleneck">The Data Bottleneck</a></li>
                        <li><a href="#citation">Citation</a></li>
                    </ul>
                </div>
            </aside>

            <article>
                <figure style="margin-top: 0; margin-bottom: 40px;">
                    <img src="assets/images/teaser.png" alt="A comic showing two researchers awake at 5 AM debugging TPU errors" style="width: 100%; border-radius: 8px; border: 1px solid #eee;">
                    <figcaption>The shared reality of TPU debugging: 5 AM messages and shared pain.</figcaption>
                </figure>

                <p>
                    Training large foundation models in academia presents a unique set of challenges. Unlike industry labs with 
                    dedicated infrastructure teams and infinite compute, academic research often requires us to be our own 
                    DevOps engineers, kernel optimizers, and distributed systems architects.
                </p>

                <p>
                    Over the past two years, we've spent thousands of hours debugging, profiling, and optimizing training 
                    runs on TPUs (v3 through v6e). This post distills the hard-earned lessons from that journey.
                </p>

                <h2 id="why-tpus">Why TPUs?</h2>
                <p>
                    With the recent developments in Generative AI (LLMs, diffusion models, etc.), it is getting more and more expensive to do 
                    impactful research. Training state-of-the-art models often requires massive amounts of compute, but in academia, 
                    most students simply cannot access that scale of hardware.
                </p>
                <p>
                    However, we still want to do good research and train large models. This is where the 
                    <a href="https://sites.research.google/trc/about/" target="_blank">TPU Research Cloud (TRC)</a> program comes in. 
                    We want to sincerely thank TRC for sponsoring free Cloud TPUs, which has empowered us and many other students 
                    to tackle ambitious research problems that would otherwise be impossible.
                </p>

                <h2 id="why-torchxla">Why TorchXLA?</h2>
                <p>
                    When we first got access to TPUs, a natural question arose: should we rewrite everything in JAX?
                </p>
                <p>
                    The answer, for us, was a pragmatic <strong>no</strong>. We're almost always 
                    building on top of existing work. The model architectures we wanted to study, the training recipes we wanted to adapt, 
                    the community codebases we wanted to extend‚Äînearly all of them were written in PyTorch. Rewriting a complex codebase 
                    from scratch in a new framework is a months-long endeavor with a high risk of introducing subtle bugs.
                </p>
                <p>
                    <a href="https://github.com/pytorch/xla" target="_blank">TorchXLA</a> offered us the best of both worlds: 
                    the ability to keep our familiar PyTorch code while compiling it to run efficiently on TPU hardware via the XLA compiler. 
                </p>
                <p>
                    Of course, the reality is more nuanced (as you'll see in the next section), but the barrier to entry is 
                 lower than a full framework migration. We could take a working LLaVA training script, 
                    make targeted modifications, and have it running on a TPU pod within weeks rather than months.
                </p>


                <h2 id="transition">From Torch To TorchXLA: The Transition</h2>
                <p>
                    Migrating a standard PyTorch training loop to run efficiently on TPUs isn't just a one-line change. It requires understanding how XLA compiles your graph.
                </p>

                <h3>Static Shape! Static Shape! Static Shape!</h3>
                <p>
                    If there is one mantra to memorize before touching a TPU, it is this: <strong>Dynamic shapes are the enemy.</strong>
                </p>
                <p>
                    PyTorch allows us to pass tensors of varying sizes 
                    through our model, and CUDA kernels handle it gracefully. On TPUs, however, the execution model is different. 
                    The XLA compiler traces your computation graph and compiles it into a static executable. 
                </p>
                <p>
                    Every time the input shape changes, XLA must <strong>recompile</strong> the entire graph. This recompilation 
                    can take minutes. If your batch size or sequence length changes every step, your training will spend 99% of its 
                    time compiling and 1% computing.
                </p>

                <h4>A Simple Example</h4>
                <p>
                    Consider a simple function that processes a batch of sentences. In standard PyTorch, we might just pass 
                    the list of sentences directly.
                </p>
                
                <pre><code class="language-python"># ‚ùå BAD: Dynamic Shapes
# If 'batch' has different lengths each time, XLA recompiles every step!
def forward(batch_of_tokens):
    # shape: [B, dynamic_seq_len]
    return model(batch_of_tokens)</code></pre>

                <p>
                    To fix this, we must ensure every tensor has a fixed, known size at compile time. This usually means 
                    padding everything to a maximum length.
                </p>

                <pre><code class="language-python"># ‚úÖ GOOD: Static Shapes
# Pad everything to a fixed max_length (e.g., 2048)
def forward(batch_of_tokens_padded):
    # shape: [B, 2048] <- Always the same!
    return model(batch_of_tokens_padded)</code></pre>

                <h4>A Real-World Example: The Cambrian Experience</h4>
                <p>
                    When we started developing <a href="https://cambrian-mllm.github.io/">Cambrian-1</a>, we adapted the LLaVA codebase to run on TPUs. Initially, training would start, but it was much slower than expected. We were seeing step times measured in minutes rather than seconds.
                </p>
                <p>
                    The demon hiding in the details was <strong>dynamic shapes</strong>.
                </p>
                <p>
                    In multimodal LLM training, data is naturally "ragged." One sample might be a text-only conversation (0 images). The next might describe a single photo (1 image). Another might compare three different charts (3 images).
                </p>
                <p>
                    In standard PyTorch/CUDA, you just loop through the images you have. But on TPU, this variation is catastrophic:
                </p>
                <ul>
                    <li><strong>Batch 1:</strong> Max 2 images ‚Üí Shape <code>[B, 2, C, H, W]</code> ‚Üí <em>Compile!</em></li>
                    <li><strong>Batch 2:</strong> Max 5 images ‚Üí Shape <code>[B, 5, C, H, W]</code> ‚Üí <em>Recompile!</em></li>
                    <li><strong>Batch 3:</strong> Text only ‚Üí Shape <code>[B, 0]</code> ‚Üí <em>Recompile!</em></li>
                </ul>
                <p>
                    The XLA compiler was recompiling the training graph for nearly every single batch.
                </p>
                
                <h4>The Solution: Padding with Dummy Images</h4>
                <p>
                    To fix this, we had to standardize the data shape at the dataloader level. We defined a fixed "max images" budget (e.g., 5 images per sample).
                </p>
                <p>
                    If a sample has fewer images, we pad it with <strong>dummy black images</strong> up to the max count. We then use a boolean mask to ensure the model ignores these dummy images during the forward pass.
                </p>

                <pre><code class="language-python"># Simplified fix for static multimodal batches
MAX_IMAGES = 5

def collate_fn(batch):
    # 1. Pad image tensors to [B, MAX_IMAGES, C, H, W]
    # 2. Create an attention mask for valid images
    
    padded_images = torch.zeros(batch_size, MAX_IMAGES, 3, 336, 336)
    image_masks = torch.zeros(batch_size, MAX_IMAGES, dtype=torch.bool)
    
    for i, sample in enumerate(batch):
        n_imgs = len(sample['images'])
        # Fill valid images
        # Spoiler Alert: This kind of indexing operation will also fail in SPMD TorchXLA :(
        padded_images[i, :n_imgs] = sample['images']
        image_masks[i, :n_imgs] = True
        # Remaining slots are zeros (dummy images)
        
    return padded_images, image_masks</code></pre>
                
                <p>
                    Once we implemented this, our step times dropped instantly from minutes to milliseconds. The graph compiled once, and the TPU could finally fly.
                </p>

                <h3>Gradient Checkpointing Compatibility</h3>
                <p>
                    Another common pitfall when adapting existing codebases: <strong>gradient checkpointing</strong>. 
                    Many PyTorch codebases use <code>torch.utils.checkpoint.checkpoint</code> for memory-efficient training. 
                    However, this doesn't play well with TorchXLA's compilation model.
                </p>
                <p>
                    You need to replace it with the XLA-native version: <code>torch_xla.utils.checkpoint.checkpoint</code>. 
                    Here's an example of monkey-patching a model's forward pass:
                </p>

                <pre><code class="language-python"># Replace torch gradient checkpointing with XLA-compatible version
from torch_xla.utils.checkpoint import checkpoint as xla_checkpoint

def patched_forward(self, sample):
    sample = self.conv_in(sample)
    
    if self.training and self.gradient_checkpointing:
        # Use XLA checkpoint instead of torch.utils.checkpoint
        for down_block in self.down_blocks:
            sample = xla_checkpoint(down_block, sample)
        sample = xla_checkpoint(self.mid_block, sample)
    else:
        for down_block in self.down_blocks:
            sample = down_block(sample)
        sample = self.mid_block(sample)
    
    sample = self.conv_norm_out(sample)
    sample = self.conv_act(sample)
    sample = self.conv_out(sample)
    return sample

# Apply the patch
import types
model.encoder.forward = types.MethodType(patched_forward, model.encoder)</code></pre>
                
                <p>
                    This pattern of "find-and-replace" for XLA compatibility is unfortunately common. 
                    Grep your codebase for <code>torch.utils.checkpoint</code> and replace accordingly.
                </p>

                <h3>Scaled Dot-Product Attention (SDPA)</h3>
                <p>
                    Another silent killer: <code>F.scaled_dot_product_attention</code>. This is the efficient fused attention 
                    implementation introduced in PyTorch 2.0, and it's used extensively in modern pretrained models and libraries 
                    like HuggingFace Diffusers.
                </p>
                <p>
                    The problem? <strong>It silently fails on TorchXLA.</strong> Your code will crash with no clear error message, 
                    leaving you debugging for hours. The fix is to replace SDPA with a manual attention implementation:
                </p>

                <pre><code class="language-python"># ‚ùå BAD: F.scaled_dot_product_attention crashes on TPU
hidden_states = F.scaled_dot_product_attention(
    query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False
)

# ‚úÖ GOOD: Manual attention implementation
import math
scale = 1.0 / math.sqrt(query.shape[-1])
attn_scores = torch.matmul(query * scale, key.transpose(-2, -1))
if attention_mask is not None:
    attn_scores = attn_scores + attention_mask
attn_probs = attn_scores.softmax(dim=-1)
hidden_states = torch.matmul(attn_probs, value)</code></pre>

                <p>
                    The tricky part is that SDPA is often buried deep inside library code. For example, when training 
                    diffusion models with the Flux VAE, we had to monkey-patch the <code>AttnProcessor2_0</code> class 
                    from HuggingFace Diffusers:
                </p>

                <pre><code class="language-python"># Monkey-patch Diffusers attention for XLA compatibility
if IS_XLA_AVAILABLE:
    from diffusers.models.attention_processor import AttnProcessor2_0
    import math
    
    def xla_compatible_attention(self, attn, hidden_states, 
                                  encoder_hidden_states=None, 
                                  attention_mask=None, temb=None, *args, **kwargs):
        # ... setup code ...
        
        query = attn.to_q(hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        
        # Reshape for multi-head attention
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        
        # Manual attention instead of F.scaled_dot_product_attention
        scale = 1.0 / math.sqrt(query.shape[-1])
        attn_scores = torch.matmul(query * scale, key.transpose(-2, -1))
        if attention_mask is not None:
            attn_scores = attn_scores + attention_mask
        attn_probs = attn_scores.softmax(dim=-1)
        hidden_states = torch.matmul(attn_probs, value)
        
        # ... rest of forward pass ...
        return hidden_states
    
    AttnProcessor2_0.__call__ = xla_compatible_attention</code></pre>

                <div style="background: #e7f3ff; border-left: 4px solid #2196F3; padding: 16px 20px; margin: 24px 0; border-radius: 0 4px 4px 0;">
                    <strong style="color: #1565C0;">üí° Debugging Tip:</strong>
                    <span style="color: #1565C0;">If your training mysteriously crashes with no helpful error message, grep for <code style="background: rgba(0,0,0,0.08);">scaled_dot_product_attention</code> in both your code AND your dependencies (e.g., <code style="background: rgba(0,0,0,0.08);">site-packages/diffusers/</code>).</span>
                </div>

                <h2 id="distributed-strategies">Distributed Training Strategies</h2>
                <p>
                    As models grow larger, you'll quickly outgrow what fits on a single TPU chip. Here's the evolution of parallelism strategies we've used on TPUs, roughly in order of increasing complexity:
                </p>
                <ul>
                    <li><strong>DDP (Distributed Data Parallel):</strong> The starting point‚Äîreplicate the model, shard the data.</li>
                    <li><strong>FSDP (Fully Sharded Data Parallel):</strong> Shard model parameters across devices to fit larger models.</li>
                    <li><strong>SPMD (Single Program Multiple Data):</strong> Fine-grained tensor-level sharding with explicit mesh control.</li>
                </ul>


                    <h3 id="DDP">DDP</h3>
                    <p>
                    A typical starting point for distributed training is Distributed Data Parallel (DDP). As of 2025, TorchXLA supports DDP via <code>xmp.spawn()</code>. 
                    </p>
                    
                    <div style="background: #fff3cd; border-left: 4px solid #ffc107; padding: 16px 20px; margin: 24px 0; border-radius: 0 4px 4px 0;">
                        <strong style="color: #856404;">‚ö†Ô∏è Known Bug:</strong>
                        <span style="color: #856404;">The more commonly used <code style="background: rgba(0,0,0,0.08);">torch.nn.parallel.DistributedDataParallel</code> has <strong>considerably worse performance</strong> on TPUs and is not fixed before TorchXLA 2.5, as far as we know. Use <code style="background: rgba(0,0,0,0.08);">xmp.spawn()</code> instead.</span>
                    </div>

                    <p>
                    <strong>Mark Step:</strong> A major difference between TorchXLA and torch CUDA is the need to call <code>xm.mark_step()</code> at the end of each training step. This signals to the XLA compiler that the computation for this step is complete and allows it to optimize execution. Otherwise, the compiler will try to unroll the loop into a single giant graph (which could be millions of times larger than expected) and lead to infinite compiling time. Although many wrappers in FSDP/SPMD handle this internally, on DDP we have much less abstraction and everything is done manually, so forgetting to call <code>xm.mark_step()</code> can be a common pitfall.
                    </p>

                    <h3 id="FSDP">FSDP</h3>
                    <p>
                        When your model no longer fits in memory on a single device, <strong>Fully Sharded Data Parallel (FSDP)</strong> becomes essential. 
                        FSDP shards model parameters, gradients, and optimizer states across devices, only gathering them when needed for computation.
                    </p>
                    <p>
                        If you're looking for a reference implementation, we highly recommend studying the 
                        <a href="https://github.com/huggingface/transformers/blob/314f10929a2215b74c2ad6ecf7b2f380c9b7468a/src/transformers/trainer.py#L1953" target="_blank">HuggingFace Transformers Trainer</a>. 
                        The <code>_wrap_model</code> method around line 1953 is an excellent starting point for understanding how FSDP wrapping is configured for TorchXLA. 
                        The Trainer handles many edge cases and provides a battle-tested template for your own implementations.
                    </p>

                    <div style="background: #fff3cd; border-left: 4px solid #ffc107; padding: 16px 20px; margin: 24px 0; border-radius: 0 4px 4px 0;">
                        <strong style="color: #856404;">‚ö†Ô∏è Known Bug ‚Äî OOM Before Sharding:</strong>
                        <p style="color: #856404; margin: 8px 0 0 0;">
                            In TorchXLA FSDP, each device first loads its own copy of the full model in <strong>fp32</strong>, and <em>then</em> applies sharding. 
                            Each TPU device typically has ~100GB of RAM. This means if your model exceeds roughly <strong>~25-30B parameters</strong>, 
                            the codebase will crash with an OOM error <em>before sharding even begins</em>. 
                        </p>
                        <p style="color: #856404; margin: 8px 0 0 0;">
                            <strong>Workarounds we tried:</strong>
                        </p>
                        <ul style="color: #856404; margin: 8px 0 0 0;">
                            <li><strong>Meta device initialization:</strong> Does <em>not</em> work with TorchXLA FSDP as of our testing.</li>
                            <li><strong>Loading in bf16:</strong> This works, but requires <strong>significant code changes</strong> in the TorchXLA library itself. 
                                See our <a href="https://github.com/cambrian-mllm/cambrian/blob/539ffc3254bba004e5d012b65c0ad6cb308897c5/cambrian/train/train_fsdp.py#L1278" target="_blank" style="color: #856404;">Cambrian train_fsdp.py</a> for a reference implementation.</li>
                        </ul>
                    </div>

                    <h3 id="SPMD">SPMD</h3>
                    <p>
                        <strong>SPMD (Single Program Multiple Data)</strong> is Google's approach to distributed training, offering fine-grained 
                        control over how tensors are sharded across devices. In theory, it's the most efficient way to train on TPUs. 
                        In practice, it requires <em>significant</em> debugging.
                    </p>
                    <p>
                        Based on our experience, we <strong>never got arbitrary SPMD sharding to work well</strong>. By "work," we mean 
                        both efficiency and effectiveness‚Äîthe code would run, but it didn't beat FSDP v1 in practice.
                    </p>
                    <p>
                        <strong>However!</strong> After months of debugging, we got <strong>FSDP via SPMD</strong> working. This hybrid approach 
                        uses SPMD's infrastructure with FSDP-style sharding semantics. Our <a href="https://github.com/cambrian-mllm/cambrian-s" target="_blank">Cambrian-S</a> 
                        codebase is built on FSDP via SPMD, and it delivers real improvements in both speed and numerical precision 
                        (since SPMD allows higher precision training more easily).
                    </p>
                    <p>
                        But it comes with its own set of bugs. Here's what we've encountered:
                    </p>

                    <h4>Bug #1: The Indexing Problem</h4>
                    <p>
                        Indexing is fundamental to AI coding. Operations like <code>a[b] = c</code> are everywhere‚Äîinserting image tokens 
                        into text sequences, gathering embeddings, scatter operations. <strong>In TorchXLA SPMD, this triggers an implicit 
                        <code>all_gather</code>.</strong>
                    </p>
                    <p>
                        When your tensors <code>a</code>, <code>b</code>, and <code>c</code> become large, the code crashes immediately 
                        with an OOM error. You've just lost all the benefits of sharding.
                    </p>

                    <div style="background: #ffebee; border-left: 4px solid #f44336; padding: 16px 20px; margin: 24px 0; border-radius: 0 4px 4px 0;">
                        <strong style="color: #c62828;">üö® Critical Bug:</strong>
                        <span style="color: #c62828;">Standard indexing operations like <code style="background: rgba(0,0,0,0.08);">embeddings[indices] = values</code> 
                        trigger implicit all-gathers in SPMD, causing OOM crashes on large tensors. This silently defeats the purpose of sharding.</span>
                    </div>

                    <p>
                        <strong>The Fix:</strong> We had to write custom scatter kernels that explicitly handle the sharding. 
                        This involves manually enabling/disabling sharding around the operation:
                    </p>

                    <pre><code class="language-python">import torch_xla.distributed.spmd as xs

class CustomScatterKernel(torch.autograd.Function):
    """Custom kernel to avoid implicit all_gather in SPMD indexing."""
    
    @staticmethod
    def forward(ctx, input_embeds, img_embeds, token_indices):
        ctx.full_input_shape = input_embeds.shape
        ctx.full_img_shape = img_embeds.shape
        ctx.dtype = input_embeds.dtype
        
        # Manually handle sharding to avoid implicit all_gather
        sharded_input = xs.enable_manual_sharding(
            input_embeds, ("fsdp", None, None)).global_tensor
        sharded_img = xs.enable_manual_sharding(
            img_embeds, ("fsdp", None, None)).global_tensor
        sharded_indices = xs.enable_manual_sharding(
            token_indices, ("fsdp", None)).global_tensor

        # Concatenate and gather on sharded tensors
        sharded_embeds = torch.cat([sharded_input, sharded_img], dim=1)
        sharded_embeds = torch.gather(
            sharded_embeds, 1, 
            sharded_indices.unsqueeze(-1).expand(-1, -1, sharded_embeds.size(-1))
        )
        
        # Restore automatic sharding
        output = xs.disable_manual_sharding(
            sharded_embeds, ("fsdp", None, None), 
            input_embeds.shape, mesh=xs.get_global_mesh()
        ).global_tensor

        ctx.save_for_backward(token_indices)
        return output

    @staticmethod
    def backward(ctx, grad_output):
        # Similar manual sharding logic for backward pass
        token_indices, = ctx.saved_tensors
        # ... scatter_add with manual sharding ...
        return grad_input, grad_img, None</code></pre>

                    <p>
                        You then carefully prepare your embeddings (remember: <strong>static shapes!</strong>) and use the custom kernel:
                    </p>

                    <pre><code class="language-python"># Instead of: input_embeds[vision_token_indices] = image_features  ‚ùå
# Use the custom kernel:
input_embeds = CustomScatterKernel.apply(
    input_embeds, image_features, vision_token_indices
)  # ‚úÖ</code></pre>

                    <p>
                        The logic is complicated, but the pattern is: wrap indexing operations with manual sharding control 
                        to prevent the XLA compiler from inserting expensive all-gathers.
                    </p>

                    <h4>Bug #2: The Initialization Problem</h4>
                    <p>
                        PyTorch's <code>nn.init.*</code> functions (like <code>xavier_uniform_</code>, <code>normal_</code>, etc.) 
                        sometimes produce <strong>unexpected results on TorchXLA</strong>. We've seen cases where initialization 
                        produces values that are orders of magnitude off from expected. <strong>If your training immediately produces 
                        NaN losses, diverges within the first few steps, or your initial loss value is astronomically large‚Äîthis 
                        initialization bug is likely the culprit.</strong>
                    </p>

                    <figure style="margin: 30px 0;">
                        <img src="assets/images/initialization_bug.png" alt="Screenshot showing abnormal weight initialization values on TPU" style="width: 100%; border-radius: 4px; border: 1px solid #ddd;">
                        <figcaption style="font-size: 0.9em; color: #666; margin-top: 8px;">
                            Xavier initialization gone wrong: expected std ~0.028, got values in the billions. 
                            This silent corruption can cause training to diverge immediately.
                        </figcaption>
                    </figure>

                    <p>
                        <strong>The Fix:</strong> Write manual initialization functions that explicitly control the random number 
                        generation and tensor operations. We maintain separate initialization paths for GPU vs TPU:
                    </p>

                    <pre><code class="language-python">if IS_XLA_AVAILABLE:
    self.initialize_weights_tpu()  # Manual TPU-friendly init
else:
    self.initialize_weights_gpu()  # Standard nn.init.* calls</code></pre>

                    <p>
                        The TPU-friendly version manually implements each initialization:
                    </p>

                    <pre><code class="language-python">@torch.no_grad()
def manual_xavier_uniform(tensor):
    """TPU-friendly xavier_uniform_ implementation."""
    fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(tensor)
    bound = math.sqrt(6.0 / (fan_in + fan_out))
    
    # Generate uniform values and scale to [-bound, bound]
    random_tensor = torch.empty_like(tensor, dtype=torch.float32).uniform_(0, 1)
    random_tensor = random_tensor * (2 * bound) - bound
    
    # Use copy_ for reliable TPU behavior
    tensor.data.copy_(random_tensor.to(tensor.dtype))
    return tensor

@torch.no_grad()
def manual_normal_(tensor, mean=0.0, std=1.0):
    """TPU-friendly normal_ implementation."""
    normal_tensor = torch.zeros_like(tensor, dtype=torch.float32)
    normal_tensor.normal_(mean=mean, std=std)
    tensor.data.copy_(normal_tensor.to(tensor.dtype))
    return tensor</code></pre>

                    <p>
                        You then apply these manual functions instead of the standard <code>nn.init.*</code> calls, 
                        with <code>xm.mark_step()</code> calls between initialization phases to force synchronization:
                    </p>

                    <pre><code class="language-python">def initialize_weights_tpu(self):
    """Robust TPU-friendly initialization."""
    
    # Initialize linear layers with manual xavier
    for name, module in self.named_modules():
        if isinstance(module, nn.Linear):
            manual_xavier_uniform(module.weight)
            if module.bias is not None:
                module.bias.data.fill_(0.0)
    
    # Force synchronization after each phase
    xm.mark_step()
    
    # Zero-init specific layers (e.g., adaLN modulation)
    for block in self.transformer_blocks:
        block.adaLN_modulation[-1].weight.data.fill_(0.0)
        block.adaLN_modulation[-1].bias.data.fill_(0.0)
    
    xm.mark_step()
    
    # Validate initialization
    self._check_weight_statistics()</code></pre>

                    <div style="background: #e7f3ff; border-left: 4px solid #2196F3; padding: 16px 20px; margin: 24px 0; border-radius: 0 4px 4px 0;">
                        <strong style="color: #1565C0;">üí° Pro Tip:</strong>
                        <span style="color: #1565C0;">Always add validation checks after initialization on TPU. Print weight statistics (max, mean, std) 
                        and check for abnormal values. Silent initialization bugs can waste days of debugging diverged training runs.</span>
                    </div>

                    <h4>Bug #3: The Compiler Gets It Wrong</h4>
                    <p>
                        Sometimes you've written everything correctly‚Äîstatic shapes, no implicit all-gathers, proper initialization‚Äîbut 
                        you <em>still</em> get unexpected OOM errors. In these cases, the XLA compiler itself may be making incorrect 
                        sharding decisions.
                    </p>
                    <p>
                        The SPMD compiler tries to infer optimal sharding for intermediate tensors, but it doesn't always get it right. 
                        A tensor that <em>should</em> stay sharded might get unexpectedly replicated, causing memory to explode.
                    </p>
                    <p>
                        <strong>The Fix:</strong> Manually remind the compiler how tensors should be sharded using <code>xs.mark_sharding()</code>:
                    </p>

                    <pre><code class="language-python"># When the compiler makes wrong sharding decisions, 
# explicitly mark how tensors should be sharded
import torch_xla.distributed.spmd as xs

# After operations that might confuse the compiler
xs.mark_sharding(tensor, xs.get_global_mesh(), ("fsdp", None))

# Example in practice:
def forward(self, x):
    # ... some computation ...
    output = self.layer(x)
    
    # Remind compiler this should stay sharded on first dim
    xs.mark_sharding(output, xs.get_global_mesh(), ("fsdp", None))
    
    return output</code></pre>

                    <p>
                        This is essentially "hinting" to the compiler: "I know what I'm doing‚Äîkeep this tensor sharded this way." 
                        It's frustrating to debug, but sprinkling <code>mark_sharding</code> calls at strategic points can 
                        turn an OOM crash into a working training run.
                    </p>

                <h2 id="portability">Can LLMs Write TPU Code?</h2>
                <p>
                    For many researchers, "vibe-coding" or LLM-assisted coding has become a de-facto practice when starting new projects. 
                    But do LLMs actually understand the nuances of TPU and TorchXLA development? Over the past two years, especially with 
                    the introduction of reasoning models, our experience has been a mix of impressive capabilities and frustrating hallucinations.
                </p>

                <h3>What Works Well</h3>
                <p>
                    <ul>
                        <li>
                            <strong>Infrastructure Management:</strong> LLMs have mastered most TPU CLI commands. Since many TRC TPUs are preemptible, 
                            having LLMs write scripts to create, inspect, and auto-restart TPUs is a huge time-saver.
                        </li>
                    </ul>
                </p>

                <h3>What Breaks & How to Fix It</h3>
                <p>
                    <ul>
                        <li>
                            <strong>Outdated TPU APIs.</strong> TPU commands change frequently. LLMs are often stuck on older versions, leading to code that is either deprecated or pure hallucination.
                            <br><em>Fix:</em> When prompting, explicitly paste the latest API documentation (or web link) or error logs.
                        </li>
                        <br>
                        <li>
                            <strong>TorchXLA Codes.</strong> TorchXLA looks deceptively similar to standard PyTorch. LLMs often default to writing standard PyTorch code (like using `.cuda()` or code with dynamic shapes) that is catastrophic for XLA performance. 
                            TorchXLA APIs also change quite frequently, so LLMs generation can be full of hallucinations. Overall this makes TorchXLA codes very fragile and hard to maintain with LLMs. 
                            <br><em>Fix:</em> Unfortunately there is no good way to fix this. On the bright side, we have been (forced) to learn about the fundamentals of many code, like sharding, model resuming, etc. This is a motivation to 
                            learn more about the fundamentals of the code. Some tips that might help: try using the best model, turn the max reasoning and tell the model explicitly this is TPU codee, TorchXLA code, please reason your best instead of relying on pre-existing knowledge. 
                        </li>
                    </ul>
                </p>

                <h2 id="data-bottleneck">The Data Bottleneck</h2>
                <p>
                    In academic clusters, we often take shared filesystems (NFS/GPFS) for granted. On Cloud TPUs, storage requires more deliberate planning. 
                    We primarily rely on two options: <strong>Google Cloud Storage (Buckets)</strong> and <strong>Persistent Disks (PD)</strong>.
                </p>

                <h3>Persistent Disks (PD): The "Normal" Filesystem</h3>
                <p>
                    <strong>Persistent Disks</strong> behave like a standard filesystem. They are reliable and less prone to random failures compared to network mounts. 
                    However, they come with significant caveats, especially when moving between TPU generations:
                </p>
                <ul>
                    <li>
                        <strong>Cost on v6:</strong> While normal PDs work well on v4 pods. On v5 pods, only balanced PD (2x expensive than normal PDs) is allowed, and only allowed to be attached to a single pod or a maximum of 10 TPU VMs. On v6 pods, only hyperdisk ML Disks (4x expensive than normal PDs) are allowed and shares the same attachment limitation as v5 pods.
                        <!-- , we found that on newer v6 TPUs, the associated storage costs can become prohibitively expensive. -->
                    </li>
                    <li>
                        <strong>The Read-Write vs. Parallelism Conflict:</strong> On a parallel TPU pod (e.g., v4-256), disks <strong>must</strong> be mounted in <em>read-only</em> mode to be attached to multiple VMs simultaneously. 
                        <strong>Read-Write</strong> access is restricted to a single VM (like a v4-8). This means it is impossible to save checkpoints or logs back to the disk during a large distributed run.
                    </li>
                    <li>
                        <strong>Mutual Exclusion:</strong> A disk cannot be mounted as Read-Only and Read-Write simultaneously. This means you cannot "hot-fix" your code 
                        or update your dataset on the disk while a training run is reading from it.
                    </li>
                </ul>

                <h4>The "Clone & Scale" Workflow</h4>
                <p>
                    To navigate the PD limitations, we adopted a staged workflow that separates development from production training.
                </p>

                <div style="display: flex; align-items: center; justify-content: center; gap: 16px; margin: 30px 0; padding: 30px 20px; background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-radius: 12px; flex-wrap: wrap;">
                    <!-- Dev Box -->
                    <div style="text-align: center;">
                        <div style="background: #fff; border: 2px solid #4CAF50; border-radius: 8px; padding: 16px 20px; min-width: 120px;">
                            <div style="font-family: 'Inter', sans-serif; font-weight: 600; font-size: 14px; color: #2e7d32;">v4-8</div>
                            <div style="font-family: 'Inter', sans-serif; font-size: 11px; color: #666; margin-top: 4px;">Dev Box</div>
                        </div>
                        <div style="margin-top: 8px; font-size: 20px;">‚ÜïÔ∏è</div>
                        <div style="background: #fff; border: 2px solid #4CAF50; border-radius: 8px; padding: 12px 16px; margin-top: 8px;">
                            <div style="font-family: 'Inter', sans-serif; font-weight: 600; font-size: 12px; color: #2e7d32;">üíæ Master Disk</div>
                            <div style="font-family: 'Inter', sans-serif; font-size: 10px; color: #4CAF50; margin-top: 2px;">Read-Write</div>
                        </div>
                    </div>
                    
                    <!-- Arrow 1 -->
                    <div style="font-size: 24px; color: #666;">‚Üí</div>
                    
                    <!-- Clone Step -->
                    <div style="text-align: center;">
                        <div style="background: #fff3e0; border: 2px dashed #ff9800; border-radius: 8px; padding: 16px 20px;">
                            <div style="font-family: 'Inter', sans-serif; font-weight: 600; font-size: 13px; color: #e65100;">üìã Snapshot</div>
                            <div style="font-family: 'Inter', sans-serif; font-size: 11px; color: #666; margin-top: 4px;">& Clone</div>
                        </div>
                    </div>
                    
                    <!-- Arrow 2 -->
                    <div style="font-size: 24px; color: #666;">‚Üí</div>
                    
                    <!-- Training Pod -->
                    <div style="text-align: center;">
                        <div style="background: #fff; border: 2px solid #2196F3; border-radius: 8px; padding: 16px 20px; min-width: 120px;">
                            <div style="font-family: 'Inter', sans-serif; font-weight: 600; font-size: 14px; color: #1565c0;">v4-256 Pod</div>
                            <div style="font-family: 'Inter', sans-serif; font-size: 11px; color: #666; margin-top: 4px;">Training</div>
                        </div>
                        <div style="margin-top: 8px; font-size: 20px;">‚¨áÔ∏è</div>
                        <div style="background: #fff; border: 2px solid #2196F3; border-radius: 8px; padding: 12px 16px; margin-top: 8px;">
                            <div style="font-family: 'Inter', sans-serif; font-weight: 600; font-size: 12px; color: #1565c0;">üíæ Cloned Disk</div>
                            <div style="font-family: 'Inter', sans-serif; font-size: 10px; color: #2196F3; margin-top: 2px;">Read-Only</div>
                        </div>
                    </div>
                </div>

                <ol>
                    <li>
                        <strong>Develop on -8:</strong> We maintain a small TPU node (e.g., v4-8) with a <strong>Read-Write</strong> disk. This is our "dev box" where we write code, debug, and cook data.
                    </li>
                    <li>
                        <strong>Snapshot & Clone:</strong> Once the code and data are ready, we create a disk snapshot/clone.
                    </li>
                    <li>
                        <strong>Train on Pod:</strong> We mount this cloned disk as <strong>Read-Only</strong> on the large training pod (e.g., v4-256). 
                        This ensures data consistency and allows the large pod to scale without lock contention.
                    </li>
                    <li>
                        <strong>Iterate:</strong> If we need to change the data, we go back to step 1, modify the RW disk, and create a new clone for the next run.
                    </li>
                </ol>

                <h3>Google Cloud Storage (Buckets)</h3>
                <p>
                    <strong>Google Cloud Storage (GCS) Buckets</strong> offer a flexible alternative to Persistent Disks. They provide virtually unlimited cloud storage and can be accessed concurrently by multiple TPU VMs without the read-write restrictions of PDs.
                    
                    <li>
                        <strong>Network Mount:</strong> GCS buckets can be mounted as a normal filesystem to TPU VMs via <code>gcsfuse</code>. This allows seamless read and write access across all VMs in a pod without the need for writing GCS-dependent read and write code.
                    </li>

                    <li>
                        <strong>Speed:</strong> If the TPU VM is located within the same region as the GCS bucket, read and write speeds can be quite fast, often comparable or even exceeding those of Persistent Disks. In our use case, it's common to see read speeds of 2-4 GB/s per bucket under training workloads. However, as any networked storage, the error rate can be high and possibly affect latency sensitive training jobs.
                    </li>
                    
                    <li>
                        <strong>Cost-Effective:</strong> GCS storage costs are generally lower than Persistent Disks and are charged based on actual storage size. This can lead to significant savings, especially for large datasets that do not require the high IOPS of PDs.
                    </li>
                    
                    <li>
                        <strong>Data Consistency:</strong> <code>GCSFuse</code> provides multi-level of consistency by introducing a `ttl-secs` parameter that controls how long file metadata and directory listings are cached. Setting this parameter to 0 ensures the strongest consistency, where all read and write operations reflect the most recent changes. But this may introduce latency overhead and large amount of cost due to frequent metadata operations. For training workloads where data is mostly read-only, a higher `ttl-secs` (e.g., 60 seconds) can be used to improve performance and reduce costs, while still maintaining reasonable consistency.
                    </li>
                    <li>    
                        <strong>Data Chunking:</strong> As any network storage, it's not encourage to read many small files from GCS buckets (for example, 1 Million JPEGs from ImageNet). Instead, it's better to chunk small files into larger tars/TFRecords to reduce the number of read operations and improve throughput. For GCS, it also gives very high cost for write/read operations, so reducing the number of operations is also critical to reduce cost.
                    </li>
                </p>

                <h2 id="conclusion">Conclusion</h2>
                <p>
                    Despite the initial friction, mastering the TPU stack has allowed us to train models like Cambrian-1, Cambrian-S, RAE, and scaled-RAE 
                    at scale. The ecosystem is maturing, and we hope this post helps you avoid some of the pitfalls we encountered.
                </p>
                <p>
                    If you have comments, suggestions, or want to share your own TPU debugging war stories, please reach out to 
                    <a href="https://tsb0601.github.io/" target="_blank">Shengbang Tong</a> or 
                    <a href="https://bytetriper.github.io/" target="_blank">Boyang Zheng</a>.
                </p>

                <div class="citation-section" id="citation">
                    <h3 class="sans-serif" style="font-size: 1.2rem; margin-top: 0;">Citation</h3>
                    <p style="font-size: 0.95rem; color: #666; margin-bottom: 10px;">Please cite this work as:</p>
                    
                <div class="citation-box">Tong, Shengbang and Zheng, Boyang. "Lessons from Two Years of TPU Training in Academia."
Cambrian Blog, Jan 2026.
https://cambrian-mllm.github.io/blog/tpu-training-experiments.html</div>

                <div class="citation-box">@article{tong2026tpu,
  author = {Tong, Shengbang and Zheng, Boyang},
  title = {Lessons from Two Years of TPU Training in Academia},
  journal = {Cambrian Blog},
  year = {2026},
  url = {https://cambrian-mllm.github.io/blog/tpu-training-experiments.html}
}</div>
                </div>
            </article>
        </div>

        <footer class="site-footer">
            <div>Cambrian Lab ¬© 2026</div>
            <div class="footer-links">
                <a href="#">Terms of service</a>
                <a href="#">Privacy notice</a>
            </div>
        </footer>
    </div>
</body>
</html>

