<!doctype html>
<html lang="en">
    <head>
        <title>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</title>
        <link rel="icon" type="image/x-icon" href="static/img/icons/earth_icon.png">

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:url" content="https://cambrian-mllm.github.io/" />
        <meta property="og:image" content="https://cambrian-mllm.github.io/static/img/preview.png" />
        <meta name="twitter:url" content="https://cambrian-mllm.github.io/" />
        <meta name="twitter:image" content="https://cambrian-mllm.github.io/static/img/preview.png" />

        <script src="./static/js/distill_template.v2.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

        <script defer="" src="./static/js/hider.js"></script>
        <script src="./static/js/image_interact.js"></script>
        <script src="./static/js/switch_videos.js"></script>

        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>

        <link rel="icon"
        href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🪼</text></svg>">

        <!-- medium zoom https://github.com/francoischalifour/medium-zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>  <!-- jquery -->
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>
    </head>
    <body>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <div class="header-content">
                    <h1 style="margin-top: 0px"><i>Cambrian-1</i></h1>
                    <h2>A Fully Open, <i>Vision-Centric</i><br>
                        Exploration of Multimodal LLMs</h2>
                    <p style="color: #FFF7D4">
                        Introducing Cambrian-1, a family of 
                        <em><strong style="color: #ffe099">vision-centric</strong></em>
                        multimodal LLMs (MLLMs).
                        Cambrian-1 is structured around five key pillars:
                        <ul style="color: #FFF7D4">
                            <li><strong style="color: #ffe099">Visual Representations</strong>: We explore various vision encoders and their combinations.
                            <li><strong style="color: #ffe099">Connector Design</strong>: We design a new dynamic and spatially-aware connector that integrates vision features with LLMs while reducing the number of tokens.
                            <li><strong style="color: #ffe099">Instruction Tuning Data</strong>: We curate high-quality visual instruction-tuning data from public sources, emphasizing the importance of distribution balancing. 
                            <li><strong style="color: #ffe099">Instruction Tuning Recipes</strong>: We discuss instruction tuning strategies and practices. 
                            <li><strong style="color: #ffe099">Benchmarking</strong>: We examine existing MLLM benchmarks and introduce a new vision-centric benchmark "CV-Bench".
                        </ul>
                    </p>
                    <div class="button-container">
                        <!-- replace arxiv -->
                        <a href="TODO" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="ai ai-arxiv"></i>
                            </span>
                            arXiv
                        </a>
                        <!-- replace pdf -->
                        <a href="static/cambrian.pdf" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="fas fa-file-pdf"></i>
                            </span>
                            <span>pdf</span>
                        </a>
                        <!-- replace image -->
                        <a href="https://github.com/cambrian-mllm/cambrian" class="button" target="_blank">
                            <span class="icon is-small">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                        </a>
                        <a href="https://huggingface.co/nyu-visionx/Cambrian-10M" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>Dataset</span>
                        </a>                        
                        <a href="https://huggingface.co/nyu-visionx" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                            </span>
                            <span>Checkpoints</span>
                        </a>
                    </div>
                </div>
                <div class="header-image">
                    <img src="static/img/cambrian.png" alt="Teaser Image" class="teaser-image">
                </div>
            </div>
        </div>
    <d-article>
        <div class="byline">
            <div class="byline-container">
                <p>
                    <a href="https://tsb0601.github.io/petertongsb/" class="author-link" target="_blank">Shengbang Tong*</a> &emsp;
                    <a href="https://ellisbrown.github.io/" class="author-link" target="_blank">Ellis Brown*</a> &emsp;
                    <a href="https://penghao-wu.github.io/" class="author-link" target="_blank">Penghao Wu*</a> &emsp;
                    <br>
                    <a href="https://sites.google.com/view/sanghyunwoo/" class="author-link" target="_blank">Sanghyun Woo</a> &emsp;
                    <a href="https://www.linkedin.com/in/manoj-middepogu/" class="author-link" target="_blank">Manoj Middepogu</a> &emsp;
                    <a href="https://www.linkedin.com/in/sai-charitha-akula-32574887" class="author-link" target="_blank">Sai Charitha Akula</a> &emsp;
                    <a href="https://jihanyang.github.io/" class="author-link" target="_blank">Jihan Yang</a> &emsp;
                    <a href="https://github.com/vealocia" class="author-link" target="_blank">Shusheng Yang</a> &emsp;
                    <a href="https://github.com/adithyaiyer1999" class="author-link" target="_blank">Adithya Jairam Iyer</a> &emsp;
                    <a href="https://xichenpan.com/" class="author-link" target="_blank">Xichen Pan</a> &emsp;
                    <a href="https://www.linkedin.com/in/ziteng-wang-694b8b227/" class="author-link" target="_blank">Ziteng Wang</a> &emsp;
                    <br>
                    <a href="https://cs.nyu.edu/~fergus/" class="author-link" target="_blank">Rob Fergus</a> &emsp;
                    <a href="https://yann.lecun.com/" class="author-link" target="_blank">Yann LeCun</a> &emsp;
                    <a href="https://www.sainingxie.com/" class="author-link" target="_blank">Saining Xie</a>
                </p>
                <p>
                    <a href="https://cs.nyu.edu/home/index.html" class="affiliation-link" id="affiliation" target="_blank">New York University</a>
                </p>
            </div>
        </div>
        

        
        <p class="text abstract">

            We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a <strong>vision-<i>centric</i></strong> approach. 
            While stronger language models can enhance multimodal capabilities, 
            the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. 

            <br><br>
            Cambrian-1 is structured around five key pillars, each offering important insights into the design space of MLLMs:
            <ul>
                <li><strong>Visual Representations</strong>: We explore various vision encoders and their combinations. (see <a href="#visual_representations">§Evaluating Visual Representations through MLLMs</a>)</li>
                <li><strong>Connector Design</strong>: We design a new dynamic and spatially-aware connector that integrates vision features with LLMs while reducing the number of tokens. (see <a href="#connector_design">§Spatial Vision Aggregator</a>)</li>
                <li><strong>Instruction Tuning Data</strong>: We curate high-quality visual instruction-tuning data from public sources, emphasizing the importance of distribution balancing. (see <a href="#instruction_data">§Instruction Tuning Data for Training MLLMs</a>)</li>
                <li><strong>Instruction Tuning Recipes</strong>: We discuss instruction tuning strategies and practices. (see <a href="#sec:inst_tuning">§Instruction Tuning Recipes</a>) </li> 
                <li><strong>Benchmarking</strong>: We examine existing MLLM benchmarks and introduce a new vision-centric benchmark "CV-Bench". (see <a href="#cv-bench">§Analysis the Benchmark and Cambrian Vision-Centric Benchmark (CV-Bench)</a>)</li>
            </ul>
        </p>

        <div class="image-row">
            <a href="#visual-representation">
                <img src="static/img/visual_representation_logo.PNG" alt="Visual Representation Logo">
            </a>         
            <a href="#connector_design">
                <img src="static/img/connector_logo.PNG" alt="Connector Logo">
            </a>
            <a href="#instruction_data">
                <img src="static/img/IT_Data_logo.PNG" alt="IT Data Logo">
            </a>           

            <a href="#sec:inst_tuning">
                <img src="static/img/IT_Recipe_Logo.PNG" alt="IT Recipe Logo">
            </a>
            <a href="#sec:benchmarking">
                <img src="static/img/eval_logo.PNG" alt="Eval Logo">
            </a>
        </div>


        <p class="text abstract">
            To this end, Cambrian-1 not only achieves state-of-the-art performances but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. See <a href="#State-of-the-art-MLLM-performance">§State-of-the-art MLLM performance</a>. 
            We provide <a href="https://huggingface.co/nyu-visionx" target="_blank">model weights</a>, 
            <a href="https://github.com/cambrian-mllm/cambrian" target="_blank">code</a>, 
            <a href="https://huggingface.co/nyu-visionx" target="_blank">datasets</a>, 
            and detailed instruction-tuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.            
            <!-- To this end, Cambrian-1 not only achieves state-of-the-art performances but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs (see <a href="#sota">&sect;State-of-the-art MLLM performance</a>). We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. 
            We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning. -->

            <!-- <br><br>
            Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations,
            offering new insights into different models and architectures—self-supervised, strongly supervised, 
            or combinations thereof—based on experiments with over 15 vision models.
            We critically examine existing MLLM benchmarks, addressing the difficulties involved in consolidating and 
            interpreting results from various tasks and introduce a new visioncentric benchmark “CV-Bench”.
            We also discuss instruction tuning strategies and practices .
            <br><br>
            To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), 
            a dynamic and spatially-aware connector that integrates high-resolution vision features 
            with LLMs while reducing the number of tokens (see <a href="#connector_design">&sect;Spatial Vision Aggregator</a>).
            <br><br>
            Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, 
            emphasizing the importance of data source balancing and distribution ratio (see <a href="#instruction_data">&sect;Instruction Tuning Data for Training MLLMs</a>).
            <br><br>
            Collectively, Cambrian-1 not only achieves state-of-the-art performances but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs
            (see <a href="#sota">&sect;State-of-the-art MLLM performance</a>). 
            We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. 
            We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning. -->
        </p>

        

        <hr>

        <div id='visual_representations' class="vision-block">
            
            <div id="sec:benchmarking" class="sub-section">
                <h1 class="text">Analyzing the Benchmarks</h1>

                    <p class="text">
                        <!-- To effectively evaluate visual representations and MLLMs, we first need to select benchmarks that accurately assess the <i>multimodal</i> 
                        capabilities of these models. We use a suite of commonly used benchmarks and begin by analysing the benchmarks themselves.
                    
                         -->
                        <p class="text">
                        <strong>Who's answering: LLM or MLLM?:</strong> We compare performance between visual-disabled and visual-enabled settings across 14 MLLMs. Our findings reveal that some benchmarks such as MMMU and AI2D are less reliant on visual inputs, whereas others such as MMVP and MME experience significant performance declines, indicating their effective evaluation of multimodality</li>
                        </p>
                        <p class="text">
                            <strong>Benchmark Clustering and Analysis:</strong> Through correlation analysis and principal component analysis of MLLM performances across various benchmarks, distinct clusters emerge categorized as "General," "Knowledge," "Chart & OCR," and "Vision-Centric." There is also <i>difficiency</i> on the size of vision-centric benchmarks</li>
                        </p>

                        <!-- <br><br> -->
                        <!-- <strong>Who's answering the question: the LLM or MLLM?</strong> 
                        For each of the 15 trained MLLMs, we disable the visual input and evaluate them across all benchmarks. 
                        Additionally, we calculate the expected score by randomly guessing. 
                        As shown in <a href="#fig:comparison">Figure 1</a> (left), SQA-I, MMMU, and AI2D have less than a 5-point gap between vision enabled and disabled, suggesting that these benchmarks may not significantly rely on visual input. 
                        TextVQA and GQA both demonstrate a nearly 40-point positive gap between random guessing and vision-disabled scores, implying a strong language bias in these benchmarks. 
                        On the other hand, benchmarks like MMVP and MME show that vision-disabled performance is notably worse than random guessing, suggesting that accurate visual grounding is particularly crucial for these tasks.        
                    </p>
                    <p class="text">
                        <strong>Clustering the Benchmarks</strong> To better understand the different aspects of MLLM performance, we analyze the correlations between the performance of our 14 MLLMs on each benchmark.
                        A confusion matrix (<a href="#fig:correlation_plot">Figure 2</a>) reveals that certain benchmarks, such as MMMU, are largely uncorrelated with the others.
                        We perform principal component analysis on the benchmark scores and observe the natural formation of clusters corresponding to "General," "Knowledge," "Chart & OCR," and "Vision-Centric" categories (<a href="#fig:comparison">Figure 1</a> right). We assign MMMU to the knowledge category based on the types of questions it includes.
                        We also find that existing vision-centric benchmarks are of insufficient size, challenging the robustness of evaluating such capabilities. Furthermore, these benchmarks do not cover crucial visual elements such as depth and spatial awareness. 
                    </p> -->
                    <div id="fig:comparison" style="display: flex; flex-direction: column; align-items: center;">
                        <div style="display: flex; justify-content: center; width: 140%;">
                            <img data-zoomable="" style="width: 100%;" src="static/img/bench_cat.png" alt="benchmark category">
                        </div>
                        <figcaption style="text-align: left; width: 140%;">
                            Figure 1: Left: Performance comparison of MLLMs with visual input enabled and disabled across various benchmarks. Right: Principal component analysis displaying clusters of benchmarks based on performance metrics, with bubble size corresponding to benchmark size.
                        </figcaption>
                    </div>
<!--                    
                    <div id="fig:correlation_plot" style="display: flex; flex-direction: column; align-items: center;">
                        <div style="display: flex; justify-content: center; width: 100%;">
                            <img data-zoomable="" style="width: 100%;" src="static/img/correlation_matrix.png" alt="correlation matrix">
                        </div>
                        <figcaption style="text-align: center; width: 100%;">
                            Figure 2: Correlation matrix for MLLM benchmarks.
                        </figcaption>
                    </div> -->
            </div>
            <div id="cv-bench" class="sub-section">

                    <p class="text">                <strong>Cambrian Vision-Centric Benchmark (CV-Bench) </strong>


                        To address the scarcity of vision-centric benchmarks, we introduce the Cambrian Vision-Centric Benchmark (CV-Bench), repurposing standard vision tasks for multimodal evaluation. CV-Bench contains approximately 2600 vision-centric VQA questions, addressing the issue of insufficient questions in previous benchmark.
                    </p>

                    <div id="fig:cvcb" style="display: flex; flex-direction: column; align-items: center;">
                        <div style="display: flex; justify-content: center; width: 140%;">
                            <img data-zoomable="" style="width: 100%;" src="static/img/cvcb.PNG" alt="benchmark category">
                        </div>
                        <figcaption style="text-align: left; width: 140%;">
                            Figure 2: Example questions in CV-Bench that focuses of 2D and 3D visual understanding.                        </figcaption>
                    </div>
<!-- 
                    <table>
                        <thead>
                            <tr>
                                <th>Type</th>
                                <th>Task</th>
                                <th>Description</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td rowspan="2"><strong>2D</strong></td>
                                <td><strong>Spatial Relationship</strong></td>
                                <td>Determine the relative position of an object with respect to the anchor object. Consider either the left-right or top-bottom relationship.</td>
                            </tr>
                            <tr>
                                <td><strong>Object Count</strong></td>
                                <td>Determine the number of instances present in the image.</td>
                            </tr>
                            <tr>
                                <td rowspan="2"><strong>3D</strong></td>
                                <td><strong>Depth Order</strong></td>
                                <td>Determine which of the two distinct objects is closer to the camera.</td>
                            </tr>
                            <tr>
                                <td><strong>Relative Distance</strong></td>
                                <td>Determine which of the two distinct objects is closer to the anchor object.</td>
                            </tr>
                        </tbody>
                    </table> -->
            </div>
            <div id="sec:inst_tuning" class="sub-section">

            <h1 class="text">Instruction Tuning Recipes </h1>
                <p class="text">
                    MLLMs connects pre-trained LLM and vision backbones using a connector such as an MLP projector. Various studies have suggested different optimal training methodologies for MLLMs.
                     <!-- We tune a set of MLLMs using Vicuna-7B as the LLM backbone and 15 different vision models as visual encoders, utilizing a 737K instruction tuning data mix and consistent hyperparameters to evaluate different tuning strategies. -->
                </p>
                <!-- <p>MLLMs integrate pre-trained LLM and vision backbones using a connector like an MLP projector. Initially, the connector is pre-trained between the frozen LLM and vision backbones using adapter data, followed by fine-tuning the connector and LLM with instruction tuning data, while the vision encoder remains frozen. Various studies have suggested different optimal training methodologies for MLLMs, which we further investigate through extensive experiments. We tune a set of MLLMs using Vicuna-7B as the LLM backbone and 15 different vision models as visual encoders, utilizing a 737K instruction tuning data mix and consistent hyperparameters to evaluate different tuning strategies.</p>
                <p class="text">
                    For our experiments, we tune a set of MLLMs using Vicuna-7B as the LLM backbone and each of our 15 vision models 
                    (<a href="#tab:vision_backbones">Table 1</a>) as the visual encoder.
                    We use a 737K instruction tuning data mix for all experiments here. 
                    All hyperparameters are matched across each experimental setting—highlighting the impact of different tuning strategies with each visual encoder. 
                </p> -->
                <!-- <div id="tab:vision_backbones" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 100%;">
                        <img data-zoomable="" style="width: 100%;" src="static/img/table/visual_backbones.png">
                    </div>
                    <figcaption style="text-align: center; width: 100%;">
                        Table 1: Summary of vision backbones categorization and setups.
                    </figcaption>
                </div> -->
                <p class="text">
                    <strong>One Stage vs Two Stage Training</strong> Recent work suggests skipping connector pre-training to reduce compute costs without harming performance. 
                    We experiment with 0, 0.5M, and 1.2M adapter data. Following LLaVA's method <d-cite key="liu2023visual"></d-cite>, we initially tune only the connector, then unfreeze both the LLM and connector for instruction tuning with a 737K mix. <a href="#fig:studyadapter">Figure 3</a> indicates that pre-training the connector boosts performance, and using more adapter data enhances it further, leading us to standardize on a 2-stage training approach with 1.2M adapter data.


<!-- 
                    <strong>One Stage vs Two Stage Training</strong>
                    Recent work advocates for skipping connector pre-training, claiming this “reduces compute cost without harming downstream performance.” 
                    To explore whether this claim holds, especially when using non-language-supervised visual encoders, 
                    we conduct experiments using 0, 0.5M, and 1.2M adapter data. 
                    Following LLaVA's recipe <d-cite key="liu2023visual"></d-cite>, we tune only the connector on the adapter data during this first phase, 
                    before unfreezing the LLM and connector during instruction tuning on the 737K mix.
                </p>
                <p class="text">
                    <a href="#fig:studyadapter">Figure 3</a> shows that pre-training the connector first enhances model performance, 
                    and training with additional adapter data further improves performance across all domains. 
                    Thus, we subsequently adopt 2-stage training with 1.2M adapter data as our standard setup. -->
                </p>
                
                <p class="text">
                    <strong>Freeze vs Unfreeze Vision Encoder</strong>
                    There are also mixed practices in freezing or unfreezing vision backbones during fine-tuning. 
                    Some argue that unfreezing the vision backbone significantly degrades performance. 
                    Our experiments demonstrate that, with a reasonable vision model learning rate, 
                    unfreezing benefits performance across all benchmarks except for a marginal change in Knowledge benchmarks.
                </p>
                <div id="fig:studyadapter" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 140%;">
                        <img data-zoomable="" style="width: 100%;" src="static/img/performance_plot.png">
                    </div>
                    <figcaption style="text-align: center; width: 140%;">
                        Figure 3: MLLMs benefit from pre-training the adapter with more data.
                    </figcaption>
                </div>
                <!-- <p class="text">
                    We suspect this is a product of the composition of the 737K instruction tuning data and the LLM-heavy focus of these benchmarks. 
                    We note that unfreezing the vision backbone introduces additional computational overhead, 
                    which prohibits testing on some larger vision models under current sharding strategies.
                </p> -->
            </div>
            <!-- <h1 class="text">Evaluating Visual Representations through MLLMs</h1>
            <p class="text">
                Current MLLMs predominantly rely on CLIP as the visual encoder due to its pre-alignment with language and ease of adaptation to the LLM token space. 
                However, strong language priors can be a double-edged sword---they compensate for deficiencies in learning effective visual representations 
                and diminish insights gained from extensive visual representation learning research.
            </p> -->
            <div id='visual-representation' class="viusal-representation-block">
            <h1 class="text">MLLMs as a Vision Model Evaluator </h1>
                
                <p class="text">
                    MLLMs explore new aspects of vision models beyond traditional benchmarks like ImageNet-1k, using a 2-stage instruction tuning with 1.2M adapter data and 737K fine-tuning data. 
                    Our evaluations show language-supervised models outperform non-CLIP models across all categories, especially in chart and OCR tasks. Despite the smaller dataset size of SSL models like DINOv2, they perform competitively in vision-centric benchmarks. 
                </p>
                <!-- <p class="text">
                    We evaluate the results on benchmarks detailed in <a href="#sec:benchmarking">Benchmarking the benchmarks</a>, 
                    calculating the average performance for each category and visualize the results in <a href="#fig:mllm_as_interface">Figure 4</a>.
                    Our findings highlight the advantages of language-supervised models over non-CLIP models across all benchmark categories, 
                    with significantly better performance on chart and OCR-related benchmarks. 
                    We hypothesize that this is due to CLIP's <em>training data</em>, such as LAION, 
                    containing abundant OCR and text-heavy data, whereas SSL and other vision models primarily train on natural images with significantly less text content. 
                    It is also noteworthy that language-supervised models are typically trained with a very large pool of data, 
                    ranging from 400 million to 10 billion samples, 
                    whereas the largest vision self-supervised training dataset, like DINOv2, consists of only <em>142 million samples</em>. 
                    The performance comparison in <a href="#fig:mllm_as_interface">Figure 4</a> between DINOv2, other SSL models, 
                    and language-supervised models underscores the potential for training superior vision-only models with more data and improved techniques. 
                    Additionally, we observe that higher-resolution models particularly enhance performance on chart and vision-centric benchmarks
                    while remaining neutral on general VQA and knowledge-based VQAs. 
                    While the majority of the backbones we examine are ViT-based, 
                    <strong>ConvNet-based architectures</strong> (such as OpenCLIP ConvNeXt) are inherently well-suited for 
                    high-resolution image postprocessing and can produce superior results on OCR&Chart and Vision-Centric benchmarks. 
                    In vision-centric benchmarks, the gap between language-supervised and other types of vision models is smaller, 
                    with a well-trained self-supervised DINOv2 model even outperforming some language-supervised models.
                </p> -->
                <div id="fig:mllm_as_interface" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 140%;">
                        <img data-zoomable="" style="width: 100%;" src="static/img/mllm_as_interface.png">
                    </div>
                    <figcaption style="text-align: center; width: 140%;">
                        Figure 4: MLLMs as an interface to evaluate visual representations.
                    </figcaption>
                </div>
                <p class="text">
                    <strong>Bridging the gap between CLIP and SSL models</strong> 
                    In the above, we observe that DINOv2 stands midway between SSL models and CLIP models on general VQA and knowledge VQA tasks, 
                    even outperforming some CLIP models on vision-centric benchmarks with higher resolution. 
                    We investigate unfreezing the vision backbones and increasing the amount of visual fine-tuning data to bridge this gap. 
                    <!-- Specifically, we scale up the instruction tuning data from 0.7M to 5M,  -->
                    In <a href="#fig:bridgegap">Figure 5</a>, we observe that by unfreezing the vision backbone, 
                    the DINOv2-based MLLM fine-tuned with 5M data surpasses the MLLM trained with a CLIP model on 0.7M data. 
                    Additionally, the gap between DINOv2 and the CLIP models is reduced under the 5M data experiment setting.
                </p>
                <div id="fig:bridgegap" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 140%;">
                        <img data-zoomable="" style="width: 100%;" src="static/img/bridge_gap.png">
                    </div>
                    <figcaption style="text-align: center; width: 140%;">
                        Figure 5: Continued fine-tuning of DINOv2 bridges the gap between CLIP and DINOv2.
                    </figcaption>
                </div>
            
                <!-- <h2 id="sec:model_ensemble" class="text">Combining Multiple Vision Encoders </h2> -->
                <p class="text">
                    <strong>Combining Multiple Vision Encoders </strong>
                    As observed in <a href="#fig:mllm_as_interface">Figure 4</a>, different vision models excel in different aspects of MLLM performance. 
                    We explore the potential of combining multiple vision encoders to leverage their distinctive representations.
                    Given that different vision encoders use varying architectures and image resolutions, we interpolate the output visual tokens to a fixed number, 576.
                    The results are tabulated in <a href="#tab:model_ensemble">Table 2</a>, where we observe consistent performance improvements with the addition of more models.
                </p>

                <div id="tab:model_ensemble" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 140%;">
                        <img data-zoomable="" style="width: 100%;" src="static/img/table/1.2m_pretrain.png">
                    </div>
                    <figcaption style="text-align: center; width: 140%;">
                        Table 2: All Benchmark Results for Model Ensemble with 1.2M Adapter Data + 737K
                        Instruction Tuning Data
                    </figcaption>
                </div>
            
                <!-- <p class="text">
                    Our study indicates that adding a non-language-supervised model (DINOv2) can improve benchmark performance, 
                    especially in vision-centric tasks. Notably, even OCR benchmarks benefit from incorporating DINOv2. 
                    This highlights the importance of self-supervised learning models in complementing language-supervised models to achieve robust multimodal understanding. 
                </p> -->
            
                <p class="text">
                    However, this strategy has two limitations: 1) it employs interpolation, which can potentially lead to information loss, 
                    especially on vision encoders with high-resolution feature maps, and 2) it treats each model equally by simple concatenation. 
                    Therefore, we seek a more effective strategy that fully leverages model combinations with less information loss and more flexibility.
                </p>

        </div>
        </div>

        <div id='connector_design' class="connector-block">

            <h1 class="text">Spatial Vision Aggregator (SVA): A New Connector Design</h1>
            <p class="text">
                To effectively aggregate features from multiple vision encoders and reduce information loss during interpolation, we use a set of learnable latent queries that interact with multiple vision features through cross-attention layers<d-cite key="dai2024instructblip"></d-cite>.
                In particular, our approach incorporates two new vision-centric design principles: 
                First, we encode spatial inductive bias by explicitly localizing the aggregation space for each token in the query.
                Second, we perform vision feature aggregation multiple times across the LLM layers, allowing the model to repeatedly refer necessary visual information.
            </p>
            <!-- <p class="text">
                Our new formulation flexibly handles different numbers of vision encoders with varying feature resolutions and remains aware of the spatial structure of visual data throughout the vision information aggregation in the LLM. We elaborate our method below.
                To facilitate cross-attention-based information aggregation, we create a <em>C</em>-dimension learnable latent token <strong>x</strong> ∈ ℝ<sup>C</sup> that is repeated <em>L</em> × <em>L</em> times to form a 2D grid, serving as the query <strong>X</strong> ∈ ℝ<sup>L<sup>2</sup> × C</sup>. The set of visual features <strong>F</strong> from <em>N</em> vision encoders serve as the context (i.e., key and value). We ensure the output resolution of every vision encoder is a multiple of <em>L</em>. Formally, the feature map of the <em>k</em>-th vision encoder (<strong>F</strong><sub>k</sub>) has a resolution of <em>m<sub>k</sub> L</em> × <em>m<sub>k</sub> L</em> × <em>C</em>, where <em>m<sub>k</sub></em> is a positive integer multiplier.
            </p>
            <strong class="text">Spatial inductive bias.</strong>
            <p class="text">
                To maintain the spatial structure during cross-attention, we align each token in the query with a specific sub-region of the feature maps in all vision encoders. Formally, a token at row <em>i</em> and column <em>j</em> in the query <strong>x</strong><sub>i,j</sub> corresponds to the sub-region <strong>F</strong><sub>k</sub>[<em>m<sub>k</sub> · i:m<sub>k</sub> · (i+1), m<sub>k</sub> · j:m<sub>k</sub> · (j+1)</em>] ∈ ℝ<sup>m<sub>k</sub><sup>2</sup> × C</sup> of the <em>k</em>-th vision feature map. As a result, a token <strong>x</strong><sub>i,j</sub> aggregates a total of ∑<sub>k</sub> m<sub>k</sub><sup>2</sup> features from <em>N</em> vision encoders through cross-attention (see <a href="#fig:vision_connector">Figure 6-left</a>).
            </p> -->
            <div id="fig:vision_connector" style="display: flex; flex-direction: column; align-items: center;">
                <div style="display: flex; justify-content: center; width: 120%;">
                    <img data-zoomable="" style="width: 100%;" src="static/img/vision_connector_v7.png">
                </div>
                <figcaption style="text-align: center; width: 120%;">
                    Figure 6: Spatial Vision Aggregator (SVA).
                </figcaption>
            </div>

            <!-- <p class="text">
                Specifically, the updated query vector <strong>q<sup>*</sup></strong><sub>i,j</sub> at position (<em>i,j</em>) is computed as:
            </p> -->
            <!-- <div class="equation">
                <pre>
                    <strong>q<sup>*</sup></strong><sub>i,j</sub> = softmax(<strong>q</strong><sub>i,j</sub> · [<strong>k</strong><sub>i,j,1</sub>, <strong>k</strong><sub>i,j,2</sub>, ..., <strong>k</strong><sub>i,j,N</sub>]<sup>⊤</sup> / √<em>C</em>) [<strong>v</strong><sub>i,j,1</sub>, <strong>v</strong><sub>i,j,2</sub>, ..., <strong>v</strong><sub>i,j,N</sub>],</pre>
            </div>
            <div class="equation">
                <pre>
                    <strong>q</strong><sub>i,j</sub> = <strong>W<sup>Q</sup></strong> <strong>x</strong><sub>i,j</sub> ∈ ℝ<sup>1×C</sup>,
                    <strong>k</strong><sub>i,j,k</sub> = <strong>W<sup>K</sup></strong><sub>k</sub> <strong>F</strong><sub>k</sub>[<em>m<sub>k</sub> · i:m<sub>k</sub> · (i+1), m<sub>k</sub> · j:m<sub>k</sub> · (j+1)</em>] ∈ ℝ<sup>m<sub>k</sub><sup>2</sup>×C</sup>,
                    <strong>v</strong><sub>i,j,k</sub> = <strong>W<sup>V</sup></strong><sub>k</sub> <strong>F</strong><sub>k</sub>[<em>m<sub>k</sub> · i:m<sub>k</sub> · (i+1), m<sub>k</sub> · j:m<sub>k</sub> · (j+1)</em>] ∈ ℝ<sup>m<sub>k</sub><sup>2</sup>×C</sup>
                </pre>
            </div> -->
            <!-- <div class="equation">
                \[
                \mathbf{q^{*}}_{i,j} = \text{softmax}\left(\frac{\mathbf{q}_{i,j} \cdot \left[\mathbf{k}_{i,j,1}, \mathbf{k}_{i,j,2}, \ldots, \mathbf{k}_{i,j,N}\right]^\top}{\sqrt{C}}\right) \left[\mathbf{v}_{i,j,1}, \mathbf{v}_{i,j,2}, \ldots, \mathbf{v}_{i,j,N}\right],
                \]
            </div>
            <div class="equation">
                \[
                \begin{align*}
                \mathbf{q}_{i,j} &= \mathbf{W}^Q \mathbf{x}_{i,j} \in \mathbb{R}^{1 \times C}, \\
                \mathbf{k}_{i,j,k} &= \mathbf{W}^K_{k} \mathbf{F}_{k}[m_{k} \cdot i : m_{k} \cdot (i+1),\ m_{k} \cdot j : m_{k} \cdot (j+1)] \in \mathbb{R}^{m_k^2 \times C}, \\
                \mathbf{v}_{i,j,k} &= \mathbf{W}^V_{k} \mathbf{F}_{k}[m_{k} \cdot i : m_{k} \cdot (i+1),\ m_{k} \cdot j : m_{k} \cdot (j+1)] \in \mathbb{R}^{m_k^2 \times C}.
                \end{align*}
                \]
            </div>
            
            <p class="text">
                Here, <strong>q</strong><sub>i,j</sub> is the query vector at position (<em>i,j</em>), calculated using the query projection matrix <strong>W<sup>Q</sup></strong> ∈ ℝ<sup>C × C</sup>. The key vectors <strong>k</strong><sub>i,j,k</sub> and value vectors <strong>v</strong><sub>i,j,k</sub> are computed for each vision encoder <em>k</em> using their respective key and value projection matrices <strong>W<sup>K</sup></strong><sub>k</sub> ∈ ℝ<sup>C × C</sup> and <strong>W<sup>V</sup></strong><sub>k</sub> ∈ ℝ<sup>C × C</sup>. Since ∑<sub>k</sub> m<sub>k</sub><sup>2</sup> features are aggregated into a single token, we effectively reduce the number of tokens.
            </p>
            <strong class="text">Multi-layer vision aggregation.</strong>
            <p class="text">
                Although our proposal effectively aggregates features from multiple vision encoders, there is still potential information loss when processing high-resolution input (<em>i.e.</em> <em>m<sub>k</sub></em> is large) or when there are multiple vision encoders (<em>i.e.</em> <em>N</em> is large). This results in a single token having to handle a larger amount of context information during aggregation. To alleviate this, we enable cross-attention to occur multiple times by inserting our proposal within the LLM layers, allowing the model to refer to the uncompressed visual information consistently (see <a href="#fig:vision_connector">Figure 6-right</a>).
            </p> -->
        </div>

        <div id="instruction_data" class="data-block">
            <h1 class="text">Instruction Tuning Data for Training MLLMs</h1>
            <p class="text">
                Previous work highlights the importance of data in training MLLMs, but explicit investigations are limited. 
                In this study, we gather all available instruction tuning data and examine data curation by enhancing diversity, balancing sources, and improving mixtures. 
                
            </p>
    
            <div class="subsection">
                <h3 class="text">Data Collection</h3>
                <p class="text" id="data_collection">
                    <strong>Collecting Instruction Tuning Data from existing data sources</strong> 
                    We first use existing multimodal benchmarks and datasets involving visual interaction data,
                    such as Visual Question Answering (VQA) and OCR data. 
                    We also collect a small volume of high-quality language-only instruction-following data to maintain its language ability. 
                    <!-- Data are categorized into General conversation, OCR, Counting, Code, Math, Science, and Language-only data.  -->
                    <!-- We list the data sources in <a href="#fig:cambrian7m">Figure 7</a>. -->
                </p>
                <div id="fig:cambrian7m" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 140%;">
                    <img data-zoomable="" style="width: 100%;" src="static/img/cambrian_7m.png">
                    </div>
                    <figcaption style="text-align: center; width: 140%;">
                        Figure 7: Cambrian-7M: A Large-Scale Curated Instruction Tuning Dataset for Training MLLM.
                    </figcaption>
                </div>
                <p class="text">
                    <strong>Targeted Internet Data Collection Engine</strong> 
                    We also introduce a data engine designed to create large-scale, reliable, 
                    high-quality knowledge-based multimodal instruction tuning data.
                    <!-- The engine selects a target field and subfield, such as "Physics," 
                    and uses an LLM like GPT-4 to identify topics (e.g., "Newton’s Laws"). 
                    It searches reliable sources like Wikipedia for each topic. 
                    We find that Wikipedia cites exhibit high-correlated text-image associations. 
                    The parser extracts image-caption-text tuples and feeds the caption-text to an LLM, such as GPT-3., 
                    to generate instruction-type Q&A pairs about the image. 
                    These Q&A pairs and the image form our VQA dataset. -->
                     <!-- Details are in <a href="#Appendix:DataEngine">Appendix: Data Engine</a>. -->
                </p>
                <div id="fig:dataengine" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 140%;">
                    <img data-zoomable="" style="width: 100%;" src="static/img/dataenginefigurepdf_crop.png">
                    </div>
                    <figcaption style="text-align: center; width: 140%;">
                        Figure 8: Targeted Internet Data Collection Engine.
                    </figcaption>
                </div>
    
                <p class="text">
                    <strong>Cambrian-10M</strong> 
                    To this end, we create a large pool of instruction tuning data, which we refer to as Cambrian-10M. 
                    This pool contains approximately 9784k data points, offering a diverse range of data for our work and future research. 
                    We also visualize its composition in <a href="#fig:cambrian7m">Figure 7</a>.
                </p>
            </div>
    
            <div id="sec:data_curation" class="subsection">
                <h3 class="text">Data Curation</h3>
                <p class="text">
                    Cambrian-10M is a large pool of instruction tuning data sourced from a variety of data sources, 
                    with an unbalanced data ratio between categories. 
                    Here, we take a preliminary step to study data curation by improving data balancing and adjusting data ratios.
                </p>
    
                <p class="text" id="data_curation">
                    <strong>Data Balancing</strong> 
                    We follow previous work to set thresholds t 
                    for the number of data points from a single data source. 
                    <!-- To study the effect of the number t, 
                    we plot the cumulative sum of counts for entries sorted by counts from tail to head 
                    (see Visualization in <a href="#fig:filter_k">Figure 9</a>).  -->
                    We choose t = 150k, 250k, 350k, and 450k in this section and observe an 
                    elbow effect in <a href="#tab:data_balance_result">Table 3</a>. 
                    We find that a threshold between 250k and 350k works the best for Cambrian-10M.
                </p>
                <div id="fig:filter_k" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 100%;">
                    <img data-zoomable="" style="width: 100%;" src="static/img/Cumulative_Sum_of_Counts.png">
                    </div>
                    <figcaption style="text-align: center; width: 100%;">
                        Figure 9: Data Balancing via Applying Thresholds on Data Sources.
                    </figcaption>
                </div>

                <div id="tab:data_balance_result" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 100%;">
                    <img data-zoomable="" style="width: 100%;" src="static/img/table/data_filter_k.png">
                    </div>
                    <figcaption style="text-align: center; width: 100%;">
                        Table 3: Threshold 𝑡 value between 250k and 350k obtains better performance.
                    </figcaption>
                </div>

                <p class="text">
                    <strong>Data Ratio</strong> 
                    <!-- Unlike previous works in VLM data curation<d-cite key="xu2023demystifying"></d-cite> <d-cite key="gadre2024datacomp"></d-cite>, 
                    which curate noisy raw image-text pairs by scraping the internet, 
                    Cambrian-10M is designed for visual instruction tuning.  -->
                    Given the various capabilities of different types of visual instruction tuning data, it is essential to balance the ratio of these data types. 
                    We conduct pilot experiments with a fixed dataset size of 1350k, 
                    examining the impact of different data ratios on downstream performance. 
                    We visualize the results in <a href="#fig:data_ratio">Figure 10</a> and summarize our findings as follows: 
                    (i) Balancing General, OCR, and Language data is crucial. 
                    <!-- The model's OCR capability is proportional to the OCR data ratio; 
                    however, an excessive OCR ratio compromises general VQA and vision-centric performance. -->
                    (ii) Performance on knowledge-intensive tasks is influenced by multiple factors, 
                    often requiring a mix of OCR, chart, reasoning, and general perception. 
                    <!-- Increasing the science data ratio can help, but a very low ratio leads to poor performance. -->
                </p>
                
                <div id="fig:data_ratio" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 100%;">
                    <img data-zoomable="" style="width: 100%;" src="static/img/data_mixture_ratio_w_avg_score.png">
                    </div>
                    <figcaption style="text-align: center; width: 100%;">
                        Figure 10: Exploring instruction tuning data mixture ratios.
                    </figcaption>
                </div>

                <p class="text">
                    <strong>Cambrian-7M</strong> 
                    We follow the identified data ratio and apply the data filtering technique while curating to that ratio. 
                    In the end, we obtain Cambrian-7M. In <a href="#tab:data_ratio_result">Table 4</a>, 
                    we observe improvements by scaling up and curating better data, even with less quantity.
                </p>
                <div id="tab:data_ratio_result" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 100%;">
                    <img data-zoomable="" style="width: 100%;" src="static/img/table/data_ratio_result.png">
                    </div>
                    <figcaption style="text-align: center; width: 100%;">
                        Table 4: Performance improves with better instruction tuning data curation.
                    </figcaption>
                </div>
                

            </div>
    
            <div class="subsection">
                <h3 class="text">Resolving "Answer Machine Phenomenon" with System Prompts</h3>
                <p class="text">
                    Here, we explore and analyze a phenomenon we term the "answer machine phenomenon." 
                    We observe that a well-trained MLLM excels in visual question answering 
                    but lacks basic conversational abilities (see examples in <a href="#fig:sysprompt">Figure 5</a>). 
                    <!-- The model tends to output shorter responses. -->
                    <!-- This discrepancy arises because benchmark questions typically require responses that are limited to 
                    a single option, choice, or word, which diverges from the broader and actual use cases of MLLMs. 
                    Similar phenomena have been discussed in other LLM studies. -->
                </p>
    
                <p class="text">
                    <!-- We suspect that this issue stems from the instruction tuning data containing an excessive number of short-response VQA tasks, 
                    leading to catastrophic forgetting in LLMs. -->
                    To address this, we find that incorporating additional system prompts during training mitigates this phenomenon. 
                    We append prompts such as "<em>Answer the question using a single word or phrase.</em>" 
                    before questions that generate a single word or phrase in the response. 
                    We observe that after integrating these system prompts, the model's benchmark performance remains unchanged, 
                    while its conversational ability significantly improves. 
                    <!-- For example, in <a href="#fig:sysprompt">Figure 11</a>, 
                    models with system prompts tend to output longer and more engaging outputs while answering the questions correctly. 
                    It also encourages the model to improve on reasoning-related tasks such as math problems by outputting a series of thoughts, 
                    and then output the answer. -->
                </p>
                <div id="fig:sysprompt" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 140%;">
                    <img data-zoomable="" style="width: 100%;" src="static/img/sysprompt.png">
                    </div>
                    <figcaption style="text-align: center; width: 140%;">
                        Figure 11: Incorporating System Prompt in Instruction Tuning Data alleviates “Answer
                        Machine Phenomenon”.
                    </figcaption>
                </div>
    
                <!-- <p class="text">
                    Further, this underscores the necessity to develop evaluation protocols like the Chatbot Arena, despite the challenges in collecting large-scale, actual use cases of multimodal data.
                </p> -->
            </div>
        </div>

        <div id='sota' class="sota-block">
            <h1 class="text">State of the Art MLLM Performance</h1>
            <p class="text">
                Finally, we leverage the insights from all of our previous studies to train a high-performance Cambrian model.
                We train with three different sizes of LLM backbones: LLaMA-3-Instruct-8B, Vicuna-1.5-13B, and Hermes-2-Yi-34B.
                We have a vision combination of four models—SigLIP, CLIP, DINOv2, and OpenCLIP ConvNeXt 
                (see <a href="#sec:model_ensemble">Combining Multiple Vision Encoders</a>) with <a href="#connector_design">Spatial Vision Aggregator</a>.
                We use 2.5M adapter data and Cambrian-7M instruction tuning data (see <a href="#sec:data_curation">Data Curation</a>).
                We evaluate our models on the <a href="#sec:benchmarking">categorized benchmarks</a>. 
                We show the results in <a href="#tab:final_table">Table 5</a>. Cambrian-1 exceeds other open-source models such as LLaVA-NeXT and Mini-Gemini. 
                Cambrian-1 also achieves comparable performance on a number of benchmarks with the best proprietary models such as GPT-4V, Gemini-Pro, and MM-1.
            </p>
            <div id="tab:final_table" style="display: flex; flex-direction: column; align-items: center;">
                <div style="display: flex; justify-content: center; width: 140%;">
                <img data-zoomable="" style="width: 100%;" src="static/img/table/final_result.png">
                </div>
                <figcaption style="text-align: center; width: 140%;">
                    Table 5: Performance improves with better instruction tuning data curation.
                </figcaption>
            </div>
            <div id="tab:final_table" style="display: flex; flex-direction: column; align-items: center;">
                <div style="display: flex; justify-content: center; width: 140%;">
                <img data-zoomable="" style="width: 100%;" src="static/img/comparison.PNG">
                </div>
                <figcaption style="text-align: center; width: 140%;">
                    Figure 12: Cambrian-1 outperforms other open-source models and achieves comparable performance with proprietary models.
                </figcaption>
            </div>

        </div>

        <div id="conclusion" style="position: relative; margin-top: 40px; margin-bottom: 0px;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">Conclusion</h2>
            <p class="text">
                To conclude, Cambrian-1 introduces a family of state-of-the-art MLLM models that achieve top performance across diverse benchmarks 
                and excel in visual-centric tasks. We provide model weights, open-source code, datasets, and detailed recipes for model training and evaluation. 
                We hope our work will strengthen the open research community and accelerate research in both visual representation learning and multimodal systems.
            </p>
        </div>

        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @article{tong2024cambrian,<br>
                &nbsp;&nbsp;title={{Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs}},<br>
                &nbsp;&nbsp;author={Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng, and Iyer, Adithya and Pan, Xichen and Wang, Austin and LeCun, Yann and Xie, Saining},<br>
                &nbsp;&nbsp;year={2024},<br>
                &nbsp;&nbsp;journal={arXiv preprint},<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>
<!--         
        <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <d-bibliography src="bibliography.bib"></d-bibliography>
        <script src="./static/js/nav-bar.js"></script>
         -->
    </body>
</html>
