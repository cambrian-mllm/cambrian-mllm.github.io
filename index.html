<!doctype html>
<html lang="en">
    <head>
        <title>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</title>
        <link rel="icon" type="image/x-icon" href="static/img/icons/earth_icon.png">

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:url" content="https://virl-platform.github.io/" />
        <meta property="og:image" content="https://virl-platform.github.io/static/img/preview.png" />
        <meta name="twitter:url" content="https://virl-platform.github.io/" />
        <meta name="twitter:image" content="https://virl-platform.github.io/static/img/preview.png" />

        <script src="./static/js/distill_template.v2.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

        <script defer="" src="./static/js/hider.js"></script>
        <script src="./static/js/image_interact.js"></script>
        <script src="./static/js/switch_videos.js"></script>

        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>

        <!-- medium zoom https://github.com/francoischalifour/medium-zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>  <!-- jquery -->
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>
    </head>
    <body>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <div class="header-content">
                    <h1 style="margin-top: 0px"><i>Cambrian-1</i></h1>
                    <h2>A Fully Open, <i>Vision-Centric</i><br>
                        Exploration of Multimodal LLMs</h2>
                    <p style="color: #FFF7D4">
                        Introducing Cambrian-1, a family of 
                        <em><strong style="color: #ffe099">vision-centric</strong></em>
                        multimodal LLMs (MLLMs).
                        Cambrian-1 is structured around five key pillars:
                        <ul style="color: #FFF7D4">
                            <li><strong style="color: #ffe099">Visual Representations</strong>: We explore various vision encoders and their combinations.
                            <li><strong style="color: #ffe099">Connector Design</strong>: We design a new dynamic and spatially-aware connector that integrates vision features with LLMs while reducing the number of tokens.
                            <li><strong style="color: #ffe099">Instruction Tuning Data</strong>: We curate high-quality visual instruction-tuning data from public sources, emphasizing the importance of distribution balancing. 
                            <li><strong style="color: #ffe099">Instruction Tuning Recipes</strong>: We discuss instruction tuning strategies and practices. 
                            <li><strong style="color: #ffe099">Benchmarking</strong>: We examine existing MLLM benchmarks and introduce a new vision-centric benchmark "CV-Bench".
                        </ul>
                    </p>
                    <div class="button-container">
                        <!-- replace arxiv -->
                        <a href="https://arxiv.org/abs/2402.03310" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="ai ai-arxiv"></i>
                            </span>
                            arXiv
                        </a>
                        <!-- replace pdf -->
                        <a href="./static/V-IRL.pdf" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="fas fa-file-pdf"></i>
                            </span>
                            <span>pdf</span>
                        </a>
                        <!-- replace image -->
                        <a href="https://github.com/VIRL-Platform/VIRL" class="button" target="_blank">
                            <span class="icon is-small">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                        </a>
                    </div>
                </div>
                <div class="header-image">
                    <img src="static/img/teaser_img_v3.jpg" alt="Teaser Image" class="teaser-image">
                </div>
            </div>
        </div>
    <d-article>
        <div class="byline">
            <div class="byline-container">
                <div class="byline-column">
                    <h3>Authors</h3>
                    <p><a href="https://tsb0601.github.io/petertongsb/" class="author-link" target="_blank">Shengbang Tong</a> &emsp;
                    <a href="https://ellisbrown.github.io/" class="author-link" target="_blank">Ellis Brown</a> &emsp;
                    <a href="https://penghao-wu.github.io/" class="author-link" target="_blank">Penghao Wu</a> &emsp;
                    <p><a href="https://sites.google.com/view/sanghyunwoo/" class="author-link" target="_blank">Sanghyun Woo</a> &emsp;
                    <a href="https://github.com/adithyaiyer1999" class="author-link" target="_blank">Adithya Jairam Iyer</a> &emsp;
                    <p><a href="https://www.linkedin.com/in/sai-charitha-akula-32574887" class="author-link" target="_blank">Sai Charitha Akula</a> &emsp;
                    <a href="https://github.com/vealocia" class="author-link" target="_blank">Shusheng Yang</a> &emsp;
                    <p><a href="https://jihanyang.github.io/" class="author-link" target="_blank">Jihan Yang</a> &emsp;
                    <a href="https://www.linkedin.com/in/manoj-middepogu/" class="author-link" target="_blank">Manoj Middepogu</a> &emsp;
                    <a class="author-link" target="_blank">Ziteng Wang</a> &emsp;
                    <p><a href="https://xichenpan.com/" class="author-link" target="_blank">Xichen Pan</a> &emsp;
                    <a href="https://cs.nyu.edu/~fergus/pmwiki/pmwiki.php" class="author-link" target="_blank">Rob Fergus</a> &emsp;
                    <a href="https://yann.lecun.com/" class="author-link" target="_blank">Yann LeCun</a> &emsp;
                    <a href="https://www.sainingxie.com/" class="author-link" target="_blank">Saining Xie</a>
                </div> 
                <div class="byline-column">
                    <h3>Affiliations</h3>
                    <p>
                        <a href="https://cs.nyu.edu/home/index.html" class="affiliation-link" target="_blank">New York University</a>
                    </p>
                </div>
            </div>
        </div>

        <!-- <div class="nav-bar" id="nav-bar">
            <a class="nav-link" href="#top" style="opacity: 0.7">
                <div style="margin: 8px 0px; text-align: center">
                    <span style="font-size: 30px;">&#128285;</span>
                </div>
            </a>
            <hr style="display: block; margin: auto;">
            <div class="geo-color">
                <a class="nav-link" href="#geo">
                    <img class="virl-tag" src="static/img/tags/geo.png">
                </a>
                <a class="nav-link" href="#peng"><img src="static/img/avatars/courier.png" title="Peng: visiting student">
                </a>
            </div>
            <hr style="display: block; margin: auto;">
            <div class="llm-color">
                <a class="nav-link" href="#language">
                    <img class="virl-tag" src="static/img/tags/lm.png">
                </a>
                <a class="nav-link" href="#aria"><img src="static/img/avatars/recommender.png" title="Aria: place recommender">
                </a>
                <a class="nav-link" href="#vivek"><img src="static/img/avatars/real_estate.png" title="Vivek: estate agent">
                </a>
            </div>
            <hr style="display: block; margin: auto;">
            <div class="cv-color">
                <a class="nav-link" href="#vision">
                    <img class="virl-tag" src="static/img/tags/cv.png">
                </a>
                <a class="nav-link" href="#rx-399"><img src="static/img/avatars/robot.png" title="RX-399: urban assistant robot">
                </a>
                <a class="nav-link" href="#imani"><img src="static/img/avatars/urban_planner.png" title="Imani: urban planner">
                </a>
                <a class="nav-link" href="#hiro"><img src="static/img/avatars/explorer.png" title="Hiro: explorer">
                </a>
            </div>
            <hr style="display: block; margin: auto;">
            <div class="col-color">
                <a class="nav-link" href="#collaboration">
                    <img class="virl-tag" src="static/img/tags/col.png">
                </a>
                <a class="nav-link" href="#ling">
                    <img src="static/img/avatars/tourist.png" title="Ling: tourist">
                </a>
                <a class="nav-link" href="#diego">
                    <img src="static/img/avatars/concierge.png" title="Diego: expert concierge">
                </a>
            </div>
            <hr style="display: block; margin: auto;">
            <div id="nav-bar-system">
                <a class="nav-link" href="#system"><img src="static/img/icons/system.png" title="System fundamentals"></a>
            </div>
            <hr style="display: block; margin: auto;">
            <div id="nav-bar-benchmark">
                <a class="nav-link" href="#benchmark"><img src="static/img/icons/benchmark.png" title="V-IRL Benchmark"></a>
            </div>
        </div> -->

        <p class="text abstract">

            We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a <strong>vision-<i>centric</i></strong> approach. 
            While stronger language models can enhance multimodal capabilities, 
            the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. 
            This gap hinders accurate sensory grounding in real-world scenarios. 
            <br><br>
            Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations,
            offering new insights into different models and architectures—self-supervised, strongly supervised, 
            or combinations thereof—based on experiments with over 15 vision models.
            We critically examine existing MLLM benchmarks, addressing the difficulties involved in consolidating and 
            interpreting results from various tasks and introduce a new visioncentric benchmark “CV-Bench”.
            We also discuss instruction tuning strategies and practices (see <a href="#visual_representations">&sect;Evaluating Visual Representations through MLLMs</a>).
            <br><br>
            To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), 
            a dynamic and spatially-aware connector that integrates high-resolution vision features 
            with LLMs while reducing the number of tokens (see <a href="#connector_design">&sect;Spatial Vision Aggregator</a>).
            <br><br>
            Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, 
            emphasizing the importance of data source balancing and distribution ratio (see <a href="#instruction_data">&sect;Instruction Tuning Data for Training MLLMs</a>).
            <br><br>
            Collectively, Cambrian-1 not only achieves state-of-the-art performances but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs
            (see <a href="#sota">&sect;State-of-the-art MLLM performance</a>). 
            We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. 
            We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.
        </p>

        <hr>

        <div id='visual_representations' class="vision-block">
            
            <h1 class="text">Evaluating Visual Representations through MLLMs</h1>
            <p class="text">
                Current MLLMs predominantly rely on CLIP as the visual encoder due to its pre-alignment with language and ease of adaptation to the LLM token space. 
                However, strong language priors can be a double-edged sword---they compensate for deficiencies in learning effective visual representations 
                and diminish insights gained from extensive visual representation learning research.
            </p>
            <div id="sec:benchmarking" class="sub-section">
                <h2 class="text">Benchmarking the Benchmarks</h2>
                    <p class="text">
                        To effectively evaluate visual representations and MLLMs, we first need to select benchmarks that accurately assess the <i>multimodal</i> 
                        capabilities of these models. We use a suite of commonly used benchmarks and begin by benchmarking the benchmarks themselves.
                        Here, we train MLLMs with 15 different vision backbones from a variety of model families using a 2-stage instruction tuning process: 
                        first training connector on 1.2M adapter data from ShareGPT-4V followed by fine-tuning both the connector and LLM on 737K instruction tuning data.
                    <br><br>
                        <strong>Who's answering the question: the LLM or MLLM?</strong> Over the years in vision-language research, 
                        a persistent challenge has been determining whether each benchmark <em>truly</em> needs visual input for answering the question. 
                        In this study, we also compare the performance of MLLMs with and without visual input. 
                        For each of the 15 trained MLLMs, we disable the visual input and evaluate them across all benchmarks. 
                        Additionally, we calculate the expected score by randomly guessing. 
                        These three conditions are visualized in <a href="#fig:comparison">Figure 1</a> (left), with benchmarks sorted by the difference between the average score with vision enabled and disabled. 
                        As shown in <a href="#fig:comparison">Figure 1</a> (left), SQA-I, MMMU, and AI2D have less than a 5-point gap between vision enabled and disabled, suggesting that these benchmarks may not significantly rely on visual input. 
                        V-star exhibits a 10-point gap, but it shows no gap between vision-disabled and random guessing, indicating that an LLM alone cannot solve the questions. 
                        TextVQA and GQA both demonstrate a nearly 40-point positive gap between random guessing and vision-disabled scores, implying a strong language bias in these benchmarks. 
                        On the other hand, benchmarks like MMVP and MME show that vision-disabled performance is notably worse than random guessing, suggesting that accurate visual grounding is particularly crucial for these tasks.        
                    </p>
                    <div id="fig:comparison" style="display: flex; flex-direction: column; align-items: center;">
                        <div style="display: flex; justify-content: center; width: 140%;">
                            <img data-zoomable="" style="width: 100%;" src="static/img/bench_cat.png" alt="benchmark category">
                        </div>
                        <figcaption style="text-align: left; width: 140%;">
                            Figure 1: Left: Performance comparison of MLLMs with visual input enabled and disabled across various benchmarks. Benchmarks are sorted by the difference between the average score with vision enabled and disabled. Right: Principal component analysis displaying clusters of benchmarks based on performance metrics, with bubble size corresponding to benchmark size.
                        </figcaption>
                    </div>
                    <p class="text">
                        <strong>Clustering the Benchmarks</strong> To better understand the different aspects of MLLM performance, we analyze the correlations between the performance of our 15 MLLMs on each benchmark.
                        A confusion matrix (<a href="#fig:correlation_plot">Figure 2</a>) reveals that certain benchmarks, such as MMMU, are largely uncorrelated with the others.
                        We perform principal component analysis on the benchmark scores and observe the natural formation of clusters corresponding to "General," "Knowledge," "Chart & OCR," and "Vision-Centric" categories (<a href="#fig:comparison">Figure 1</a> right). We assign MMMU to the knowledge category based on the types of questions it includes (see <a href="#Appendix">Appendix</a>: Benchmarking the Benchmarks).
                        We also find that existing vision-centric benchmarks are of insufficient size, challenging the robustness of evaluating such capabilities. Furthermore, these benchmarks do not cover crucial visual elements such as depth and spatial awareness. 
                    </p>
                    <div id="fig:correlation_plot" style="display: flex; flex-direction: column; align-items: center;">
                        <div style="display: flex; justify-content: center; width: 100%;">
                            <img data-zoomable="" style="width: 100%;" src="static/img/correlation_matrix.png" alt="correlation matrix">
                        </div>
                        <figcaption style="text-align: center; width: 100%;">
                            Figure 2: Correlation matrix for MLLM benchmarks.
                        </figcaption>
                    </div>
            </div>
            <div id="cv-bench" class="sub-section">
                <h2 class="text">Cambrian Vision-Centric Benchmark (CV-Bench) </h2>
                    <p class="text">
                        To address the limitations of existing vision-centric benchmarks mentioned above, we introduce the Cambrian Vision-Centric Benchmark (CV-Bench). 
                        By repurposing standard vision benchmarks, we aim to assess models on classic vision tasks in a multimodal context. 
                        We create vision-centric VQA questions by leveraging rich ground truth information provided by the benchmarks. 
                        The core objective is to precisely ground described objects in the prompt and subsequently perform various fundamental 2D and 3D visual understanding tasks.
                    </p>

                    <table>
                        <thead>
                            <tr>
                                <th>Type</th>
                                <th>Task</th>
                                <th>Description</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td rowspan="2"><strong>2D</strong></td>
                                <td><strong>Spatial Relationship</strong></td>
                                <td>Determine the relative position of an object with respect to the anchor object. Consider either the left-right or top-bottom relationship.</td>
                            </tr>
                            <tr>
                                <td><strong>Object Count</strong></td>
                                <td>Determine the number of instances present in the image.</td>
                            </tr>
                            <tr>
                                <td rowspan="2"><strong>3D</strong></td>
                                <td><strong>Depth Order</strong></td>
                                <td>Determine which of the two distinct objects is closer to the camera.</td>
                            </tr>
                            <tr>
                                <td><strong>Relative Distance</strong></td>
                                <td>Determine which of the two distinct objects is closer to the anchor object.</td>
                            </tr>
                        </tbody>
                    </table>
            </div>
            <h2 class="text">Instruction Tuning Recipes </h2>
                <p class="text">
                    MLLMs start with pre-trained LLM and vision backbones, connecting these modules with a connector such as a projector (MLP). 
                    The original LLaVA proposes a 2-stage frozen training process: 
                    first, pre-training a connector between frozen LLM and vision backbones using adapter data (such as VQA based on captions), 
                    and then fine-tuning both the connector and LLM with instruction tuning data while leaving the vision encoder frozen. 
                    Various studies have drawn different conclusions regarding the optimal training methodology for MLLMs. 
                    Here, we revisit this topic with extensive experiments.
                </p>
                <p class="text">
                    For our experiments, we tune a set of MLLMs using Vicuna-7B as the LLM backbone and each of our 15 vision models 
                    (<a href="#tab:vision_backbones">Table 1</a>) as the visual encoder.
                    We use a 737K instruction tuning data mix for all experiments here. 
                    All hyperparameters are matched across each experimental setting—highlighting the impact of different tuning strategies with each visual encoder. 
                </p>
                <div id="tab:vision_backbones" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 100%;">
                        <img data-zoomable="" style="width: 100%;" src="static/img/table/visual_backbones.png">
                    </div>
                    <figcaption style="text-align: center; width: 100%;">
                        Table 1: Summary of vision backbones categorization and setups.
                    </figcaption>
                </div>
                <p class="text">
                    <strong>One Stage vs Two Stage Training</strong>
                    Recent work advocates for skipping connector pre-training, claiming this “reduces compute cost without harming downstream performance.” 
                    To explore whether this claim holds, especially when using non-language-supervised visual encoders, 
                    we conduct experiments using 0, 0.5M, and 1.2M adapter data. 
                    Following LLaVA's recipe <d-cite key="liu2023visual"></d-cite>, we tune only the connector on the adapter data during this first phase, 
                    before unfreezing the LLM and connector during instruction tuning on the 737K mix.
                </p>
                <p class="text">
                    <a href="#fig:studyadapter">Figure 3</a> shows that pre-training the connector first enhances model performance, 
                    and training with additional adapter data further improves performance across all domains. 
                    Thus, we subsequently adopt 2-stage training with 1.2M adapter data as our standard setup.
                </p>
                <div id="fig:studyadapter" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 140%;">
                        <img data-zoomable="" style="width: 100%;" src="static/img/performance_plot.png">
                    </div>
                    <figcaption style="text-align: center; width: 140%;">
                        Figure 3: MLLMs benefit from pre-training the adapter with more data.
                    </figcaption>
                </div>
                <p class="text">
                    <strong>Freeze vs Unfreeze Vision Encoder</strong>
                    There are also mixed practices in freezing or unfreezing vision backbones during fine-tuning. 
                    Some argue that unfreezing the vision backbone significantly degrades performance. 
                    Our experiments demonstrate that, with a reasonable vision model learning rate, 
                    unfreezing benefits performance across all benchmarks except for a marginal change in Knowledge benchmarks.
                </p>
                <p class="text">
                    We suspect this is a product of the composition of the 737K instruction tuning data and the LLM-heavy focus of these benchmarks. 
                    We note that unfreezing the vision backbone introduces additional computational overhead, 
                    which prohibits testing on some larger vision models under current sharding strategies.
                </p>
            <h2 class="text">MLLMs as a Vision Model Evaluator </h2>
                
                <p class="text">
                    As discussed in earlier sections, MLLMs provide a new interface to explore aspects of vision models beyond traditional benchmarks like ImageNet-1k linear probing. 
                    We study the 2-stage instruction tuning setting using 1.2M adapter data, 737K fine-tuning data, and frozen visual encoders to allow comparison of the widest range of models.
                </p>
                <p class="text">
                    We evaluate the results on benchmarks detailed in <a href="#sec:benchmarking">Benchmarking the benchmarks</a>, 
                    calculating the average performance for each category and visualize the results in <a href="#fig:mllm_as_interface">Figure 4</a>.
                    Our findings highlight the advantages of language-supervised models over non-CLIP models across all benchmark categories, 
                    with significantly better performance on chart and OCR-related benchmarks. 
                    We hypothesize that this is due to CLIP's <em>training data</em>, such as LAION, 
                    containing abundant OCR and text-heavy data, whereas SSL and other vision models primarily train on natural images with significantly less text content. 
                    It is also noteworthy that language-supervised models are typically trained with a very large pool of data, 
                    ranging from 400 million to 10 billion samples, 
                    whereas the largest vision self-supervised training dataset, like DINOv2, consists of only <em>142 million samples</em>. 
                    The performance comparison in <a href="#fig:mllm_as_interface">Figure 4</a> between DINOv2, other SSL models, 
                    and language-supervised models underscores the potential for training superior vision-only models with more data and improved techniques. 
                    Additionally, we observe that higher-resolution models particularly enhance performance on chart and vision-centric benchmarks
                    while remaining neutral on general VQA and knowledge-based VQAs. 
                    While the majority of the backbones we examine are ViT-based, 
                    <strong>ConvNet-based architectures</strong> (such as OpenCLIP ConvNeXt) are inherently well-suited for 
                    high-resolution image postprocessing and can produce superior results on OCR&Chart and Vision-Centric benchmarks. 
                    In vision-centric benchmarks, the gap between language-supervised and other types of vision models is smaller, 
                    with a well-trained self-supervised DINOv2 model even outperforming some language-supervised models.
                </p>
                <div id="fig:mllm_as_interface" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 140%;">
                        <img data-zoomable="" style="width: 100%;" src="static/img/mllm_as_interface.png">
                    </div>
                    <figcaption style="text-align: center; width: 140%;">
                        Figure 4: MLLMs as an interface to evaluate visual representations.
                    </figcaption>
                </div>
                <p class="text">
                    <strong>Bridging the gap between CLIP and SSL models</strong> 
                    In the above, we observe that DINOv2 stands midway between SSL models and CLIP models on general VQA and knowledge VQA tasks, 
                    even outperforming some CLIP models on vision-centric benchmarks with higher resolution. 
                    Here, we study whether the continued finetuning of an MLLM based on an SSL model can achieve performance similar to that of CLIP models. 
                    Given that DINOv2 is trained with much less data compared to CLIP models, 
                    we investigate unfreezing the vision backbones and increasing the amount of visual fine-tuning data to bridge this gap. 
                    Specifically, we scale up the instruction tuning data from 0.7M to 5M, 
                    and instruction tune MLLMs with (1) a DINOv2 ViT-L/14@336 encoder and (2) with a CLIP ViT-L/14@336 encoder in both frozen and unfrozen settings.
                    In <a href="#fig:bridgegap">Figure 5</a>, we observe that by unfreezing the vision backbone, 
                    the DINOv2-based MLLM fine-tuned with 5M data surpasses the MLLM trained with a CLIP model on 0.7M data. 
                    Additionally, the gap between DINOv2 and the CLIP models is reduced under the 5M data experiment setting.
                </p>
                <div id="fig:bridgegap" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 140%;">
                        <img data-zoomable="" style="width: 100%;" src="static/img/bridge_gap.png">
                    </div>
                    <figcaption style="text-align: center; width: 140%;">
                        Figure 5: Continued fine-tuning of DINOv2 bridges the gap between CLIP and DINOv2.
                    </figcaption>
                </div>
            
                <h2 id="sec:model_ensemble" class="text">Combining Multiple Vision Encoders </h2>
                <p class="text">
                    As observed in <a href="#fig:mllm_as_interface">Figure 4</a>, different vision models excel in different aspects of MLLM performance. 
                    In this study, we explore the potential of combining multiple vision encoders to leverage their distinctive representations, aiming to build a more capable MLLM.
                </p>
            
                <p class="text">
                    Given that different vision encoders use varying architectures and image resolutions, we interpolate the output visual tokens to a fixed number, 576, in this subsection. 
                    We then concatenate these tokens along the feature dimension, following a method similar to A-MoF. 
                    The results are tabulated in <a href="#tab:model_ensemble">Table 2</a>, where we observe consistent performance improvements with the addition of more models.
                </p>

                <div id="tab:model_ensemble" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 140%;">
                        <img data-zoomable="" style="width: 100%;" src="static/img/table/1.2m_pretrain.png">
                    </div>
                    <figcaption style="text-align: center; width: 140%;">
                        Table 2: All Benchmark Results for Model Ensemble with 1.2M Adapter Data + 737K
                        Instruction Tuning Data
                    </figcaption>
                </div>
            
                <p class="text">
                    Our study indicates that adding a non-language-supervised model (DINOv2) can improve benchmark performance, 
                    especially in vision-centric tasks. Notably, even OCR benchmarks benefit from incorporating DINOv2. 
                    This highlights the importance of self-supervised learning models in complementing language-supervised models to achieve robust multimodal understanding. 
                </p>
            
                <p class="text">
                    However, this strategy has two limitations: 1) it employs interpolation, which can potentially lead to information loss, 
                    especially on vision encoders with high-resolution feature maps, and 2) it treats each model equally by simple concatenation. 
                    Therefore, we seek a more effective strategy that fully leverages model combinations with less information loss and more flexibility.
                </p>

        </div>

        <div id='connector_design' class="connector-block">

            <h1 class="text">Spatial Vision Aggregator (SVA): A New Connector Design</h1>
            <p class="text">
                To effectively aggregate features from multiple vision encoders and reduce information loss during interpolation, we use a set of learnable latent queries that interact with multiple vision features through cross-attention layers<d-cite key="dai2024instructblip"></d-cite>.
                In particular, our approach incorporates two new vision-centric design principles: 
                First, we encode spatial inductive bias by explicitly localizing the aggregation space for each token in the query.
                Second, we perform vision feature aggregation multiple times across the LLM layers, allowing the model to repeatedly refer necessary visual information.
            </p>
            <p class="text">
                Our new formulation flexibly handles different numbers of vision encoders with varying feature resolutions and remains aware of the spatial structure of visual data throughout the vision information aggregation in the LLM. We elaborate our method below.
                To facilitate cross-attention-based information aggregation, we create a <em>C</em>-dimension learnable latent token <strong>x</strong> ∈ ℝ<sup>C</sup> that is repeated <em>L</em> × <em>L</em> times to form a 2D grid, serving as the query <strong>X</strong> ∈ ℝ<sup>L<sup>2</sup> × C</sup>. The set of visual features <strong>F</strong> from <em>N</em> vision encoders serve as the context (i.e., key and value). We ensure the output resolution of every vision encoder is a multiple of <em>L</em>. Formally, the feature map of the <em>k</em>-th vision encoder (<strong>F</strong><sub>k</sub>) has a resolution of <em>m<sub>k</sub> L</em> × <em>m<sub>k</sub> L</em> × <em>C</em>, where <em>m<sub>k</sub></em> is a positive integer multiplier.
            </p>
            <strong class="text">Spatial inductive bias.</strong>
            <p class="text">
                To maintain the spatial structure during cross-attention, we align each token in the query with a specific sub-region of the feature maps in all vision encoders. Formally, a token at row <em>i</em> and column <em>j</em> in the query <strong>x</strong><sub>i,j</sub> corresponds to the sub-region <strong>F</strong><sub>k</sub>[<em>m<sub>k</sub> · i:m<sub>k</sub> · (i+1), m<sub>k</sub> · j:m<sub>k</sub> · (j+1)</em>] ∈ ℝ<sup>m<sub>k</sub><sup>2</sup> × C</sup> of the <em>k</em>-th vision feature map. As a result, a token <strong>x</strong><sub>i,j</sub> aggregates a total of ∑<sub>k</sub> m<sub>k</sub><sup>2</sup> features from <em>N</em> vision encoders through cross-attention (see <a href="#fig:vision_connector">Figure 6-left</a>).
            </p>
            <div id="fig:vision_connector" style="display: flex; flex-direction: column; align-items: center;">
                <div style="display: flex; justify-content: center; width: 120%;">
                    <img data-zoomable="" style="width: 100%;" src="static/img/vision_connector_v7.png">
                </div>
                <figcaption style="text-align: center; width: 120%;">
                    Figure 6: Spatial Vision Aggregator (SVA).
                </figcaption>
            </div>

            <p class="text">
                Specifically, the updated query vector <strong>q<sup>*</sup></strong><sub>i,j</sub> at position (<em>i,j</em>) is computed as:
            </p>
            <!-- <div class="equation">
                <pre>
                    <strong>q<sup>*</sup></strong><sub>i,j</sub> = softmax(<strong>q</strong><sub>i,j</sub> · [<strong>k</strong><sub>i,j,1</sub>, <strong>k</strong><sub>i,j,2</sub>, ..., <strong>k</strong><sub>i,j,N</sub>]<sup>⊤</sup> / √<em>C</em>) [<strong>v</strong><sub>i,j,1</sub>, <strong>v</strong><sub>i,j,2</sub>, ..., <strong>v</strong><sub>i,j,N</sub>],</pre>
            </div>
            <div class="equation">
                <pre>
                    <strong>q</strong><sub>i,j</sub> = <strong>W<sup>Q</sup></strong> <strong>x</strong><sub>i,j</sub> ∈ ℝ<sup>1×C</sup>,
                    <strong>k</strong><sub>i,j,k</sub> = <strong>W<sup>K</sup></strong><sub>k</sub> <strong>F</strong><sub>k</sub>[<em>m<sub>k</sub> · i:m<sub>k</sub> · (i+1), m<sub>k</sub> · j:m<sub>k</sub> · (j+1)</em>] ∈ ℝ<sup>m<sub>k</sub><sup>2</sup>×C</sup>,
                    <strong>v</strong><sub>i,j,k</sub> = <strong>W<sup>V</sup></strong><sub>k</sub> <strong>F</strong><sub>k</sub>[<em>m<sub>k</sub> · i:m<sub>k</sub> · (i+1), m<sub>k</sub> · j:m<sub>k</sub> · (j+1)</em>] ∈ ℝ<sup>m<sub>k</sub><sup>2</sup>×C</sup>
                </pre>
            </div> -->
            <div class="equation">
                \[
                \mathbf{q^{*}}_{i,j} = \text{softmax}\left(\frac{\mathbf{q}_{i,j} \cdot \left[\mathbf{k}_{i,j,1}, \mathbf{k}_{i,j,2}, \ldots, \mathbf{k}_{i,j,N}\right]^\top}{\sqrt{C}}\right) \left[\mathbf{v}_{i,j,1}, \mathbf{v}_{i,j,2}, \ldots, \mathbf{v}_{i,j,N}\right],
                \]
            </div>
            <div class="equation">
                \[
                \begin{align*}
                \mathbf{q}_{i,j} &= \mathbf{W}^Q \mathbf{x}_{i,j} \in \mathbb{R}^{1 \times C}, \\
                \mathbf{k}_{i,j,k} &= \mathbf{W}^K_{k} \mathbf{F}_{k}[m_{k} \cdot i : m_{k} \cdot (i+1),\ m_{k} \cdot j : m_{k} \cdot (j+1)] \in \mathbb{R}^{m_k^2 \times C}, \\
                \mathbf{v}_{i,j,k} &= \mathbf{W}^V_{k} \mathbf{F}_{k}[m_{k} \cdot i : m_{k} \cdot (i+1),\ m_{k} \cdot j : m_{k} \cdot (j+1)] \in \mathbb{R}^{m_k^2 \times C}.
                \end{align*}
                \]
            </div>
            
            <p class="text">
                Here, <strong>q</strong><sub>i,j</sub> is the query vector at position (<em>i,j</em>), calculated using the query projection matrix <strong>W<sup>Q</sup></strong> ∈ ℝ<sup>C × C</sup>. The key vectors <strong>k</strong><sub>i,j,k</sub> and value vectors <strong>v</strong><sub>i,j,k</sub> are computed for each vision encoder <em>k</em> using their respective key and value projection matrices <strong>W<sup>K</sup></strong><sub>k</sub> ∈ ℝ<sup>C × C</sup> and <strong>W<sup>V</sup></strong><sub>k</sub> ∈ ℝ<sup>C × C</sup>. Since ∑<sub>k</sub> m<sub>k</sub><sup>2</sup> features are aggregated into a single token, we effectively reduce the number of tokens.
            </p>
            <strong class="text">Multi-layer vision aggregation.</strong>
            <p class="text">
                Although our proposal effectively aggregates features from multiple vision encoders, there is still potential information loss when processing high-resolution input (<em>i.e.</em> <em>m<sub>k</sub></em> is large) or when there are multiple vision encoders (<em>i.e.</em> <em>N</em> is large). This results in a single token having to handle a larger amount of context information during aggregation. To alleviate this, we enable cross-attention to occur multiple times by inserting our proposal within the LLM layers, allowing the model to refer to the uncompressed visual information consistently (see <a href="#fig:vision_connector">Figure 6-right</a>).
            </p>
        </div>

        <div id="instruction_data" class="data-block">
            <h1 class="text">Instruction Tuning Data for Training MLLMs</h1>
            <p class="text">
                Previous work highlights the importance of data in training MLLMs, but explicit investigations are limited. 
                In this study, we gather all available instruction tuning data and examine data curation by enhancing diversity, balancing sources, and improving mixtures. 
                Unless specified otherwise, experiments involve fine-tuning on the Vicuna-7B-1.5 model and OpenAI's CLIP ViT-L/14@336px model.
            </p>
    
            <div class="subsection">
                <h2 class="text">Data Collection</h2>
                <p class="text" id="data_collection">
                    <strong>Collecting Instruction Tuning Data from existing data sources</strong> 
                    Unlike language data, multimodal (visual) instruction-tuning data is much rarer and harder to collect. 
                    To address this, we use existing multimodal benchmarks and datasets involving visual interaction data,
                    such as Visual Question Answering (VQA) and OCR data. 
                    Previous work also highlights the catastrophic forgetting that commonly occurs when fine-tuning multimodal LLMs. 
                    We also collect a small volume of high-quality language-only instruction-following data to maintain its language ability. 
                    Data are categorized into General conversation, OCR, Counting, Code, Math, Science, and Language-only data. 
                    We list the data sources in <a href="#fig:cambrian7m">Figure 7</a>.
                </p>
                <div id="fig:cambrian7m" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 140%;">
                    <img data-zoomable="" style="width: 100%;" src="static/img/cambrian_7m.png">
                    </div>
                    <figcaption style="text-align: center; width: 140%;">
                        Figure 7: Cambrian-7M: A Large-Scale Curated Instruction Tuning Dataset for Training MLLM.
                    </figcaption>
                </div>
                <p class="text">
                    <strong>Targeted Internet Data Collection Engine</strong> 
                    We introduce a data engine designed to create large-scale, reliable, 
                    high-quality knowledge-based multimodal instruction tuning data (see <a href="#fig:dataengine">Figure 8</a>).
                    The engine selects a target field and subfield, such as "Physics," 
                    and uses an LLM like GPT-4 to identify topics (e.g., "Newton’s Laws"). 
                    It searches reliable sources like Wikipedia for each topic. 
                    We find that Wikipedia cites exhibit high-correlated text-image associations. 
                    The parser extracts image-caption-text tuples and feeds the caption-text to an LLM, such as GPT-3., 
                    to generate instruction-type Q&A pairs about the image. 
                    These Q&A pairs and the image form our VQA dataset.
                     <!-- Details are in <a href="#Appendix:DataEngine">Appendix: Data Engine</a>. -->
                </p>
                <div id="fig:dataengine" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 140%;">
                    <img data-zoomable="" style="width: 100%;" src="static/img/dataenginefigurepdf_crop.png">
                    </div>
                    <figcaption style="text-align: center; width: 140%;">
                        Figure 8: Targeted Internet Data Collection Engine.
                    </figcaption>
                </div>
    
                <p class="text">
                    <strong>Cambrian-10M</strong> 
                    To this end, we create a large pool of instruction tuning data, which we refer to as Cambrian-10M. 
                    This pool contains approximately 9784k data points, offering a diverse range of data for our work and future research. 
                    We also visualize its composition in <a href="#fig:cambrian7m">Figure 7</a>.
                </p>
            </div>
    
            <div id="sec:data_curation" class="subsection">
                <h2 class="text">Data Curation</h2>
                <p class="text">
                    Cambrian-10M is a large pool of instruction tuning data sourced from a variety of data sources, 
                    with an unbalanced data ratio between categories. 
                    Here, we take a preliminary step to study data curation by improving data balancing and adjusting data ratios.
                </p>
    
                <p class="text" id="data_curation">
                    <strong>Data Balancing</strong> 
                    We follow previous work to set thresholds t 
                    for the number of data points from a single data source. To study the effect of the number t, 
                    we plot the cumulative sum of counts for entries sorted by counts from tail to head 
                    (see Visualization in <a href="#fig:filter_k">Figure 9</a>). 
                    We choose t = 150k, 250k, 350k, and 450k in this section and observe an 
                    elbow effect in <a href="#tab:data_balance_result">Table 3</a>. 
                    We find that a threshold between 250k and 350k works the best for Cambrian-10M.
                </p>
                <div id="fig:filter_k" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 100%;">
                    <img data-zoomable="" style="width: 100%;" src="static/img/Cumulative_Sum_of_Counts.png">
                    </div>
                    <figcaption style="text-align: center; width: 100%;">
                        Figure 9: Data Balancing via Applying Thresholds on Data Sources.
                    </figcaption>
                </div>

                <div id="tab:data_balance_result" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 100%;">
                    <img data-zoomable="" style="width: 100%;" src="static/img/table/data_filter_k.png">
                    </div>
                    <figcaption style="text-align: center; width: 100%;">
                        Table 3: Threshold 𝑡 value between 250k and 350k obtains better performance.
                    </figcaption>
                </div>

                <p class="text">
                    <strong>Data Ratio</strong> 
                    Unlike previous works in VLM data curation<d-cite key="xu2023demystifying"></d-cite> <d-cite key="gadre2024datacomp"></d-cite>, 
                    which curate noisy raw image-text pairs by scraping the internet, 
                    Cambrian-10M is designed for visual instruction tuning. 
                    Given the various capabilities of different types of data, it is essential to balance the ratio of these data types. 
                    We conduct pilot experiments with a fixed dataset size of 1350k, 
                    examining the impact of different data ratios on downstream performance. 
                    We visualize the results in <a href="#fig:data_ratio">Figure 10</a> and summarize our findings as follows: 
                    (i) Balancing General, OCR, and Language data is crucial. 
                    The model's OCR capability is proportional to the OCR data ratio; 
                    however, an excessive OCR ratio compromises general VQA and vision-centric performance.
                    (ii) Performance on knowledge-intensive tasks is influenced by multiple factors, 
                    often requiring a mix of OCR, chart, reasoning, and general perception. 
                    Increasing the science data ratio can help, but a very low ratio leads to poor performance.
                </p>
                
                <div id="fig:data_ratio" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 100%;">
                    <img data-zoomable="" style="width: 100%;" src="static/img/data_mixture_ratio_w_avg_score.png">
                    </div>
                    <figcaption style="text-align: center; width: 100%;">
                        Figure 10: Exploring instruction tuning data mixture ratios.
                    </figcaption>
                </div>

                <p class="text">
                    <strong>Cambrian-7M</strong> 
                    We follow the identified data ratio and apply the data filtering technique while curating to that ratio. 
                    In the end, we obtain Cambrian-7M. In <a href="#tab:data_ratio_result">Table 4</a>, 
                    we observe improvements by scaling up and curating better data, even with less quantity.
                </p>
                <div id="tab:data_ratio_result" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 100%;">
                    <img data-zoomable="" style="width: 100%;" src="static/img/table/data_ratio_result.png">
                    </div>
                    <figcaption style="text-align: center; width: 100%;">
                        Table 4: Performance improves with better instruction tuning data curation.
                    </figcaption>
                </div>
                

            </div>
    
            <div class="subsection">
                <h2 class="text">Resolving "Answer Machine Phenomenon" with System Prompts</h2>
                <p class="text">
                    Here, we explore and analyze a phenomenon we term the "answer machine phenomenon." 
                    We observe that a well-trained MLLM excels in visual question answering 
                    but lacks basic conversational abilities (see examples in <a href="#fig:sysprompt">Figure 5</a>). 
                    The model tends to output shorter responses.
                    This discrepancy arises because benchmark questions typically require responses that are limited to 
                    a single option, choice, or word, which diverges from the broader and actual use cases of MLLMs. 
                    Similar phenomena have been discussed in other LLM studies.
                </p>
    
                <p class="text">
                    We suspect that this issue stems from the instruction tuning data containing an excessive number of short-response VQA tasks, 
                    leading to catastrophic forgetting in LLMs.
                    To address this, we find that incorporating additional system prompts during training mitigates this phenomenon. 
                    We append prompts such as "<em>Answer the question using a single word or phrase.</em>" 
                    before questions that generate a single word or phrase in the response. 
                    We observe that after integrating these system prompts, the model's benchmark performance remains unchanged, 
                    while its conversational ability significantly improves. For example, in <a href="#fig:sysprompt">Figure 11</a>, 
                    models with system prompts tend to output longer and more engaging outputs while answering the questions correctly. 
                    It also encourages the model to improve on reasoning-related tasks such as math problems by outputting a series of thoughts, 
                    and then output the answer.
                </p>
                <div id="fig:sysprompt" style="display: flex; flex-direction: column; align-items: center;">
                    <div style="display: flex; justify-content: center; width: 140%;">
                    <img data-zoomable="" style="width: 100%;" src="static/img/sysprompt.png">
                    </div>
                    <figcaption style="text-align: center; width: 140%;">
                        Figure 11: Incorporating System Prompt in Instruction Tuning Data alleviates “Answer
                        Machine Phenomenon”.
                    </figcaption>
                </div>
    
                <p class="text">
                    Further, this underscores the necessity to develop evaluation protocols like the Chatbot Arena, despite the challenges in collecting large-scale, actual use cases of multimodal data.
                </p>
            </div>
        </div>

        <div id='sota' class="sota-block">
            <h1 class="text">State of the Art MLLM Performance</h1>
            <p class="text">
                Finally, we leverage the insights from all of our previous studies to train a high-performance Cambrian model.
                We train with three different sizes of LLM backbones: LLaMA-3-Instruct-8B, Vicuna-1.5-13B, and Hermes-2-Yi-34B.
                We have a vision combination of four models—SigLIP, CLIP, DINOv2, and OpenCLIP ConvNeXt 
                (see <a href="#sec:model_ensemble">Combining Multiple Vision Encoders</a>) with <a href="#connector_design">Spatial Vision Aggregator</a>.
                We use 2.5M adapter data and Cambrian-7M instruction tuning data (see <a href="#sec:data_curation">Data Curation</a>).
                We evaluate our models on the <a href="#sec:benchmarking">categorized benchmarks</a>. 
                We show the results in <a href="#tab:final_table">Table 5</a>. Cambrian-1 exceeds other open-source models such as LLaVA-NeXT and Mini-Gemini. 
                Cambrian-1 also achieves comparable performance on a number of benchmarks with the best proprietary models such as GPT-4V, Gemini-Pro, and MM-1.
            </p>
            <div id="tab:final_table" style="display: flex; flex-direction: column; align-items: center;">
                <div style="display: flex; justify-content: center; width: 140%;">
                <img data-zoomable="" style="width: 100%;" src="static/img/table/final_result.png">
                </div>
                <figcaption style="text-align: center; width: 140%;">
                    Table 5: Performance improves with better instruction tuning data curation.
                </figcaption>
            </div>
        </div>

        <div id="conclusion" style="position: relative; margin-top: 40px; margin-bottom: 0px;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">Conclusion</h2>
            <p class="text">
                To conclude, Cambrian-1 introduces a family of state-of-the-art MLLM models that achieve top performance across diverse benchmarks 
                and excel in visual-centric tasks. We provide model weights, open-source code, datasets, and detailed recipes for model training and evaluation. 
                We hope our work will strengthen the open research community and accelerate research in both visual representation learning and multimodal systems.
            </p>
        </div>

        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @article{peter2024cambrian,<br>
                &nbsp;&nbsp;title={{Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs}},<br>
                &nbsp;&nbsp;author={To list},<br>
                &nbsp;&nbsp;year={2024},<br>
                &nbsp;&nbsp;journal={arXiv preprint},<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>
        
        <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <d-bibliography src="bibliography.bib"></d-bibliography>
        <script src="./static/js/nav-bar.js"></script>
        
    </body>
</html>
